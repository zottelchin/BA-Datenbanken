@inproceedings{10.1109/ICSM.2015.7332449,
author = {Vendome, Christopher and Linares-Vasquez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel M. and Poshyvanyk, Denys},
title = {When and Why Developers Adopt and Change Software Licenses},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332449},
doi = {10.1109/ICSM.2015.7332449},
abstract = {Software licenses legally govern the way in which developers can use, modify, and redistribute a particular system. While previous studies either investigated licensing through mining software repositories or studied licensing through FOSS reuse, we aim at understanding the rationale behind developers' decisions for choosing or changing software licensing by surveying open source developers. In this paper, we analyze when developers consider licensing, the reasons why developers pick a license for their project, and the factors that influence licensing changes. Additionally, we explore the licensing-related problems that developers experienced and expectations they have for licensing support from forges (e.g., GitHub). Our investigation involves, on one hand, the analysis of the commit history of 16,221 Java open source projects to identify the commits where licenses were added or changed. On the other hand, it consisted of a survey—in which 138 developers informed their involvement in licensing-related decisions and 52 provided deeper insights about the rationale behind the actions that they had undertaken. The results indicate that developers adopt licenses early in the project's development and change licensing after some period of development (if at all). We also found that developers have inherent biases with respect to software licensing. Additionally, reuse—whether by a non-contributor or for commercial purposes—is a dominant reason why developers change licenses of their systems. Finally, we discuss potential areas of research that could ameliorate the difficulties that software developers are facing with regard to licensing issues of their software systems.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {31–40},
numpages = {10},
series = {ICSME '15}
}

@inproceedings{10.1145/2635868.2635922,
author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
title = {A Large Scale Study of Programming Languages and Code Quality in Github},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635922},
doi = {10.1145/2635868.2635922},
abstract = { What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {155–165},
numpages = {11},
keywords = {bug fix, software domain, programming language, regression analysis, type system, empirical research, code quality},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/3338906.3341181,
author = {Guerrero, Alejandro and Fresno, Rafael and Ju, An and Fox, Armando and Fernandez, Pablo and Muller, Carlos and Ruiz-Cort\'{e}s, Antonio},
title = {Eagle: A Team Practices Audit Framework for Agile Software Development},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341181},
doi = {10.1145/3338906.3341181},
abstract = {Agile/XP (Extreme Programming) software teams are expected to follow a number of specific practices in each iteration, such as estimating the effort (”points”) required to complete user stories, properly using branches and pull requests to coordinate merging multiple contributors’ code, having frequent ”standups” to keep all team members in sync, and conducting retrospectives to identify areas of improvement for future iterations. We combine two observations in developing a methodology and tools to help teams monitor their performance on these practices. On the one hand, many Agile practices are increasingly supported by web-based tools whose ”data exhaust” can provide insight into how closely the teams are following the practices. On the other hand, some of the practices can be expressed in terms similar to those developed for expressing service level objectives (SLO) in software as a service; as an example, a typical SLO for an interactive Web site might be ”over any 5-minute window, 99% of requests to the main page must be delivered within 200ms” and, analogously, a potential Team Practice (TP) for an Agile/XP team might be ”over any 2-week iteration, 75% of stories should be ’1-point’ stories”. Following this similarity, we adapt a system originally developed for monitoring and visualizing service level agreement (SLA) compliance to monitor selected TPs for Agile/XP software teams. Specifically, the system consumes and analyzes the data exhaust from widely-used tools such as GitHub and Pivotal Tracker and provides team(s) and coach(es) a ”dashboard” summarizing the teams’ adherence to various practices. As a qualitative initial investigation of its usefulness, we deployed it to twenty student teams in a four-sprint software engineering project course. We find an improvement of the adherence to team practice and a positive students’ self-evaluations of their team practices when using the tool, compared to previous experiences using an Agile/XP methodology. The demo video is located at <a>https://youtu.be/A4xwJMEQh9c</a> and a landing page with a live demo at <a>https://isa-group.github.io/2019-05-eagle-demo/</a>.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1139–1143},
numpages = {5},
keywords = {team dashboard, team practice, agile, team practice agreement},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3183713.3183746,
author = {Gao, Yihan and Huang, Silu and Parameswaran, Aditya},
title = {Navigating the Data Lake with DATAMARAN: Automatically Extracting Structure from Log Datasets},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183746},
doi = {10.1145/3183713.3183746},
abstract = {Organizations routinely accumulate semi-structured log datasets generated as the output of code; these datasets remain unused and uninterpreted, and occupy wasted space---this phenomenon has been colloquially referred to as "data lake'' problem. One approach to leverage these semi-structured datasets is to convert them into a structured relational format, following which they can be analyzed in conjunction with other datasets. We present DATAMARAN, an tool that extracts structure from semi-structured log datasets with no human supervision. DATAMARAN automatically identifies field and record endpoints, separates the structured parts from the unstructured noise or formatting, and can tease apart multiple structures from within a dataset, in order to efficiently extract structured relational datasets from semi-structured log datasets, at scale with high accuracy. Compared to other unsupervised log dataset extraction tools developed in prior work, DATAMARAN does not require the record boundaries to be known beforehand, making it much more applicable to the noisy log files that are ubiquitous in data lakes. DATAMARAN can successfully extract structured information from all datasets used in prior work, and can achieve 95% extraction accuracy on automatically collected log datasets from GitHub---a substantial 66% increase of accuracy compared to unsupervised schemes from prior work. Our user study further demonstrates that the extraction results of DATAMARAN are closer to the desired structure than competing algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {943–958},
numpages = {16},
keywords = {log datasets, unsupervised structure extraction},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/2983990.2984041,
author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
title = {Probabilistic Model for Code with Decision Trees},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984041},
doi = {10.1145/2983990.2984041},
abstract = { In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).  The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy.  Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy. },
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {731–747},
numpages = {17},
keywords = {Probabilistic Models of Code, Code Completion, Decision Trees},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984041,
author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
title = {Probabilistic Model for Code with Decision Trees},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984041},
doi = {10.1145/3022671.2984041},
abstract = { In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).  The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy.  Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy. },
journal = {SIGPLAN Not.},
month = oct,
pages = {731–747},
numpages = {17},
keywords = {Code Completion, Decision Trees, Probabilistic Models of Code}
}

@inproceedings{10.1145/3410530.3414365,
author = {Pellatt, Lloyd and Roggen, Daniel},
title = {CausalBatch: Solving Complexity/Performance Tradeoffs for Deep Convolutional and LSTM Networks for Wearable Activity Recognition},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414365},
doi = {10.1145/3410530.3414365},
abstract = {Deep neural networks consisting of a combination of convolutional feature extractor layers and Long Short Term Memory (LSTM) recurrent layers are widely used models for activity recognition from wearable sensors ---referred to as DeepConvLSTM architectures hereafter. However, the subtleties of training these models on sequential time series data is not often discussed in the literature. Continuous sensor data must be segmented into temporal 'windows', and fed through the network to produce a loss which is used to update the parameters of the network. If trained naively using batches of randomly selected data as commonly reported, then the temporal horizon (the maximum delay at which input samples can effect the output of the model) of the network is limited to the length of the window. An alternative approach, which we will call CausalBatch training, is to construct batches deliberately such that each consecutive batch contains windows which are contiguous in time with the windows of the previous batch, with only the first batch in the CausalBatch consisting of randomly selected windows. After a given number of consecutive batches (referred to as the CausalBatch duration τ), the LSTM states are reset, new random starting points are chosen from the dataset and a new CausalBatch is started. This approach allows us to increase the temporal horizon of the network without increasing the window size, which enables networks to learn data dependencies on a longer timescale without increasing computational complexity.We evaluate these two approaches on the Opportunity dataset. We find that using the CausalBatch method we can reduce the training time of DeepConvLSTM by up to 90%, while increasing the user-independent accuracy by up to 6.3% and the class weighted F1 score by up to 5.9% compared to the same model trained by random batch training with the best performing choice of window size for the latter. Compared to the same model trained using the same window length, and therefore the same computational complexity and almost identical training time, we observe an 8.4% increase in accuracy and 14.3% increase in weighted F1 score. We provide the source code for all experiments as well as a Pytorch reference implementation of DeepConvLSTM in a public github repository.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {272–277},
numpages = {6},
keywords = {batch training, activity recognition, wearable computing, neural networks, deep learning, best practices, LSTM},
location = {Virtual Event, Mexico},
series = {UbiComp-ISWC '20}
}

@inproceedings{10.1145/3368089.3417926,
author = {Li, Boao and Yan, Meng and Xia, Xin and Hu, Xing and Li, Ge and Lo, David},
title = {DeepCommenter: A Deep Code Comment Generation Tool with Hybrid Lexical and Syntactical Information},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417926},
doi = {10.1145/3368089.3417926},
abstract = {As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods. We implement DeepCommenter in the form of an Integrated Development Environment (i.e., Intellij IDEA) plug-in. Such plug-in is built upon a Client/Server architecture. The client formats the code selected by the user, sends request to the server and inserts the comment generated by the server above the selected code. The server listens for client’s request, analyzes the requested code using the pre-trained model and sends back the generated comment to the client. The pre-trained model learns both the lexical and syntactical information from source code tokens and Abstract Syntax Trees (AST) respectively and combines these two types of information together to generate comments. To evaluate DeepCommenter, we conduct experiments on a large corpus built from a large number of open source Java projects on GitHub. The experimental results on different metrics show that DeepCommenter outperforms the state-of-the-art approaches by a substantial margin.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1571–1575},
numpages = {5},
keywords = {Deep Learning, Comment Generation, Program Comprehension},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3318464.3389738,
author = {Yan, Cong and He, Yeye},
title = {Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389738},
doi = {10.1145/3318464.3389738},
abstract = {Data preparation is widely recognized as the most time-consuming process in modern business intelligence (BI) and machine learning (ML) projects. Automating complex data preparation steps (e.g., Pivot, Unpivot, Normalize-JSON, etc.)holds the potential to greatly improve user productivity, and has therefore become a central focus of research. We propose a novel approach to "auto-suggest" contextualized data preparation steps, by "learning" from how data scientists would manipulate data, which are documented by data science notebooks widely available today. Specifically, we crawled over 4M Jupyter notebooks on GitHub, and replayed them step-by-step, to observe not only full input/output tables (data-frames) at each step, but also the exact data-preparation choices data scientists make that they believe are best suited to the input data (e.g., how input tables are Joined/Pivoted/Unpivoted, etc.). By essentially "logging" how data scientists interact with diverse tables, and using the resulting logs as a proxy of "ground truth", we can learn-to-recommend data preparation steps best suited to given user data, just like how search engines (Google or Bing) leverage their click-through logs to learn-to-rank documents. This data-driven and log-driven approach leverages the "collective wisdom" of data scientists embodied in the notebooks, and is shown to significantly outperform strong baselines including commercial systems in terms of accuracy.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1539–1554},
numpages = {16},
keywords = {learning-to-recommend, data wrangling, data preparation, pivot and unpivot},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/2810103.2813604,
author = {Perl, Henning and Dechand, Sergej and Smith, Matthew and Arp, Daniel and Yamaguchi, Fabian and Rieck, Konrad and Fahl, Sascha and Acar, Yasemin},
title = {VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813604},
doi = {10.1145/2810103.2813604},
abstract = {Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. However, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is sufficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flawfinder, to flag potentially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the amount of false alarms by over 99 % at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {426–437},
numpages = {12},
keywords = {vulnerabilities, static analysis, machine learning},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{10.1145/3307339.3343175,
author = {Pearson, Antony},
title = {Extracting Structure from Contaminated Symbolic Data},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343175},
doi = {10.1145/3307339.3343175},
abstract = {Symbolic data is the epitome of modern biological datasets. Modern sequencing technologies produce millions of reads giving insights on genome sequence, transcription levels, epigenetic modifications, and much more. To analyze those sequences one usually makes assumptions on their underlying structure, e.g., that the number of reads has Poisson distribution, or that transcription factor binding events occur independently at nonoverlapping promoters. These types of assumptions are often not exactly correct in reality. In fact, even when they are valid, a small amount of data "contamination" may make them appear untrue. The traditional approach to questioning assumptions on data has been hypothesis testing. This approach has various shortcomings, however. Particularly, its Boolean nature does not give room for a null hypothesis to be "approximately true.'' This tutorial introduces a methodology to assess statistical assumptions on symbolic data that may be contaminated. It will give a general overview on how to approach these problems numerically, and present analytical results for some special classes of structured probability distributions. It will demonstrate the applicability of this rather new methodology with DNA methylation data to question the common but unconscious assumption that methylation of CpGs is exchangeable, and transcription factor binding site k-mers to question the use of logoplots in summarizing TFBS behaviour. Data and code for this tutorial, in the form an iPython Notebook, will be made available via GitHub. This work is in collaboration with M. E. Lladser.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {556},
numpages = {1},
keywords = {dna methylation, epigenetics, exchangeability, independence, symbolic data, contamination, transcription factor binding sites},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/3388440.3414701,
author = {Hippe, Kyle and Gbenro, Sola and Cao, Renzhi},
title = {ProLanGO2: Protein Function Prediction with Ensemble of Encoder-Decoder Networks},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414701},
doi = {10.1145/3388440.3414701},
abstract = {Predicting protein function from protein sequence is a main challenge in the computational biology field. Traditional methods that search protein sequences against existing databases may not work well in practice, particularly when little or no homology exists in the database. We introduce the ProLanGO2 method which utilizes the natural language processing and machine learning techniques to tackle the protein function prediction problem with protein sequence as input. Our method has been benchmarked blindly in the latest Critical Assessment of protein Function Annotation algorithms (CAFA 4) experiment. There are a few changes compared to the old version of ProLanGO. First of all, the latest version of the UniProt database is used. Second, the Uniprot database is filtered by the newly created fragment sequence database FSD to prepare for the protein sequence language. Third, the Encoder-Decoder network, a model consisting of two RNNs (encoder and decoder), is used to train models on the dataset. Fourth, if no k-mers of a protein sequence exist in the FSD, we select the top ten GO terms with the highest probability in all sequences from the Uniprot database that didn't contain any k-mers in FSD, and use those ten GO terms as back up for the prediction of new protein sequence. Finally, we selected the 100 best performing models and explored all combinations of those models to select the best performance ensemble model. We benchmark those different combinations of models on CAFA 3 dataset and select three top performance ensemble models for prediction in the latest CAFA 4 experiment as CaoLab. We have also evaluated the performance of our ProLanGO2 method on 253 unseen sequences taken from the UniProt database and compared with several other protein function prediction methods, the results show that our method achieves great performance among sequence-based protein function prediction methods. Our method is available in GitHub: https://github.com/caorenzhi/ProLanGO2.git.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {103},
numpages = {6},
keywords = {Recurrent Neural Network, Machine learning, Protein function prediction},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3416506.3423579,
author = {Liu, Binbin and Dong, Wei and Zhang, Yating and Wang, Daiyan and Liu, Jiaxin},
title = {Boosting Component-Based Synthesis with Control Structure Recommendation},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423579},
doi = {10.1145/3416506.3423579},
abstract = {Component-based synthesis is an important research field in program synthesis. API-based synthesis is a subfield of component-based synthesis, the component library of which are Java APIs. Unlike existing work in API-based synthesis that can only generate loop-free programs constituted by APIs, state-of-the-art work FrAngel can generate programs with control structures. However, for the generation of control structures, it samples different types of control structures all at random. Given the information about the desired method (such as method name and input/output types), experienced programmers can have an initial thought about the possible control structures that could be used in implementing the desired method. The knowledge about control structures in the method can be learned from high-quality projects. In this paper, we propose a novel approach of recommending control structures for API-based synthesis based on deep learning. A neural network that can jointly embed the natural language description, method name, and input/output types into high-dimensional vectors to predict the possible control structures of the desired method is proposed. We integrate the prediction model into the synthesizer to improve the efficiency of synthesis. We train our model on a codebase of high-quality Java projects from GitHub. The prediction results of the neural network are fed to the API-based synthesizer to guide the sampling process of control structures. The experimental results on 40 programming tasks show that our approach can effectively improve the efficiency of synthesis.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {19–28},
numpages = {10},
keywords = {control structure recommendation, program synthesis, deep learning},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.1145/3159450.3162206,
author = {Cutler, Barbara and Peveler, Matthew and Breese, Samuel and Maicus, Evan and Milanova, Ana and Holzbauer, Buster and Aikens, Andrew and Anderson, James and Barthelmess, Josh and Cyrus, Timothy and Lee, Marisa and Montealegre, Leon and Wang, Jessica},
title = {Supporting Team Submissions and Peer Grading within Submitty: (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162206},
doi = {10.1145/3159450.3162206},
abstract = {Submitty is an open source programming assignment submission system from the Rensselaer Center for Open Source Software (RCOS) at Rensselaer Polytechnic Institute (RPI) accessed via an online interface. Submitty allows students to submit their code through file upload or version control, such as an internal Git/SVN server or Github, where it is then tested with a highly configurable and customizable automated grader. For each assignment, instructors can specify whether or not students can work in teams. For team assignments, the instructor can either assign teammates or allow the students to choose. In addition to the auto-grading for submissions, Submitty supports human grading. The human graded rubric is developed by the graders as they work, allowing reuse of common feedback messages and partial credit points. The rubric can be searched and modified during and after grading is complete for consistency. By default, grading is handled by instructors and TAs who are assigned to sections of students, which can be rotated through the semester. However, an instructor can choose to incorporate peer grading, which will allow students to anonymously view and submit grades for each other, receiving multiple peer grades per assignment. Submitty has been used at RPI for several years for a variety of courses, serving over 1500 students and 50 instructors and TAs each semester, and has recently been used by several other universities. We will present "case studies" of assignment configurations for autograding and manual grading and demonstrate the grading interface in support of team submissions and peer grading.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1111},
numpages = {1},
keywords = {education, version control, team assignments, peer grading, autograding},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1145/3373376.3378503,
author = {Angstadt, Kevin and Jeannin, Jean-Baptiste and Weimer, Westley},
title = {Accelerating Legacy String Kernels via Bounded Automata Learning},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378503},
doi = {10.1145/3373376.3378503},
abstract = {The adoption of hardware accelerators, such as FPGAs, into general-purpose computation pipelines continues to rise, but programming models for these devices lag far behind their CPU counterparts. Legacy programs must often be rewritten at very low levels of abstraction, requiring intimate knowledge of the target accelerator architecture. While techniques such as high-level synthesis can help port some legacy software, many programs perform poorly without manual, architecture-specific optimization.We propose an approach that combines dynamic and static analyses to learn a model of functional behavior for off-the-shelf legacy code and synthesize a hardware description from this model. We develop a framework that transforms Boolean string kernels into hardware descriptions using techniques from both learning theory and software verification. These include Angluin-style state machine learning algorithms, bounded software model checking with incremental loop unrolling, and string decision procedures. Our prototype implementation can correctly learn functionality for kernels that recognize regular languages and provides a near approximation otherwise. We evaluate our prototype tool on a benchmark suite of real-world, legacy string functions mined from GitHub repositories and demonstrate that we are able to learn fully-equivalent hardware designs in 72% of cases and close approximations in another 11%. Finally, we identify and discuss challenges and opportunities for more general adoption of our proposed framework to a wider class of function types.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {235–249},
numpages = {15},
keywords = {legacy programs, automata processing, automata learning},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1109/ESEM.2017.22,
author = {Falessi, Davide and Smith, Wyatt and Serebrenik, Alexander},
title = {STRESS: A Semi-Automated, Fully Replicabile Approach for Project Selection},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.22},
doi = {10.1109/ESEM.2017.22},
abstract = {The mining of software repositories has provided significant advances in a multitude of software engineering fields, including defect prediction. Several studies show that the performance of a software engineering technology (e.g., prediction model) differs across different project repositories. Thus, it is important that the project selection is replicable. The aim of this paper is to present STRESS, a semi-automated and fully replicable approach that allows researchers to select projects by configuring the desired level of diversity, fit, and quality. STRESS records the rationale behind the researcher decisions and allows different users to re-run or modify such decisions. STRESS is open-source and it can be used used locally or even online (www.falessi.com/STRESS/). We perform a systematic mapping study that considers studies that analyzed projects managed with JIRA and Git to asses the project selection replicability of past studies. We validate the feasible application of STRESS in realistic research scenarios by applying STRESS to select projects among the 211 Apache Software Foundation projects. Our systematic mapping study results show that none of the 68 analyzed studies is completely replicable. Regarding STRESS, it successfully supported the project selection among all 211 ASF projects. It also supported the measurement of 100 projects characteristics, including the 32 criteria of the studies analyzed in our mapping study. The mapping study and STRESS are, to our best knowledge, the first attempt to investigate and support the replicability of project selection. We plan to extend them to other technologies such as GitHub.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {151–156},
numpages = {6},
keywords = {apache, replication, mining software repositories},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1109/ICSE.2017.30,
author = {Wittern, Erik and Ying, Annie T. T. and Zheng, Yunhui and Dolby, Julian and Laredo, Jim A.},
title = {Statically Checking Web API Requests in JavaScript},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.30},
doi = {10.1109/ICSE.2017.30},
abstract = {Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {244–254},
numpages = {11},
keywords = {static analysis, web APIs, JavaScript},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3293882.3338996,
author = {Wang, Cong and Gao, Jian and Jiang, Yu and Xing, Zhenchang and Zhang, Huafeng and Yin, Weiliang and Gu, Ming and Sun, Jiaguang},
title = {Go-Clone: Graph-Embedding Based Clone Detector for Golang},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338996},
doi = {10.1145/3293882.3338996},
abstract = {Golang (short for Go programming language) is a fast and compiled language, which has been increasingly used in industry due to its excellent performance on concurrent programming. Golang redefines concurrent programming grammar, making it a challenge for traditional clone detection tools and techniques. However, there exist few tools for detecting duplicates or copy-paste related bugs in Golang. Therefore, an effective and efficient code clone detector on Golang is especially needed.  In this paper, we present Go-Clone, a learning-based clone detector for Golang. Go-Clone contains two modules -- the training module and the user interaction module. In the training module, firstly we parse Golang source code into llvm IR (Intermediate Representation). Secondly, we calculate LSFG (labeled semantic flow graph) for each program function automatically. Go-Clone trains a deep neural network model to encode LSFGs for similarity classification. In the user interaction module, users can choose one or more Golang projects. Go-Clone identifies and presents a list of function pairs, which are most likely clone code for user inspection. To evaluate Go-Clone's performance, we collect 6,110 commit versions from 48 Github projects to construct a Golang clone detection data set. Go-Clone can reach the value of AUC (Area Under Curve) and ACC (Accuracy) for 89.61% and 83.80% in clone detection. By testing several groups of unfamiliar data, we also demonstrates the generility of Go-Clone. The address of the abstract demo video: https://youtu.be/o5DogtYGbeo},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {374–377},
numpages = {4},
keywords = {deep neural network, code similarity, go programming language, code clone detection},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3287324.3287554,
author = {Malan, David J. and Lloyd, Doug and Zidane, Kareem},
title = {Interactive Programming Environments for Teachers and Students},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287554},
doi = {10.1145/3287324.3287554},
abstract = {We present in this hands-on workshop a suite of interactive programming environments for teachers and students, each of them cloud-based and free. The first is CS50 Sandbox, a web app at sandbox.cs50.io that enables teachers and students to create temporary programming environments quickly and share copies of those sandboxes with others. With this app can a teacher start programs in class that students can then finish, distribute starter code for problems, and post interactive solutions. The second tool is CS50 Lab, a web app at lab.cs50.io that enables teachers to create step-by-step programming lessons, providing incremental feedback at each step, and enables students to progress from an empty file (or starter code) to working code, with hints and feedback along the way. Via this app can teachers author their own Codecademy-style lessons using just a GitHub repository of their own. And third in the suite is CS50 IDE, a web app at ide.cs50.io built atop Cloud9 that provides students with their own cloud-based Linux environment. Each of these environments offers a built-in file browser and code editor and, most importantly, an interactive terminal window with shell access to their very own container. And each enables students to write programs in any language. Throughout this workshop will we discuss lessons learned from having deployed these tools in CS50 at Harvard to hundreds of students on campus and thousands of students online. And we'll discuss challenges encountered and best practices adopted.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1242},
numpages = {1},
keywords = {sandbox, programming, ide},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1109/MOBILESoft.2017.29,
author = {Kessentini, Marouane and Ouni, Ali},
title = {Detecting Android Smells Using Multi-Objective Genetic Programming},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.29},
doi = {10.1109/MOBILESoft.2017.29},
abstract = {The evolution rate of mobile applications is much higher than regular software applications having shorter release deadlines and smaller code base. Mobile applications tend to be evolved quickly by developers to meet several new customer requirements and fix discovered bugs. However, evolving the existing features and design may introduce bad design practices, also called code smells, which can highly decrease the maintainability and performance of these mobile applications. However, unlike the area of object-oriented software systems, the detection of code smells in mobile applications received a very little of attention. Recent, few studies defined a set of quality metrics for Android applications and proposed a support to manually write a set of rules to detect code smells by combining these quality metrics. However, finding the best combination of metrics and their thresholds to identify code smells is left to the developer as a manual process. In this paper, we propose to automatically generate rules for the detection of code smells in Android applications using a multi-objective genetic programming algorithm (MOGP). The MOGP algorithm aims at finding the best set of rules that cover a set of code smell examples of Android applications based on two conflicting objective functions of precision and recall. We evaluate our approach on 184 Android projects with source code hosted in GitHub. The statistical test of our results show that the generated detection rules identified 10 Android smell types on these mobile applications with an average correctness higher than 82% and an average relevance of 77% based on the feedback of active developers of mobile apps.},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {122–132},
numpages = {11},
keywords = {search-based software engineering, Android apps, quality},
location = {Buenos Aires, Argentina},
series = {MOBILESoft '17}
}

@inproceedings{10.1145/3127005.3127009,
author = {Businge, John and Kawuma, Simon and Bainomugisha, Engineer and Khomh, Foutse and Nabaasa, Evarist},
title = {Code Authorship and Fault-Proneness of Open-Source Android Applications: An Empirical Study},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127009},
doi = {10.1145/3127005.3127009},
abstract = {Context: In recent years, many research studies have shown how human factors play a significant role in the quality of software components. Code authorship metrics have been introduced to establish a chain of responsibility and simplify management when assigning tasks in large and distributed software development teams. Researchers have investigated the relationship between code authorship metrics and fault occurrences in software systems. However, we have observed that these studies have only been carried on large software systems having hundreds to thousands of contributors. In our preliminary investigations on Android applications that are considered to be relatively small, we observed that applications systems are not totally owned by a single developer (as one could expect) and that cases of no clear authorship also exist like in large systems. To this end, we do believe that the Android applications could face the same challenges faced by large software systems and could also benefit from such studies.Goal: We investigate the extent to which the findings obtained on large software systems applies to Android applications. Approach: Building on the designs of previous studies, we analyze 278 Android applications carefully selected from GitHub. We extract code authorship metrics from the applications and examine the relationship between code authorship metrics and faults using statistical modeling.Results: Our analyses confirm most of the previous findings, i.e., Android applications with higher levels of code authorship among contributors experience fewer faults.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {33–42},
numpages = {10},
keywords = {Total Contributors, Major Contributors, Most Values Contributors, Minor Contributors, Software faults},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1109/ICSE.2017.28,
author = {Liu, Han and Sun, Chengnian and Su, Zhendong and Jiang, Yu and Gu, Ming and Sun, Jiaguang},
title = {Stochastic Optimization of Program Obfuscation},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.28},
doi = {10.1109/ICSE.2017.28},
abstract = {Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques.This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = 〈t2, t2, ... , tn〉 (∀i ε [1, n]. ti ε T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq.We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {221–231},
numpages = {11},
keywords = {program obfuscation, markov chain monte carlo methods, obscurity language model},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3383219.3383264,
author = {Madeyski, Lech and Lewowski, Tomasz},
title = {MLCQ: Industry-Relevant Code Smell Data Set},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383264},
doi = {10.1145/3383219.3383264},
abstract = {Context Research on code smells accelerates and there are many studies that discuss them in the machine learning context. However, while data sets used by researchers vary in quality, all which we encountered share visible shortcomings---data sets are gathered from a rather small number of often outdated projects by single individuals whose professional experience is unknown.Aim This study aims to provide a new data set that addresses the aforementioned issues and, additionally, opens new research opportunities.Method We collaborate with professional software developers (including the code quest company behind the codebeat automated code review platform integrated with GitHub) to review code samples with respect to bad smells. We do not provide additional hints as to what do we mean by a given smell, because our goal is to extract professional developers' contemporary understanding of code smells instead of imposing thresholds from the legacy literature. We gather samples from active open source projects manually verified for industry-relevance and provide repository links and revisions. Records in our MLCQ data set contain the type of smell, its severity and the exact location in source code, but do not contain any source code metrics which can be calculated using various tools. To open new research opportunities, we provide results of an extensive survey of developers involved in the study including a wide range of details concerning their professional experience in software development and many other characteristics. This allows us to track each code review to the developer's background. To the best of our knowledge, this is a unique trait of the presented data set.Conclusions The MLCQ data set with nearly 15000 code samples was created by software developers with professional experience who reviewed industry-relevant, contemporary Java open source projects. We expect that this data set should stay relevant for a longer time than data sets that base on code released years ago and, additionally, will enable researchers to investigate the relationship between developers' background and code smells' perception.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {342–347},
numpages = {6},
keywords = {software development, code smells, bad code smells, data set, software quality},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.5555/3398761.3398866,
author = {Muri\'{c}, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\'{e}s and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio},
title = {Massive Cross-Platform Simulations of Online Social Networks},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {895–903},
numpages = {9},
keywords = {collaborative platforms, ai agents, agent based simulation, online social networks, massive scale simulations},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1145/3234152.3234160,
author = {Ortu, Marco and Pinna, Andrea and Tonelli, Roberto and Marchesi, Michele and Bowes, David and Destefanis, Giuseppe},
title = {Angry-Builds: An Empirical Study of Affect Metrics and Builds Success on Github Ecosystem},
year = {2018},
isbn = {9781450364225},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3234152.3234160},
doi = {10.1145/3234152.3234160},
abstract = {Automatic and repeatable builds are an established software engineering practices for achieving continuous integration and continuous delivery processes. The building phase of modern software systems is an important part of the development process such that dedicated roles as "Release Engineer" are more and more required. Software development is a collaborative activity, and when multiple developers work on the same project, they will be changing a shared master development branch at overlapping intervals. This overlap occurs because developers create parallel branches for working and then merge these branches when features are completed. Continuous integration, CI, is a workflow strategy which helps ensure everyone\^{a}\u{A}undefineds changes will integrate with the current version of the project. This activity allows developers to catch bugs and reduce merge conflicts. Improving the building process leads to higher productivity and therefore shorter time to market, but understanding or measuring such a delicate phase is a big challenge. Open Source Communities provide valuable empirical data such as GitHub an Travis CI. These repositories represent a golden mine containing important data which can help researchers understanding the process behind the manufacturing of a software artifact. By analyzing Travis CI logs, we can directly connect a particular build with the development process behind it, not only regarding code changes but also regarding human activities, such as discussions about the implementation of a specific feature or bug resolution. Thanks to this information we can analyze the social activities of the build process enabling us to apply the same approach used for the development process.},
booktitle = {Proceedings of the 19th International Conference on Agile Software Development: Companion},
articleno = {35},
numpages = {2},
location = {Porto, Portugal},
series = {XP '18}
}

@inproceedings{10.1145/3236024.3275535,
author = {Foo, Darius and Chua, Hendy and Yeo, Jason and Ang, Ming Yi and Sharma, Asankhaya},
title = {Efficient Static Checking of Library Updates},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275535},
doi = {10.1145/3236024.3275535},
abstract = {Software engineering practices have evolved to the point where a developer writing a new application today doesn’t start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {791–796},
numpages = {6},
keywords = {call graphs, api diffs, library upgrades, semantic versioning, automated remediation},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ICPC.2017.3,
author = {Mostafa, Shaikh and Rodriguez, Rodney and Wang, Xiaoyin},
title = {NetDroid: Summarizing Network Behavior of Android Apps for Network Code Maintenance},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.3},
doi = {10.1109/ICPC.2017.3},
abstract = {Network access is one of the most common features of Android applications. Statistics show that almost 80% of Android apps ask for network permission and thus may have some network-related features. Android apps may access multiple servers to retrieve or post various types of data, and the code to handle such network features often needs to change as a result of server API evolution or the content change of data transferred. Since various network code is used by multiple features, maintenance of network-related code is often difficult because the code may scatter in different places in the code base, and it may not be easy to predict the impact of a code change to the network behavior of an Android app. In this paper, we present an approach to statically summarize network behavior from the byte code of Android apps. Our approach is based on string taint analysis, and generates a summary of network requests by statically estimating the possible values of network API arguments. To evaluate our technique, we applied our technique to top 500 android apps from the official Google Play market, and the result shows that our approach is able to summarize network behavior for most apps efficiently (averagely less than 50 second for an app). Furthermore, we performed an empirical evaluation on 8 real-world maintenance tasks extracted from bug reports of open-source Android projects on Github. The empirical evaluation shows that our technique is effective in locating relevant network code.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {165–175},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3357384.3358043,
author = {Shrestha, Prasha and Maharjan, Suraj and Arendt, Dustin and Volkova, Svitlana},
title = {Learning from Dynamic User Interaction Graphs to Forecast Diverse Social Behavior},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358043},
doi = {10.1145/3357384.3358043},
abstract = {Most of the existing graph analytics for understanding social behavior focuses on learning from static rather than dynamic graphs using hand-crafted network features or recently emerged graph embeddings learned independently from a downstream predictive task, and solving predictive (e.g., link prediction) rather than forecasting tasks directly. To address these limitations, we propose (1) a novel task -- forecasting user interactions over dynamic social graphs, and (2) a novel deep learning, multi-task, node-aware attention model that focuses on forecasting social interactions, going beyond recently emerged approaches for learning dynamic graph embeddings. Our model relies on graph convolutions and recurrent layers to forecast future social behavior and interaction patterns in dynamic social graphs. We evaluate our model on the ability to forecast the number of retweets and mentions of a specific news source on Twitter (focusing on deceptive and credible news sources) with R^2 of 0.79 for retweets and 0.81 for mentions. An additional evaluation includes model forecasts of user-repository interactions on GitHub and comments to a specific video on YouTube with a mean absolute error close to 2% and R^2 exceeding 0.69. Our results demonstrate that learning from connectivity information over time in combination with node embeddings yields better forecasting results than when we incorporate the state-of-the-art graph embeddings e.g., Node2Vec and DeepWalk into our model. Finally, we perform in-depth analyses to examine factors that influence model performance across tasks and different graph types e.g., the influence of training and forecasting windows as well as graph topological properties.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2033–2042},
numpages = {10},
keywords = {attention, social activity forecasting, dynamic graphs, node-aware attention},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1109/MSR.2019.00073,
author = {Markovtsev, Vadim and Long, Waren and Mougard, Hugo and Slavnov, Konstantin and Bulychev, Egor},
title = {STYLE-ANALYZER: Fixing Code Style Inconsistencies with Interpretable Unsupervised Algorithms},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00073},
doi = {10.1109/MSR.2019.00073},
abstract = {Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes.This paper introduces style-analyzer, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. style-analyzer is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. style-analyzer can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of style-analyzer by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. style-analyzer includes a web application to visualize how the rules are triggered. We release style-analyzer as a reusable and extendable open source software package on GitHub for the benefit of the community.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {468–478},
numpages = {11},
keywords = {interpretable machine learning, code style, assisted code review, decision tree forest},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3381991.3395396,
author = {Enck, William},
title = {Analysis of Access Control Enforcement in Android},
year = {2020},
isbn = {9781450375689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3381991.3395396},
doi = {10.1145/3381991.3395396},
abstract = {Over the past decade, the Android operating system install-base has proliferated to billions of devices, rivaling Microsoft Windows as a top computing platform. One of the most attractive aspects of Android is its vast collection of applications, available from application stores such as Google's Play Store. Developers have been drawn to Android due to its semantically-rich runtime APIs, which simplify the creation of third-party applications. Many of these APIs provide access to security- and privacy-sensitive information and resources such as the device's geographic location, audio recorded from the device's microphone, and the ability to send and receive SMS messages. In providing these APIs to third-party applications, the Android OS has implicitly taken responsibility for their protection, increasing its access control burden. As a result, current versions of Android have thousands of manually placed access control checks throughout the platform. The goal of this talk is to motivate the need for and utility of semi-automated tools to analyze and validate the access control checks that occur within Android's system code. The challenges are two-fold. First, analysis of Android's middleware code is more challenging than that of third-party applications, which has been studied in-depth over the past decade [3-5]. The code spans hundreds of system services, which are implemented in a combination of Java, C++, and C. The system services also have heavy inter-dependencies with one another, frequently invoking entry points in each other using Android's Binder inter-process communication (IPC) framework within the Linux kernel. Second, identifying what is an access control check is nontrivial. While there are well-known checks based on user-authorized permissions and Linux-layer user and group identifiers, system services also use an array of different service-specific checks that must be captured and modeled to assess the correctness of access control enforcement. In this talk, we will discuss these challenges in the context of two case studies. We will begin by discussing ACMiner [6], a tool designed to assess the correctness of access control checks in Android's middleware using consistency analysis. For each Binder entry point in each system service, ACMiner statically analyzes the code to identify all potential access control checks. To do so, ACMiner uses the names of methods and variables and the values of constant strings used in conditional statements to infer the security-semantics of each check on the control-flow path to instructions that throw a SecurityException. ACMiner then uses association rule mining to identify not only which entry points have inconsistent access control checks, but also to suggest what checks should be added. In applying ACMiner to the Android Open Source Project (AOSP), we found the suggestions to be invaluable when determining whether or not an inconsistency was a vulnerability. Next, we discuss the Android Re-Delegation Finder (ARF) [7]. When designing ACMiner, we optimized our static program analysis by terminating the control-flow analysis of an entry point when the execution reaches another entry point in the same or different system service. Upon further study, we found that entry points frequently call one another, often changing the protection domain of execution when they do (e.g., by explicitly clearing the calling identity, or calling the entry point of a system service executing in a different process). As with most modern operating systems, Android uses deputies (i.e., system services) to safely perform privileged functionality on behalf of third-party applications. Deputies are inherently necessary for the protection of system resources. However, by losing the calling identity, entry points to Android's system services can become confused deputies. ARF builds on the access control policy extracted by ACMiner to identify potential confused deputy vulnerabilities. Neither ACMiner or ARF were designed to eliminate all false positives. In a code-base as vast as Android, it is unrealistic to expect every nuance can be captured programmatically. Instead, ACMiner and ARF were designed to be semi-automated. Our goal is to drastically reduce the amount of time it takes for a security analyst with domain expertise to identify and fix vulnerabilities. Over the course of our research, we have applied our tools to AOSP versions~7, 8, and 9, discovering many vulnerabilities, seven of which have been assigned CVEs by Google. Moving forward, we hope that our tools can be used not only to identify new vulnerabilities, but also to aid regression testing as new versions of Android are released. Both tools have been made open-source and are hosted on Github [1,2].},
booktitle = {Proceedings of the 25th ACM Symposium on Access Control Models and Technologies},
pages = {117–118},
numpages = {2},
keywords = {access control, android security, static program analysis},
location = {Barcelona, Spain},
series = {SACMAT '20}
}

@inproceedings{10.1145/3041021.3054726,
author = {Beheshti, Seyed-Mehdi-Reza and Tabebordbar, Alireza and Benatallah, Boualem and Nouri, Reza},
title = {On Automating Basic Data Curation Tasks},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054726},
doi = {10.1145/3041021.3054726},
abstract = {Big data analytics is firmly recognized as a strategic priority for modern enterprises. At the heart of big data analytics lies the data curation process, consists of tasks that transform raw data (unstructured, semi-structured and structured data sources) into curated data, i.e. contextualized data and knowledge that is maintained and made available for use by end-users and applications. To achieve this, the data curation process may involve techniques and algorithms for extracting, classifying, linking, merging, enriching, sampling, and the summarization of data and knowledge. To facilitate the data curation process and enhance the productivity of researchers and developers, we identify and implement a set of basic data curation APIs and make them available as services to researchers and developers to assist them in transforming their raw data into curated data. The curation APIs enable developers to easily add features - such as extracting keyword, part of speech, and named entities such as Persons, Locations, Organizations, Companies, Products, Diseases, Drugs, etc.; providing synonyms and stems for extracted information items leveraging lexical knowledge bases for the English language such as WordNet; linking extracted entities to external knowledge bases such as Google Knowledge Graph and Wikidata; discovering similarity among the extracted information items, such as calculating similarity between string and numbers; classifying, sorting and categorizing data into various types, forms or any other distinct class; and indexing structured and unstructured data - into their data applications. These services can be accessed via a REST API, and the data is returned as a JSON file that can be integrated into data applications. The curation APIs are available as an open source project on GitHub.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {165–169},
numpages = {5},
keywords = {data curation, curation api, big data analytics},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3233547.3233628,
author = {Turk, Erdem and Arit, Turkan and Susus, Delikanli Mertcan and Ucar, Ilayda and Suzek, Baris E.},
title = {ProSetComp: A Platform for Protein Set Comparisons},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233628},
doi = {10.1145/3233547.3233628},
abstract = {The amount of data available in public bioinformatics resources and the complexity of user interfaces they are served through often challenges appreciation and effective utilization of these valuable resources. While education, documentation and training activities mitigate this problem, there is still a need to develop user interfaces to serve simple day-to-day needs of scientists. To this end, we developed ProSetComp; a simple web-based platform to create and compare protein sets, following a traditional software development process; from requirement analysis to implementation. First, we interviewed and collected user scenarios from wet lab scientists with seniority, research interests and backgrounds. Reviewing the user scenarios, we identified one high impact need that drove the development of ProSetComp; ability to 1) create protein sets by searching databases, 2) compare these protein sets in different dimensions such as functional domains, pathways, molecular functions and biological processes, and 3) visualize results graphically. Next, we collected and integrated necessary data from several bioinformatics resources including UniProt, Reactome, Gene Ontology and PFAM in a local relational database. Finally, we designed user interfaces that facilitate the creation of protein sets by using form-based query generators and exploring the relationship between created protein sets using tabular and graphical representations. The current internal release of the platform contains ~120 million protein entries. The user interface supports &gt;50 search criteria to create up-to four protein sets and comparison of these sets in four dimensions; protein domains, molecular functions, biological processes, and pathways. The commonality and differences between protein sets, along with tables, can be explored using novel user interface components such as Venn and UpSet diagrams. The first public release of ProSetComp (http://ceng.mu.edu.tr/labs/bioinfo/prosetcomp) is targeted for mid-August, 2018 and planned to be updated monthly thereafter. Upon public release, source code ProSetComp will become available through GitHub. The database content and user interface will be expanded as per community needs. The ProSetComp project is supported by The Scientific and Technological Research Council of Turkey (TUBITAK, Grant number: 216Z111).},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {519},
numpages = {1},
keywords = {comparative proteomics, protein bioinformatics, functional analysis},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3107411.3107414,
author = {Yao, Yao and Liu, Zheng and Singh, Satpreet and Wei, Qi and Ramsey, Stephen A.},
title = {CERENKOV: Computational Elucidation of the Regulatory Noncoding Variome},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107414},
doi = {10.1145/3107411.3107414},
abstract = {We describe a novel computational approach, CERENKOV (Computational Elucidation of the REgulatory NonKOding Variome), for discriminating regulatory single nucleotide polymorphisms (rSNPs) from non-regulatory SNPs within noncoding genetic loci. CERENKOV is specifically designed for recognizing rSNPs in the context of a post-analysis of a genome-wide association study (GWAS); it includes a novel accuracy scoring metric (which we call average rank, or AVGRANK) and a novel cross-validation strategy (locus-based sampling) that both correctly account for the "sparse positive bag" nature of the GWAS post-analysis rSNP recognition problem. We trained and validated CERENKOV using a reference set of 15,331 SNPs (the OSU17 SNP set) whose composition is based on selection criteria (linkage disequilibrium and minor allele frequency) that we designed to ensure relevance to GWAS post-analysis. CERENKOV is based on a machine-learning algorithm (gradient boosted decision trees) incorporating 246 SNP annotation features that we extracted from genomic, epigenomic, phylogenetic, and chromatin datasets. CERENKOV includes novel features based on replication timing and DNA shape. We found that tuning a classifier for AUPVR performance does not guarantee optimality for AVGRANK. We compared the validation performance of CERENKOV to nine other methods for rSNP recognition (including GWAVA, RSVP, DeltaSVM, DeepSEA, Eigen, and DANQ), and found that CERENKOV's validation performance is the strongest out of all of the classifiers that we tested, by both traditional global rank-based measures (〈AUPVR〉 = 0.506; 〈AUROC〉 = 0.855) and AVGRANK (〈AVGRANK〉 = 3.877). The source code for CERENKOV is available on GitHub and the SNP feature data files are available for download via the CERENKOV website.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {79–88},
numpages = {10},
keywords = {machine learning, SNV, noncoding, SNP, RSNP, GWAS},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1109/ESEM.2017.55,
author = {Campos, Eduardo C. and Maia, Marcelo A.},
title = {Common Bug-Fix Patterns: A Large-Scale Observational Study},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.55},
doi = {10.1109/ESEM.2017.55},
abstract = {[Background]: There are more bugs in real-world programs than human programmers can realistically address. Several approaches have been proposed to aid debugging. A recent research direction that has been increasingly gaining interest to address the reduction of costs associated with defect repair is automatic program repair. Recent work has shown that some kind of bugs are more suitable for automatic repair techniques. [Aim]: The detection and characterization of common bug-fix patterns in software repositories play an important role in advancing the field of automatic program repair. In this paper, we aim to characterize the occurrence of known bug-fix patterns in Java repositories at an unprecedented large scale. [Method]: The study was conducted for Java GitHub projects organized in two distinct data sets: the first one (i.e., Boa data set) contains more than 4 million bug-fix commits from 101,471 projects and the second one (i.e., Defects4J data set) contains 369 real bug fixes from five open-source projects. We used a domain-specific programming language called Boa in the first data set and conducted a manual analysis on the second data set in order to confront the results. [Results]: We characterized the prevalence of the five most common bug-fix patterns (identified in the work of Pan et al.) in those bug fixes. The combined results showed direct evidence that developers often forget to add IF preconditions in the code. Moreover, 76% of bug-fix commits associated with the IF-APC bug-fix pattern are isolated from the other four bug-fix patterns analyzed. [Conclusion]: Targeting on bugs that miss preconditions is a feasible alternative in automatic repair techniques that would produce a relevant payback.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {404–413},
numpages = {10},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1109/ICSE.2019.00076,
author = {Cabral, George G. and Minku, Leandro L. and Shihab, Emad and Mujahid, Suhaib},
title = {Class Imbalance Evolution and Verification Latency in Just-in-Time Software Defect Prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00076},
doi = {10.1109/ICSE.2019.00076},
abstract = {Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that rebuild classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency - the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {666–676},
numpages = {11},
keywords = {software defect prediction, online learning, ensembles, concept drift, verification latency, class imbalance},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3239235.3239244,
author = {Walkinshaw, Neil and Minku, Leandro},
title = {Are 20% of Files Responsible for 80% of Defects?},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239244},
doi = {10.1145/3239235.3239244},
abstract = {Background: Over the past two decades a mixture of anecdote from the industry and empirical studies from academia have suggested that the 80:20 rule (otherwise known as the Pareto Principle) applies to the relationship between source code files and the number of defects in the system: a small minority of files (roughly 20%) are responsible for a majority of defects (roughly 80%).Aims: This paper aims to establish how widespread the phenomenon is by analysing 100 systems (previous studies have focussed on between one and three systems), with the goal of whether and under what circumstances this relationship does hold, and whether the key files can be readily identified from basic metrics.Method: We devised a search criterion to identify defect fixes from commit messages and used this to analyse 100 active Github repositories, spanning a variety of languages and domains. We then studied the relationship between files, basic metrics (churn and LOC), and defect fixes.Results: We found that the Pareto principle does hold, but only if defects that incur fixes to multiple files count as multiple defects. When we investigated multi-file fixes, we found that key files (belonging to the top 20%) are commonly fixed alongside other much less frequently-fixed files. We found LOC to be poorly correlated with defect proneness, Code Churn was a more reliable indicator, but only for extremely high values of Churn.Conclusions: It is difficult to reliably identify the "most fixed" 20% of files from basic metrics. However, even if they could be reliably predicted, focussing on them would probably be misguided. Although fixes will naturally involve files that are often involved in other fixes too, they also tend to include other less frequently-fixed files.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {2},
numpages = {10},
keywords = {pareto principle, survey, defect distribution},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3377811.3380347,
author = {Yan, Jiwei and Liu, Hao and Pan, Linjie and Yan, Jun and Zhang, Jian and Liang, Bin},
title = {Multiple-Entry Testing of Android Applications by Constructing Activity Launching Contexts},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380347},
doi = {10.1145/3377811.3380347},
abstract = {Existing GUI testing approaches of Android apps usually test apps from a single entry. In this way, the marginal activities far away from the default entry are difficult to be covered. The marginal activities may fail to be launched due to requiring a great number of activity transitions or involving complex user operations, leading to uneven coverage on activity components. Besides, since the test space of GUI programs is infinite, it is difficult to test activities under complete launching contexts using single-entry testing approaches.In this paper, we address these issues by constructing activity launching contexts and proposing a multiple-entry testing framework. We perform an inter-procedural, flow-, context- and path-sensitive analysis to build activity launching models and generate complete launching contexts. By activity exposing and static analysis, we could launch activities directly under various contexts without performing long event sequence on GUI. Besides, to achieve an in-depth exploration, we design an adaptive exploration framework which supports the multiple-entry exploration and dynamically assigns weights to entries in each turn.Our approach is implemented in a tool called Fax, with an activity launching strategy Faxla and an exploration strategy Faxex. The experiments on 20 real-world apps show that Faxla can cover 96.4% and successfully launch 60.6% activities, based on which Faxex further achieves a relatively 19.7% improvement on method coverage compared with the most popular tool Monkey. Our tool also behaves well in revealing hidden bugs. Fax can trigger over seven hundred unique crashes, including 180 Errors and 539 Warnings, which is significantly higher than those of other tools. Among the 46 bugs reported to developers on Github, 33 have been fixed up to now.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {457–468},
numpages = {12},
keywords = {ICC, static analysis, Android app, multiple-entry testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3225151.3264698,
title = {Outstanding Doctoral Dissertation Award},
year = {2018},
isbn = {9781450358309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3225151.3264698},
doi = {10.1145/3225151.3264698},
abstract = {Dr. Jun-Yan Zhu is a pioneer in the use of modern machine learning in computer graphics. His dissertation is arguably the first to systematically attack the problem of natural image synthesis using deep neural networks. As such, his work has already had an enormous impact on the field, with several of his contributions, most notably CycleGAN, becoming widely-used tools not just for researchers in computer graphics and beyond, but also for visual artists.A key open problem in data-driven image synthesis is how to make sure that the synthesized image looks realistic, i.e., lies on the manifold of natural images? In Part I of his thesis, Zhu takes a discriminative approach to address a particular instance of this problem, training a classifier to estimate the realism of spliced image composites. Since it is difficult to obtain enough human-labeled training data to learn what looks realistic, he instead learned to classify between real images and automatically-generated composites, whether they look realistic or not. The surprising finding: resulting classifier can actually predict how realistic a new composite would look to a human. Moreover, this realism score can be used to improve the composite realism by iteratively updating the image via a learned transform. This work could be thought of as an early precursor to the conditional Generative Adversarial Network (GAN) architectures. He also developed a similar discriminative learning approach for improving the photograph aesthetics of portraits (SIGAsia'14).In Part II, Zhu takes the opposite, generative approach to modeling natural images and constrains the output of a photo editing tool to lie on this manifold. He built real-time data-driven exploration and editing interfaces based on both classic image averaging models (SIGGRAPH'14) and more recent Generative Adversarial Networks. The latter work and the associated software iGAN was the first use of GANs in a real-time application, and it contributed to the popularization of GANs in the community. In Part III, Zhu combines the lessons learned from his earlier work for developing a novel set of image-to-image translation algorithms. Of particular importance is the CycleGAN framework (ICCV'17), which revolutionized image-based computer graphics as a general-purpose framework for transferring the visual style from one set of images onto another, e.g., translating summer into winter and horses into zebras, generating real photographs from computer graphics renderings, etc. It was the first to show artistic collection style transfer (e.g., using all of Van Gogh paintings instead of only the "Starry Night"), and translating a painting into a photograph. In the short time since CycleGAN was published, it has already been applied to many different problems far beyond computer graphics, from generating synthetic training data (computer vision), to converting MRIs into CT scans (medical imaging), to applications in NLP and speech synthesis. In addition to his dissertation work, he also contributed to learning-based methods for interactive colorization (SIGGRAPH'17) and light field videography (SIGGRAPH'17).Apart from several well-cited papers in top graphics and vision venues, Zhu's work has an impact in other ways as well. His research has been repeatedly featured in the popular press, including New Yorker, Economist, Forbes, Wired, etc. Jun-Yun is also exemplary in facilitating the reproducibility of research and making it easy for researchers and practitioners to build on his contributions. He has open-sourced many of his projects and, as a sign of his impact, he has earned over 22,000 GitHub stars and 1,900 followers. Most impressively, his code has been used widely not just by researchers and developers, but also by visual artists (e.g. see #cycleGAN on Twitter).Bio: Jun-Yan Zhu received his B.E in Computer Sciences from Tsinghua University in 2012. He obtained his Ph.D. in Electrical Engineering and Computer Sciences from UC Berkeley in 2017 supervised by Alexei Efros, after spending five years at CMU and UC Berkeley. His Ph.D. work was supported by a Facebook Fellowship. Jun-Yan is currently a postdoctoral researcher at MIT CSAIL.},
booktitle = {ACM SIGGRAPH 2018 Awards},
articleno = {5},
numpages = {1},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/3318170.3318177,
author = {Sorensen, Tyler and Pai, Sreepathi and Donaldson, Alastair F.},
title = {Performance Evaluation of OpenCL Standard Support (and Beyond)},
year = {2019},
isbn = {9781450362306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318170.3318177},
doi = {10.1145/3318170.3318177},
abstract = {In this talk, we will discuss how support (or lack of it) for various OpenCL (OCL) features affects performance of graph applications executing on GPU platforms. Given that adoption of OCL features varies widely across vendors, our results can help quantify the performance benefits and potentially motivate the timely adoption of these OCL features.Our findings are drawn from the experience of developing an OCL backend for a state-of-the-art graph application DSL, IrGL, originally developed with a CUDA backend [1]. IrGL allows competitive algorithms for applications such as breadth-first-search, page-rank, and single-source-shortest-path to be written at a high level. A series of optimisations can then be applied by the compiler to generate OCL code. These user-selectable optimisations exercise various features of OCL: on one end of the spectrum, applications compiled without optimisations require only core OCL version 1.1 features; on the other end, a certain optimisation requires inter-workgroup forward progress guarantees, which are yet to be officially supported by OCL, but have been empirically validated and are relied upon e.g. to achieve global device-wide synchronisation [3]. Other optimisations require OCL features such as: fine-grained memory consistency guarantees (added in OCL 2.0) and subgroup primitives (added to core in OCL 2.1).Our compiler can apply 6 independent optimisations (Table 1), each of which requires an associated minimum version of OCL to be supported. Increased OCL support enables more and more optimisations: 2 optimisations are supported with OCL 1.x; 1 additional optimization with OCL 2.0; and a further 2 with OCL 2.1. Using OCL FP to denote v2.1 extended with forward progress guarantees (not officially supported at present), the last optimisation is enabled. We will discuss the OCL features required for each optimisation and the idioms in which the features are used. Use-case discussions of these features (e.g. memory consistency and subgroup primitives) are valuable as there appear to be very few open-source examples: a GitHub search yields only a small number of results.Our compiler enables us to carry out a large and controlled study, in which the performance benefit of various levels of OCL support can be evaluated. We gather runtime data exhaustively on all combinations across: all optimisations, 17 applications, 3 graph inputs, 6 different GPUs, spanning 4 vendors: Nvidia, AMD, Intel and ARM (Table 2).We show two notable results in this abstract: our first result, summarised in Figure 1, shows that all optimizations can be beneficial across a range of GPUs, despite significant architectural differences (e.g. subgroup size as seen in Table 2). This provides motivation that previous vendor specific approaches (e.g. for Nvidia) can be ported to OCL and achieve speedups on range of devices.Our second result, summarised in Figure 2, shows that if feature support is limited to OCL 2.0 (or below), the available optimisations (fg wg sz256) fail to achieve any speedups in over 70% of the chip/application/input benchmarks. If support for OCL 2.1 (adding the optimizations: sg coop-cv) is considered, this number drops to 60% but observed speedups are modest, rarely exceeding 2x. Finally, if forward progress guarantees are assumed (adding the oitergb optimization), speedups are observed in over half of the cases, including impressive speedups of over 14x for AMD and Intel GPUs. This provides compelling evidence for forward progress properties to be considered for adoption for a future OCL version.An extended version of this material can be found in [2, ch. 5].},
booktitle = {Proceedings of the International Workshop on OpenCL},
articleno = {8},
numpages = {2},
location = {Boston, MA, USA},
series = {IWOCL'19}
}

@proceedings{10.1145/2445196,
title = {SIGCSE '13: Proceeding of the 44th ACM Technical Symposium on Computer Science Education},
year = {2013},
isbn = {9781450318686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the proceedings of the 44th ACM Technical Symposium on Computer Science Education, or SIGCSE 2013, where you will find over one hundred papers as well as multiple other session formats that document the latest in computer science education: research, tool building, teaching, curriculum and philosophy. The theme of this year's symposium is "The Changing Face of Computing", and features a number of talks and sessions focused on how changes in computing technology and changes in student demographics requires a change in the way computing is taught.SIGCSE 2013's opening keynote session on Thursday will be different from anything seen at SIGCSE previously. It consists of "flash talks" (where several "all stars" in computing will be given five minutes to share 20 slides, each of which automatically advance) that answer the question: "What can WE do to change the face of computing?" Jane Margolis of UCLA will provide SIGCE 2013's closing keynote session on Saturday, where she'll examine how underrepresentation in computing relates to a larger educational crisis and issues we face as world citizens. In addition, during a special keynote session on Friday, Stanford's Provost (John Etchemendy) will discuss whether massively open online courses will change our universities as we know them or be a "flash in the pan".We are pleased to announce the winners of the two annual SIGCSE awards. Professor Michael K\"{o}lling of University of Kent will receive the SIGCSE award for Outstanding Contribution to Computer Science Education, and will provide Friday's keynote address. Henry Walker of Grinnell College will accept the lunch for free; SIGCSE Old Timers can purchase a ticket and (a) enjoy a delicious meal, (b) mentor a First Timer, and (c) listen to Professor Walker's talk.)Symposium statistics are presented in the following table. We thank the authors, reviewers, and Program Committee members whose enormous and vital service generated this program. This year's program includes the usual wide selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition, as well as some unusual offerings, such as the Codebreaker dramadocumentary on Alan Turing's remarkable and tragic life story, a puzzle extravaganza with a raffle for those who complete it, and a CSTA K- 13 Computing Teachers Workshop. Our exhibit hall features a number of exhibitors showcasing the latest in hardware, software tools, textbooks and educational programs and research. We also continue to offer accessibility at SIGCSE 2013 for the deaf and hard of hearing.We are excited about the variety of pre-symposium events that will exist at SIGCSE 2013. As of the press deadline for this overview, meetings on the following topics will occur on Wednesday: Managing the Academic Career for Women, Open Source Software, Computational Thinking Through Computing and Music, CSAB Computing Accreditation Workshop, Git &amp; GitHub Foundations for Educators, SIGCAS Share and Learn, Using the GENI Networking Testbed, Exploring the Next Generation of Scratch, and Integrating Computing Ethics into the Curriculum.},
location = {Denver, Colorado, USA}
}

@proceedings{10.1145/2676723,
title = {SIGCSE '15: Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGCSE 2015, the 46th ACM Technical Symposium on Computer Science Education. This year's symposium presents the latest advances in computer science education in a variety of formats: papers, posters, panels, special sessions, workshops, birds of a feather meetings, and, new this year, demonstrations and lightning talks. This year's symposium theme is "Keep Connected. Keep Committed. Keep Computing." We are asking our attendees to consider how they can use the conference to keep connected to each other and the field, to keep committed to the cause of computing education, and most fundamentally, to keep computing and to demonstrate to all our students how exciting this field truly is.We are delighted to have Jessica Hodgins of Carnegie Mellon University and Disney Research giving SIGCSE 2015's opening plenary address on Thursday. Her work at Carnegie Mellon and Disney Research will demonstrate what new and exciting things will keep us computing in the coming years. Keith Hampton of Rutgers University is speaking at our Saturday luncheon. His work in social media demonstrates how keeping us connected is an important part of building strong relationships. Finally, it is our pleasure to announce the recipients of the two annual SIGCSE awards, the recipients of which demonstrate keeping committed to the field of computing education and to the SIGCSE organization and community. Frank Young of Rose-Hulman Institute of Technology will receive the SIGCSE Award for Lifetime Service to the Computer Science Education Community, and will speak at our First Timer's Lunch on Thursday. (SIGCSE First Timers will receive their lunch for free. SIGCSE Old Timers are encouraged to purchase a ticket, join us for lunch, meet some First Timers, and recognize Frank's contributions.) Mark Allen Weiss of Florida International University is the recipient of the SIGCSE Award for Outstanding Contributions to Computer Science Education. Mark will give the plenary address on Friday.As noted above, we are very excited to be introducing two new tracks for SIGCSE 2015. A Demos track will be presented in the exhibit hall during breaks, and a session dedicated to Lightning Talks will take place on Friday afternoon at 3:45pm.Symposium statistics are presented in the following table. This year's program includes the usual wide selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition, as well as another puzzle challenge. Our exhibit hall features a number of exhibitors showcasing the latest in hardware, software tools, textbooks and educational programs and research.Proposal type Paper - Accepted - 105 Received - 289 Acceptance Rate - 36%Panel - Accepted - 10 Received - 18 Acceptance Rate - 56%Special Session - Accepted - 13 Received - 25 Acceptance Rate - 52%Workshop - Accepted - 30 Received - 71 Acceptance Rate - 42%Poster - Accepted - 51 Received - 117 Acceptance Rate - 44%Birds of a Feather - Accepted - 38 Received - 55 Acceptance Rate - 69%Demos - Accepted - 10 Received - 32 Acceptance Rate - 31%Lightning Talks - Accepted - 11 Received - 26 Acceptance Rate - 42%We encourage you to participate in our SIGCSE 2015 Pre-symposium Events. As of the publication deadline for this overview, meetings on the following topics will occur on Wednesday: ACM SIGCAS Symposium on Computing for the Social Good: Educational Practices, CSTeachingTips.org: Tip-A-Thon, GENI in your Classroom, Git and GitHub: Foundations for Educators, LittleFe Build-Out, Managing the Academic Career for Women Faculty in Undergraduate Computing Programs, SIGCSE 2015 Department Chairs Roundtable, and Teaching to Diversity in Computer Science.},
location = {Kansas City, Missouri, USA}
}

