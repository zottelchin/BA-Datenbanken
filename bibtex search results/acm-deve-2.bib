@inproceedings{10.1109/ASE.2019.00064,
author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Goues, Claire Le and Neubig, Graham and Vasilescu, Bogdan},
title = {DIRE: A Neural Approach to Decompiled Identifier Naming},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00064},
doi = {10.1109/ASE.2019.00064},
abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {628–639},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3292500.3330699,
author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
title = {Pythia: AI-Assisted Code Completion System},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330699},
doi = {10.1145/3292500.3330699},
abstract = {In this paper, we propose a novel end-to-end approach for AI-assisted code completion called Pythia. It generates ranked lists of method and API recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code IDE. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 ms. We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices. The offline evaluation results obtained on 2700 Python open source software GitHub repositories show a top-5 accuracy of 92%, surpassing the baseline models by 20% averaged over classes, for both intra and cross-project settings.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2727–2735},
numpages = {9},
keywords = {naturalness of software, neural networks, code completion},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3321707.3321842,
author = {Hansen, Nikolaus},
title = {A Global Surrogate Assisted CMA-ES},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321842},
doi = {10.1145/3321707.3321842},
abstract = {We explore the arguably simplest way to build an effective surrogate fitness model in continuous search spaces. The model complexity is linear or diagonal-quadratic or full quadratic, depending on the number of available data. The model parameters are computed from the Moore-Penrose pseudoinverse. The model is used as a surrogate fitness for CMA-ES if the rank correlation between true fitness and surrogate value of recently sampled data points is high. Otherwise, further samples from the current population are successively added as data to the model. We empirically compare the IPOP scheme of the new model assisted lq-CMA-ES with a variety of previously proposed methods and with a simple portfolio algorithm using SLSQP and CMA-ES. We conclude that a global quadratic model and a simple portfolio algorithm are viable options to enhance CMA-ES. The model building code is available as part of the pycma Python module on Github and PyPI.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {664–672},
numpages = {9},
keywords = {evolution strategies, quadratic model, CMA-ES, covariance matrix adaptation, surrogate},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3196321.3196334,
author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
title = {Deep Code Comment Generation},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196334},
doi = {10.1145/3196321.3196334},
abstract = {During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {200–210},
numpages = {11},
keywords = {deep learning, program comprehension, comment generation},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3180155.3180209,
author = {Trockman, Asher and Zhou, Shurui and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {Adding Sparkle to Social Coding: An Empirical Study of Repository Badges in the <i>Npm</i> Ecosystem},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180209},
doi = {10.1145/3180155.3180209},
abstract = {In fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {511–522},
numpages = {12},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1109/MSR.2017.46,
author = {Madeyski, Lech and Kawalerowicz, Marcin},
title = {Continuous Defect Prediction: The Idea and a Related Dataset},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.46},
doi = {10.1109/MSR.2017.46},
abstract = {We would like to present the idea of our Continuous Defect Prediction (CDP) research and a related dataset that we created and share. Our dataset is currently a set of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data we mine from software repositories. Our dataset embraces 1265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset we use TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories. We extend this data to a file change level and calculate the software process metrics that may be used, for example, as features to predict risky software changes that could break the build if committed to a repository with CI enabled.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {515–518},
numpages = {4},
keywords = {defect prediction, continuous defect prediction, mining software repositories, open science, software repository},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1109/ICPC.2017.36,
author = {Lin, Bin and Ponzanelli, Luca and Mocci, Andrea and Bavota, Gabriele and Lanza, Michele},
title = {On the Uniqueness of Code Redundancies},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.36},
doi = {10.1109/ICPC.2017.36},
abstract = {Code redundancy widely occurs in software projects. Researchers have investigated the existence, causes, and impacts of code redundancy, showing that it can be put to good use, for example in the context of code completion. When analyzing source code redundancy, previous studies considered software projects as sequences of tokens, neglecting the role of the syntactic structures enforced by programming languages. However, differences in the redundancy of such structures may jeopardize the performance of applications leveraging code redundancy.We present a study of the redundancy of several types of code constructs in a large-scale dataset of active Java projects mined from GitHub, unveiling that redundancy is not uniform and mainly resides in specific code constructs. We further investigate the implications of the locality of redundancy by analyzing the performance of language models when applied to code completion. Our study discloses the perils of exploiting code redundancy without taking into account its strong locality in specific code constructs.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {121–131},
numpages = {11},
keywords = {code redundancy, empirical study, code completion},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/2652524.2652565,
author = {Onoue, Saya and Hata, Hideaki and Matsumoto, Kenichi},
title = {Software Population Pyramids: The Current and the Future of OSS Development Communities},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652565},
doi = {10.1145/2652524.2652565},
abstract = {Context: Since human power is an essential resource, the number of contributors in a software development community is one of the health indicators of an open source software (OSS) project. For maintaining and increasing the populations in software development communities, both attracting new contributors and retaining existing contributors are important. Goal: Our goal is understanding the current status of projects' population, especially the different experienced contributors' composition of the projects. Method: We propose software population pyramids, a graphical illustration of the distribution of various experience groups in a software development community. Results: From the study with OSS projects in GitHub, we found that the shapes of software population pyramids varies depending on the current status of OSS development communities. Conclusions: This paper present a software population pyramid of the distribution of various experience groups in a software community population. Our results can be considered as predictors of the near future of a project.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {34},
numpages = {4},
keywords = {OSS, population pyramid, software development community},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.5555/3358807.3358819,
author = {Bijlani, Ashish and Ramachandran, Umakishore},
title = {Extension Framework for File Systems in User Space},
year = {2019},
isbn = {9781939133038},
publisher = {USENIX Association},
address = {USA},
abstract = {User file systems offer numerous advantages over their in-kernel implementations, such as ease of development and better system reliability. However, they incur heavy performance penalty. We observe that existing user file system frameworks are highly general; they consist of a minimal interposition layer in the kernel that simply forwards all low-level requests to user space. While this design offers flexibility, it also severely degrades performance due to frequent kernel-user context switching.This work introduces EXTFUSE, a framework for developing extensible user file systems that also allows applications to register "thin" specialized request handlers in the kernel to meet their specific operative needs, while retaining the complex functionality in user space. Our evaluation with two FUSE file systems shows that EXTFUSE can improve the performance of user file systems with less than a few hundred lines on average. EXTFUSE is available on GitHub.},
booktitle = {Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference},
pages = {121–134},
numpages = {14},
location = {Renton, WA, USA},
series = {USENIX ATC '19}
}

@inproceedings{10.1109/ICSE-Companion.2019.00049,
author = {Wang, Kaiyuan and Sullivan, Allison and Khurshid, Sarfraz},
title = {ARepair: A Repair Framework for Alloy},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00049},
doi = {10.1109/ICSE-Companion.2019.00049},
abstract = {Researchers have proposed many automated program repair techniques for imperative languages, e.g. Java. However, little work has been done to repair programs written in declarative languages, e.g. Alloy. We proposed ARepair, the first automated program repair technique for faulty Alloy models. ARepair takes as input a faulty Alloy model and a set of tests that capture the desired model properties, and produces a fixed model that passes all tests. ARepair uses tests written for the recently introduced AUnit framework, which provides a notion of unit testing for Alloy models. In this paper, we describes our Java implementation of ARepair, which is a command-line tool, released as an open-source project on GitHub. Our experimental results show that ARepair is able to fix 28 out of 38 real-world faulty models we collected. The demo video for ARepair can be found at https://youtu.be/436drvWvbEU.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {103–106},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3219819.3219978,
author = {Yang, Tong and Gong, Junzhi and Zhang, Haowei and Zou, Lei and Shi, Lei and Li, Xiaoming},
title = {HeavyGuardian: Separate and Guard Hot Items in Data Streams},
year = {2018},
isbn = {9781450355520},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219819.3219978},
doi = {10.1145/3219819.3219978},
abstract = {Data stream processing is a fundamental issue in many fields, such as data mining, databases, network traffic measurement. There are five typical tasks in data stream processing: frequency estimation, heavy hitter detection, heavy change detection, frequency distribution estimation, and entropy estimation. Different algorithms are proposed for different tasks, but they seldom achieve high accuracy and high speed at the same time. To address this issue, we propose a novel data structure named HeavyGuardian. The key idea is to intelligently separate and guard the information of hot items while approximately record the frequencies of cold items. We deploy HeavyGuardian on the above five typical tasks. Extensive experimental results show that HeavyGuardian achieves both much higher accuracy and higher speed than the state-of-the-art solutions for each of the five typical tasks. The source codes of HeavyGuardian and other related algorithms are available at GitHub.},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2584–2593},
numpages = {10},
keywords = {data sturcture, probabilistic and approximate data, data stream processing},
location = {London, United Kingdom},
series = {KDD '18}
}

@inproceedings{10.1145/3229762.3229763,
author = {Gong, Jiong and Shen, Haihao and Zhang, Guoming and Liu, Xiaoli and Li, Shane and Jin, Ge and Maheshwari, Niharika and Fomenko, Evarist and Segal, Eden},
title = {Highly Efficient 8-Bit Low Precision Inference of Convolutional Neural Networks with IntelCaffe},
year = {2018},
isbn = {9781450359238},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229762.3229763},
doi = {10.1145/3229762.3229763},
abstract = {High throughput and low latency inference of deep neural networks are critical for the deployment of deep learning applications. This paper presents the efficient inference techniques of IntelCaffe, the first Intel(R) optimized deep learning framework that supports efficient 8-bit low precision inference and model optimization techniques of convolutional neural networks on Intel(R) Xeon(R) Scalable Processors. The 8-bit optimized model is automatically generated with a calibration process from FP32 model without the need of fine-tuning or retraining. We show that the inference throughput and latency with ResNet-50, Inception-v3 and SSD are improved by 1.38X-2.9X and 1.35X-3X respectively with neglectable accuracy loss from IntelCaffe FP32 baseline and by 56X-75X and 26X-37X from BVLC Caffe. All these techniques have been open-sourced on IntelCaffe GitHub (https://github.com/intel/caffe), and the artifact is provided to reproduce the result on Amazon AWS Cloud.},
booktitle = {Proceedings of the 1st on Reproducible Quality-Efficient Systems Tournament on Co-Designing Pareto-Efficient Deep Learning},
articleno = {2},
numpages = {9},
keywords = {Model Optimization, Convolutional Neural Network, Deep Learning, Intel Caffe},
location = {Williamsburg, VA, USA},
series = {ReQuEST '18}
}

@inproceedings{10.1007/978-3-319-21155-8_4,
author = {Criado, Javier and Mart\'{\i}nez, Salvador and Iribarne, Luis and Cabot, Jordi},
title = {Enabling the Reuse of Stored Model Transformations Through Annotations},
year = {2015},
isbn = {9783319211541},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-21155-8_4},
doi = {10.1007/978-3-319-21155-8_4},
abstract = {With the increasing adoption of MDE, model transformations, one of its core concepts together with metamodeling, stand out as a valuable asset. Therefore, a mechanism to annotate and store existing model transformations appears as a critical need for their efficient exploitation and reuse. Unfortunately, although several reuse mechanisms have been proposed for software artifacts in general and models in particular, none of them is specially tailored to the domain of model transformations. In order to fill this gap, we present here such a mechanism. Our approach is composed by two elements 1 a new DSL specially conceived for describing model transformations in terms of their functional and non-functional properties 2 a semi-automatic process for annotating and querying repositories of model transformations using as criteria the properties of our DSL. We validate the feasibility of our approach through a prototype implementation that integrates our approach in a GitHub repository.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Model Transformations - Volume 9152},
pages = {43–58},
numpages = {16}
}

@inproceedings{10.1145/3379597.3387484,
author = {Cor\`{o}, Federico and Verdecchia, Roberto and Cruciani, Emilio and Miranda†, Breno and Bertolino, Antonia},
title = {JTeC: A Large Collection of Java Test Classes for Test Code Analysis and Processing},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387484},
doi = {10.1145/3379597.3387484},
abstract = {The recent push towards test automation and test-driven development continues to scale up the dimensions of test code that needs to be maintained, analysed, and processed side-by-side with production code. As a consequence, on the one side regression testing techniques, e.g., for test suite prioritization or test case selection, capable to handle such large-scale test suites become indispensable; on the other side, as test code exposes own characteristics, specific techniques for its analysis and refactoring are actively sought. We present JTeC, a large-scale dataset of test cases that researchers can use for benchmarking the above techniques or any other type of tool expressly targeting test code. JTeC collects more than 2.5M test classes belonging to 31K+ GitHub projects and summing up to more than 430 Million SLOCs of ready-to-use real-world test code.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {578–582},
numpages = {5},
keywords = {Software Testing, Large Scale, Test Suite, Java, GitHub},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3306446.3340827,
author = {Chua, Bee Bee and Zhang, Ying},
title = {Predicting Open Source Programming Language Repository File Survivability from Forking Data},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340827},
doi = {10.1145/3306446.3340827},
abstract = {Very few studies have looked at repositories' programming language survivability in response to forking conditions. A high number of repository programming languages does not alone ensure good forking performance. To address this issue and assist project owners in adopting the right programming language, it is necessary to predict programming language survivability from forking in repositories. This paper therefore addresses two related questions: are there statistically meaningful patterns within repository data and, if so, can these patterns be used to predict programming language survival? To answer these questions we analysed 47,000 forking instances in 1000 GitHub projects. We used Euclidean distance applied in the K-Nearest Neighbour algorithm to predict the distance between repository file longevity and forking conditions. We found three pattern types ('once-only', intermittent or steady) and propose reasons for short-lived programming languages.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {2},
numpages = {8},
keywords = {open source, forking, programming language, survivability, prediction, euclidean distance, K-nearest neighbour},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1145/3338906.3338918,
author = {Zhou, Shurui and Vasilescu, Bogdan and K\"{a}stner, Christian},
title = {What the Fork: A Study of Inefficient and Efficient Forking Practices in Social Coding},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338918},
doi = {10.1145/3338906.3338918},
abstract = {Forking and pull requests have been widely used in open-source communities as a uniform development and contribution mechanism, giving developers the flexibility to modify their own fork without affecting others before attempting to contribute back. However, not all projects use forks efficiently; many experience lost and duplicate contributions and fragmented communities. In this paper, we explore how open-source projects on GitHub differ with regard to forking inefficiencies. First, we observed that different communities experience these inefficiencies to widely different degrees and interviewed practitioners to understand why. Then, using multiple regression modeling, we analyzed which context factors correlate with fewer inefficiencies.We found that better modularity and centralized management are associated with more contributions and a higher fraction of accepted pull requests, suggesting specific best practices that project maintainers can adopt to reduce forking-related inefficiencies in their communities.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {350–361},
numpages = {12},
keywords = {Fork-based development, Modularity, Collaboration efficiency, Centralized Management},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2896825.2896827,
author = {Ringlstetter, Andreas and Scherzinger, Stefanie and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Data Model Evolution Using Object-NoSQL Mappers: Folklore or State-of-the-Art?},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896827},
doi = {10.1145/2896825.2896827},
abstract = {In big data software engineering, the schema flexibility of NoSQL document stores is a major selling point: When the document store itself does not actively manage a schema, the data model is maintained within the application. Just like object-relational mappers for relational databases, object-NoSQL mappers are part of professional software development with NoSQL document stores. Some mappers go beyond merely loading and storing Java objects: Using dedicated evolution annotations, developers may conveniently add, remove, or rename attributes from stored objects, and also conduct more complex transformations. In this paper, we analyze the dissemination of this technology in Java open source projects. While we find evidence on GitHub that evolution annotations are indeed being used, developers do not employ them so much for evolving the data model, but to solve different tasks instead. Our observations trigger interesting questions for further research.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {33–36},
numpages = {4},
keywords = {object-NoSQL mappers, data model evolution},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1007/978-3-319-09195-2_5,
author = {L\"{a}mmel, Ralf and Varanovich, Andrei},
title = {Interpretation of Linguistic Architecture},
year = {2014},
isbn = {9783319091945},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-09195-2_5},
doi = {10.1007/978-3-319-09195-2_5},
abstract = {The megamodeling language MegaL is designed to model the linguistic architecture of software systems: the relationships between software artifacts (e.g., files), software languages (e.g., programming languages), and software technologies (e.g., code generators) used in a system. The present paper delivers a form of interpretation for such megamodels: resolution of megamodel elements to resources (e.g., system artifacts) and evaluation of relationships, subject to designated programs (such as pluggable 'tools' for checking). Interpretation reduces concerns about the adequacy and meaning of megamodels, as it helps to apply the megamodels to actual systems. We leverage Linked Data principles for surfacing resolved megamodels by linking, for example, artifacts to GitHub repositories or concepts to DBpedia resources. We provide an executable specification (i.e., semantics) of interpreted megamodels and we discuss an implementation in terms of an object-oriented framework with dynamically loaded plugins.},
booktitle = {Proceedings of the 10th European Conference on Modelling Foundations and Applications - Volume 8569},
pages = {67–82},
numpages = {16},
keywords = {Linked Data, software language, interpretation, software technology, ontology, megamodel, technological space}
}

@inproceedings{10.5555/3400397.3400573,
author = {Mehdi, Nabeel and Starly, Binil},
title = {A Simulator Testbed for MT-Connect Based Machines in a Scalable and Federated Multi-Enterprise Environment},
year = {2019},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {The emergence and steady adoption of machine communication protocols like the MTConnect are steering the manufacturing sector towards greater machine interoperability, higher operational productivity, substantial cost savings with advanced decision-making capabilities at the shop-floor level. MTConnect GitHub repository and NIST Smart Manufacturing Systems (SMS) Test Bed are two major resources for collecting data from CNC machines. However, these tools would be insufficient and protractive in Modeling &amp; Simulation (M&amp;S) scenarios where spawning hundreds of MTConnect agents and thousands of adapters with real-time virtual machining is necessary for advancing research in the digital supply chain. This paper introduces a flexible simulator testbed of multiple MTConnect agents and adapters for simulating Levels 0 &amp; 1 of the ISA-95 framework and help support R&amp;D activities in complex multi-enterprise supply chain scenarios. To the best knowledge of the authors, there is no publicly accessible multi-enterprise MTConnect testbed yet.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {2178–2189},
numpages = {12},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.1145/2639490.2639506,
author = {Murgia, Alessandro and Concas, Giulio and Tonelli, Roberto and Ortu, Marco and Demeyer, Serge and Marchesi, Michele},
title = {On the Influence of Maintenance Activity Types on the Issue Resolution Time},
year = {2014},
isbn = {9781450328982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2639490.2639506},
doi = {10.1145/2639490.2639506},
abstract = {The ISO/IEC 14764 standard specifies four types of software maintenance activities spanning the different motivations that software engineers have while performing changes to an existing software system. Undoubtedly, this classification has helped in organizing the workflow within software projects, however for planning purposes the relative time differences for the respective tasks remains largely unexplored.In this empirical study, we investigate the influence of the maintenance type on issue resolution time. From GitHub's issue repository, we analyze more than 14000 issue reports taken from 34 open source projects and classify them as corrective, adaptive, perfective or preventive maintenance. Based on this data, we show that the issue resolution time depends on the maintenance type. Moreover, we propose a statistical model to describe the distribution of the issue resolution time for each type of maintenance activity. Finally, we demonstrate the usefulness of this model for scheduling the maintenance workload.},
booktitle = {Proceedings of the 10th International Conference on Predictive Models in Software Engineering},
pages = {12–21},
numpages = {10},
keywords = {software maintenance, empirical software engineering, issue repository, issue resolution-time},
location = {Turin, Italy},
series = {PROMISE '14}
}

@inproceedings{10.1145/2642803.2642811,
author = {Aarnoutse, Floor and Renes, Cassandra and Snijders, Remco and Jansen, Slinger},
title = {The Reality of an Associate Model: Comparing Partner Activity in the Eclipse Ecosystem},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642811},
doi = {10.1145/2642803.2642811},
abstract = {Two determinants of software ecosystem health are productivity of and value creation by the actors in the ecosystem. While keystone players use partnership models to orchestrate actors, the relationship between the type of partnership and activity has not been studied. To address this gap, we have researched the partnership model of the Eclipse Ecosystem and the activity of different types of partners. We have used Eclipse Dash and GitHub to gather data about the activity of Eclipse partners. The results show that a higher level of membership is related to more activity. However, it is also observed that non-member companies are more active than associate members, which suggests that Eclipse can and should improve their partnership model by motivating associate members and incorporating active non-member companies. In addition, other software ecosystems could use these results and implications to improve their own partnership models.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {8},
numpages = {6},
keywords = {Eclipse, Software Ecosystem, Associate model, Partner activity, Membership},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/3411502.3418429,
author = {Li, Kaiyuan and Woo, Maverick and Jia, Limin},
title = {On the Generation of Disassembly Ground Truth and the Evaluation of Disassemblers},
year = {2020},
isbn = {9781450380898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411502.3418429},
doi = {10.1145/3411502.3418429},
abstract = {When a software transformation or software security task needs to analyze a given program binary, the first step is often disassembly. Since many modern disassemblers have become highly accurate on many binaries, we believe reliable disassembler benchmarking requires standardizing the set of binaries used and the disassembly ground truth about these binaries. This paper presents (i) a first version of our work-in-progress disassembly benchmark suite, which comprises $879$ binaries from diverse projects compiled with multiple compilers and optimization settings, and (ii) a novel disassembly ground truth generator leveraging the notion of "listing files'', which has broad support by clang, gcc, icc, and msvc. In additional, it presents our evaluation of four prominent open-source disassemblers using this benchmark suite and a custom evaluation system. Our entire system and all generated data are maintained openly on GitHub to encourage community adoption.},
booktitle = {Proceedings of the 2020 ACM Workshop on Forming an Ecosystem Around Software Transformation},
pages = {9–14},
numpages = {6},
keywords = {benchmark suite, ground-truth generation, disassembly},
location = {Virtual Event, USA},
series = {FEAST'20}
}

@inproceedings{10.1145/3368089.3418541,
author = {Wang, Zhendong},
title = {Assisting the Elite-Driven Open Source Development through Activity Data},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418541},
doi = {10.1145/3368089.3418541},
abstract = {Elite developers, who own the administrative privileges for a project, maintain a diverse profile of contributing activities, and drive the development of open source software (OSS). To advance our understanding and further support the OSS community, I present a fresh approach to investigate developers’ public activities from the fine-grained event data provided by GitHub. Further, I develop this approach into an analysis framework for collecting, modeling, and analyzing elite developers’ online contributing activities. Employing this framework, I have conducted empirical studies on various OSS projects and ecosystems to characterize elite developers’ full-spectrum activities and their dynamics, and also unveil relationships between their effort allocation and projects’ technical outcomes. Finally, I propose to design and implement a toolset based on this framework and my results to date, which supports individual developers’ decision-making and assists their routine workflows with automation.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1670–1673},
numpages = {4},
keywords = {Elite developers, open source software, project outcomes},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1007/978-3-030-31901-4_2,
author = {Kao, Po-Yu and Zhang, Angela and Goebel, Michael and Chen, Jefferson W. and Manjunath, B. S.},
title = {Predicting Fluid Intelligence of Children Using T1-Weighted MR Images and a StackNet},
isbn = {978-3-030-31900-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31901-4_2},
doi = {10.1007/978-3-030-31901-4_2},
abstract = {In this work, we utilize T1-weighted MR images and StackNet to predict fluid intelligence in adolescents. Our framework includes feature extraction, feature normalization, feature denoising, feature selection, training a StackNet, and predicting fluid intelligence. The extracted feature is the distribution of different brain tissues in different brain parcellation regions. The proposed StackNet consists of three layers and 11 models. Each layer uses the predictions from all previous layers including the input layer. The proposed StackNet is tested on a public benchmark Adolescent Brain Cognitive Development Neurocognitive Prediction Challenge 2019 and achieves a mean squared error of 82.42 on the combined training and validation set with 10-fold cross-validation. The proposed StackNet achieves a mean squared error of 94.25 on the testing data. The source code is available on GitHub ().},
booktitle = {Adolescent Brain Cognitive Development Neurocognitive Prediction},
pages = {9–16},
numpages = {8},
keywords = {Machine learning, StackNet, T1-weighted MRI, Fluid intelligence (Gf)}
}

@inproceedings{10.1145/3211346.3211353,
author = {Sachdev, Saksham and Li, Hongyu and Luan, Sifei and Kim, Seohyun and Sen, Koushik and Chandra, Satish},
title = {Retrieval on Source Code: A Neural Code Search},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211353},
doi = {10.1145/3211346.3211353},
abstract = {Searching over large code corpora can be a powerful productivity tool for both beginner and experienced developers because it helps them quickly find examples of code related to their intent. Code search becomes even more attractive if developers could express their intent in natural language, similar to the interaction that Stack Overflow supports. In this paper, we investigate the use of natural language processing and information retrieval techniques to carry out natural language search directly over source code, i.e. without having a curated Q&amp;A forum such as Stack Overflow at hand. Our experiments using a benchmark suite derived from Stack Overflow and GitHub repositories show promising results. We find that while a basic word–embedding based search procedure works acceptably, better results can be obtained by adding a layer of supervision, as well as by a customized ranking strategy.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {31–41},
numpages = {11},
keywords = {code search, word-embedding, TF-IDF},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@inproceedings{10.1145/2876034.2893422,
author = {Martin, Taylor and Brasiel, Sarah and Jeong, Soojeong and Close, Kevin and Lawanto, Kevin and Janisciewcz, Phil},
title = {Macro Data for Micro Learning: Developing the FUN! Tool for Automated Assessment of Learning},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2893422},
doi = {10.1145/2876034.2893422},
abstract = {Digital learning environments are becoming more common for students to engage in during and outside of school. With the immense amount of data now available from these environments, researchers need tools to process, manage, and analyze the data. Current methods used by many education researchers are inefficient; however, without data science experience tools used in other professions are not accessible. In this paper, we share about a tool we created called the Functional Understanding Navigator! (FUN! Tool). We have used this tool for different research projects which has allowed us the opportunity to (1) organize our workflow process from start to finish, (2) record log data of all of our analyses, and (3) provide a platform to share our analyses with others through GitHub. This paper extends and improves existing work in educational data mining and learning analytics.},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {233–236},
numpages = {4},
keywords = {assessment, digital learning environments, micro learning, educational data mining},
location = {Edinburgh, Scotland, UK},
series = {L@S '16}
}

@inproceedings{10.5555/2820690.2820696,
author = {Bass, Len and Holz, Ralph and Rimba, Paul and Tran, An Binh and Zhu, Liming},
title = {Securing a Deployment Pipeline},
year = {2015},
publisher = {IEEE Press},
abstract = {At the RELENG 2014 Q&amp;A, the question was asked, "What is your greatest concern?" and the response was "someone subverting our deployment pipeline". That is the motivation for this paper. We explore what it means to subvert a pipeline and provide several different scenarios of subversion. We then focus on the issue of securing a pipeline. As a result, we provide an engineering process that is based on having trusted components mediate access to sensitive portions of the pipeline from other components, which can remain untrusted. Applying our process to a pipeline we constructed involving Chef, Jenkins, Docker, Github, and AWS, we find that some aspects of our process result in easy to make changes to the pipeline, whereas others are more difficult. Consequently, we have developed a design that hardens the pipeline, although it does not yet completely secure it.},
booktitle = {Proceedings of the Third International Workshop on Release Engineering},
pages = {4–7},
numpages = {4},
keywords = {continuous deployment, DevOps, supply chain},
location = {Florence, Italy},
series = {RELENG '15}
}

@inproceedings{10.1109/COMPSAC.2013.55,
author = {Bissyand\'{e}, Tegawend\'{e} F. and Thung, Ferdian and Lo, David and Jiang, Lingxiao and R\'{e}veill\`{e}re, Laurent},
title = {Popularity, Interoperability, and Impact of Programming Languages in 100,000 Open Source Projects},
year = {2013},
isbn = {9780769549866},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/COMPSAC.2013.55},
doi = {10.1109/COMPSAC.2013.55},
abstract = {Programming languages have been proposed even before the era of the modern computer. As years have gone, computer resources have increased and application domains have expanded, leading to the proliferation of hundreds of programming languages, each attempting to improve over others or to address new programming paradigms. These languages range from procedural languages like C, object-oriented languages like Java, and functional languages such as ML and Haskell. Unfortunately, there is a lack of large scale and comprehensive studies that examine the "popularity", "interoperability", and "impact" of various programming languages. To fill this gap, this study investigates a hundred thousands of open source software projects from GitHub to answer various research questions on the "popularity", "interoperability" and "impact" of various languages measured in different ways (e.g., in terms of lines of code, development teams, issues, etc.).},
booktitle = {Proceedings of the 2013 IEEE 37th Annual Computer Software and Applications Conference},
pages = {303–312},
numpages = {10},
keywords = {Popularity, GitHub, Programming languages, Interoperability, Software projects, Open source},
series = {COMPSAC '13}
}

@inproceedings{10.1109/ISDEA.2012.223,
author = {Ben, Xu and Beijun, Shen and Weicheng, Yang},
title = {Mining Developer Contribution in Open Source Software Using Visualization Techniques},
year = {2013},
isbn = {9780769549231},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISDEA.2012.223},
doi = {10.1109/ISDEA.2012.223},
abstract = {The research of developers' contribution is an important part of the software evolution area. It allows project owners to find potential long-term contributors earlier and helps the newcomers to improve their behaviors. In this paper, we examined the contribution characteristics of developers in open source environment based on visual analysis, and presented approaches from three aspects-influencing factors, time characteristics and region characteristics. Our analysis used data from github and revealed some regular patterns. We found that the code which newcomers started to contribute with more people engaged in would lead to less contribution in some degree. We also found that there's a relation between developers' early and later period contribution. In addition, developers from different regions were more likely to have dominant relationship. Our findings may provide some support for future research in the area of software evolution.},
booktitle = {Proceedings of the 2013 Third International Conference on Intelligent System Design and Engineering Applications},
pages = {934–937},
numpages = {4},
keywords = {software evolution, visual analysis, contribution characteristics, open source software},
series = {ISDEA '13}
}

@inproceedings{10.1109/ICSE.2019.00089,
author = {Nguyen, Hoan Anh and Nguyen, Tien N. and Dig, Danny and Nguyen, Son and Tran, Hieu and Hilton, Michael},
title = {Graph-Based Mining of in-the-Wild, Fine-Grained, Semantic Code Change Patterns},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00089},
doi = {10.1109/ICSE.2019.00089},
abstract = {Prior research exploited the repetitiveness of code changes to enable several tasks such as code completion, bug-fix recommendation, library adaption, etc. These and other novel applications require accurate detection of semantic changes, but the state-of-the-art methods are limited to algorithms that detect specific kinds of changes at the syntactic level. Existing algorithms relying on syntactic similarity have lower accuracy, and cannot effectively detect semantic change patterns. We introduce a novel graph-based mining approach, CPatMiner, to detect previously unknown repetitive changes in the wild, by mining fine-grained semantic code change patterns from a large number of repositories. To overcome unique challenges such as detecting meaningful change patterns and scaling to large repositories, we rely on fine-grained change graphs to capture program dependencies.We evaluate CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub across a population of 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to 108 open-source developers. We found that 70% of respondents recognized those patterns as their meaningful frequent changes. Moreover, 79% of respondents even named the patterns, and 44% wanted future IDEs to automate such repetitive changes. We found that the mined change patterns belong to various development activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%, including refactorings). Second, we compared our tool with the state-of-the-art, AST-based technique, and reported that it detects 2.1x more meaningful patterns. Third, we use CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K instances of change patterns from which we provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the change patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns are not intermittent but spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {819–830},
numpages = {12},
keywords = {graph mining, semantic change pattern mining},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3186411.3186418,
author = {Rigger, Manuel and Marr, Stefan and Kell, Stephen and Leopoldseder, David and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {An Analysis of X86-64 Inline Assembly in C Programs},
year = {2018},
isbn = {9781450355797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3186411.3186418},
doi = {10.1145/3186411.3186418},
abstract = {C codebases frequently embed nonportable and unstandardized elements such as inline assembly code. Such elements are not well understood, which poses a problem to tool developers who aspire to support C code. This paper investigates the use of x86-64 inline assembly in 1264 C projects from GitHub and combines qualitative and quantitative analyses to answer questions that tool authors may have. We found that 28.1% of the most popular projects contain inline assembly code, although the majority contain only a few fragments with just one or two instructions. The most popular instructions constitute a small subset concerned largely with multicore semantics, performance optimization, and hardware control. Our findings are intended to help developers of C-focused tools, those testing compilers, and language designers seeking to reduce the reliance on inline assembly. They may also aid the design of tools focused on inline assembly itself.},
booktitle = {Proceedings of the 14th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {84–99},
numpages = {16},
keywords = {Empirical Survey, GitHub, Inline Assembly, C},
location = {Williamsburg, VA, USA},
series = {VEE '18}
}

@article{10.1145/3296975.3186418,
author = {Rigger, Manuel and Marr, Stefan and Kell, Stephen and Leopoldseder, David and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {An Analysis of X86-64 Inline Assembly in C Programs},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296975.3186418},
doi = {10.1145/3296975.3186418},
abstract = {C codebases frequently embed nonportable and unstandardized elements such as inline assembly code. Such elements are not well understood, which poses a problem to tool developers who aspire to support C code. This paper investigates the use of x86-64 inline assembly in 1264 C projects from GitHub and combines qualitative and quantitative analyses to answer questions that tool authors may have. We found that 28.1% of the most popular projects contain inline assembly code, although the majority contain only a few fragments with just one or two instructions. The most popular instructions constitute a small subset concerned largely with multicore semantics, performance optimization, and hardware control. Our findings are intended to help developers of C-focused tools, those testing compilers, and language designers seeking to reduce the reliance on inline assembly. They may also aid the design of tools focused on inline assembly itself.},
journal = {SIGPLAN Not.},
month = mar,
pages = {84–99},
numpages = {16},
keywords = {Empirical Survey, Inline Assembly, C, GitHub}
}

@inproceedings{10.1145/3098279.3122150,
author = {Henze, Niels and Mayer, Sven and Le, Huy Viet and Schwind, Valentin},
title = {Improving Software-Reduced Touchscreen Latency},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122150},
doi = {10.1145/3098279.3122150},
abstract = {The latency of current mobile devices' touchscreens is around 100ms and has widely been explored. Latency down to 2ms is noticeable, and latency as low as 25ms reduces users' performance. Previous work reduced touch latency by extrapolating a finger's movement using an ensemble of shallow neural networks and showed that predicting 33ms into the future increases users' performance. Unfortunately, this prediction has a high error. Predicting beyond 33ms did not increase participants' performance, and the error affected the subjective assessment. We use more recent machine learning techniques to reduce the prediction error. We train LSTM networks and multilayer perceptrons using a large data set and regularization. We show that linear extrapolation causes an 116.7% higher error and the previously proposed ensembles of shallow networks cause a 26.7% higher error compared to the LSTM networks. The trained models, the data used for testing, and the source code is available on GitHub.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {107},
numpages = {8},
keywords = {lag, latency, machine learning, touch input, LSTM, touchscreen, multilayer perceptron, prediction},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.1145/2950290.2983955,
author = {Zhang, Hongyu and Jain, Anuj and Khandelwal, Gaurav and Kaushik, Chandrashekhar and Ge, Scott and Hu, Wenxiang},
title = {Bing Developer Assistant: Improving Developer Productivity by Recommending Sample Code},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983955},
doi = {10.1145/2950290.2983955},
abstract = { In programming practice, developers often need sample code in order to learn how to solve a programming-related problem. For example, how to reuse an Application Programming Interface (API) of a large-scale software library and how to implement a certain functionality. We believe that previously written code can help developers understand how others addressed the similar problems and can help them write new programs. We develop a tool called Bing Developer Assistant (BDA), which improves developer productivity by recommending sample code mined from public software repositories (such as GitHub) and web pages (such as Stack Overflow). BDA can automatically mine code snippets that implement an API or answer a code search query. It has been implemented as a free-downloadable extension of Microsoft Visual Studio and has received more than 670K downloads since its initial release in December 2014. BDA is publicly available at: http://aka.ms/devassistant. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {956–961},
numpages = {6},
keywords = {Software Reuse, Code Search, GitHub, API Usage Extraction, API},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3386527.3405916,
author = {Piech, Chris and Abu-El-Haija, Sami},
title = {Human Languages in Source Code: Auto-Translation for Localized Instruction},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3405916},
doi = {10.1145/3386527.3405916},
abstract = {Computer science education has promised open access around the world, but access is largely determined by what human language you speak. As younger students learn computer science it is less appropriate to assume that they should learn English beforehand. To that end, we present CodeInternational, the first tool to translate code between human languages. To develop a theory of non-English code, and inform our translation decisions, we conduct a study of public code repositories on GitHub. The study is to the best of our knowledge the first on human-language in code and covers 2.9 million Java repositories. To demonstrate CodeInternational's educational utility, we build an interactive version of the popular English-language Karel reader and translate it into 100 spoken languages. Our translations have already been used in classrooms around the world, and represent a first step in an important open CS-education problem.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {167–174},
numpages = {8},
keywords = {translation, source-code, human-language, github},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3194793.3194797,
author = {Alsaeed, Ziyad and Young, Michal},
title = {Extending Existing Inference Tools to Mine Dynamic APIs},
year = {2018},
isbn = {9781450357548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194793.3194797},
doi = {10.1145/3194793.3194797},
abstract = {APIs often feature dynamic relations between client and service provider, such as registering for notifications or establishing a connection to a service. Dynamic specification mining techniques attempt to fill gaps in missing or decaying documentation, but current miners are blind to relations established dynamically. Because they cannot recover properties involving these dynamic structures, they may produce incomplete or misleading specifications. We have devised an extension to current dynamic specification mining techniques that ameliorates this shortcoming. The key insight is to monitor not only values dynamically, but also properties to track dynamic data structures that establish new relations between client and service provider. We have implemented this approach as an extension to the instrumentation component of Daikon, the leading example of dynamic invariant mining in the research literature. We evaluated our tool by applying it to selected modules of widely used software systems published on GitHub.},
booktitle = {Proceedings of the 2nd International Workshop on API Usage and Evolution},
pages = {23–26},
numpages = {4},
keywords = {design patterns, specification mining, dynamic analysis},
location = {Gothenburg, Sweden},
series = {WAPI '18}
}

@inproceedings{10.1007/978-3-319-35122-3_24,
author = {M\'{e}ndez-Acu\~{n}a, David and Galindo, Jos\'{e} A. and Combemale, Benoit and Blouin, Arnaud and Baudry, Benoit and Guernic, Gurvan},
title = {Reverse-Engineering Reusable Language Modules from Legacy Domain-Specific Languages},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_24},
doi = {10.1007/978-3-319-35122-3_24},
abstract = {The use of domain-specific languages DSLs has become a successful technique in the development of complex systems. Nevertheless, the construction of this type of languages is time-consuming and requires highly-specialized knowledge and skills. An emerging practice to facilitate this task is to enable reuse through the definition of language modules which can be later put together to build up new DSLs. Still, the identification and definition of language modules are complex and error-prone activities, thus hindering the reuse exploitation when developing DSLs. In this paper, we propose a computer-aided approach to i identify potential reuse in a set of legacy DSLs; and ii capitalize such potential reuse by extracting a set of reusable language modules with well defined interfaces that facilitate their assembly. We validate our approach by using realistic DSLs coming out from industrial case studies and obtained from public GitHub repositories.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {368–383},
numpages = {16},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/3340531.3412078,
author = {Jia, Ruipeng and Cao, Yanan and Shi, Haichao and Fang, Fang and Liu, Yanbing and Tan, Jianlong},
title = {DistilSum: Distilling the Knowledge for Extractive Summarization},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412078},
doi = {10.1145/3340531.3412078},
abstract = {A popular choice for extractive summarization is to conceptualize it as sentence-level classification, supervised by binary labels. While the common metric ROUGE prefers to measure the text similarity, instead of the performance of classifier. For example, BERTSUMEXT, the best extractive classifier so far, only achieves a precision of 32.9% at the top 3 extracted sentences (P@3) on CNN/DM dataset. It is obvious that current approaches cannot model the complex relationship of sentences exactly with 0/1 targets. In this paper, we introduce DistilSum, which contains teacher mechanism and student model. Teacher mechanism produces high entropy soft targets at a high temperature. Our student model is trained with the same temperature to match these informative soft targets and tested with temperature of 1 to distill for ground-truth labels. Compared with large version of BERTSUMEXT, our experimental result on CNN/DM achieves a substantial improvement of 0.99 ROUGE-L score (text similarity) and 3.95 P@3 score (performance of classifier). Our source code will be available on Github.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {2069–2072},
numpages = {4},
keywords = {knowledge distillation, summarization, neural networks},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3379597.3387485,
author = {Brandt, Carolin E. and Panichella, Annibale and Zaidman, Andy and Beller, Moritz},
title = {LogChunks: A Data Set for Build Log Analysis},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387485},
doi = {10.1145/3379597.3387485},
abstract = {Build logs are textual by-products that a software build process creates, often as part of its Continuous Integration (CI) pipeline. Build logs are a paramount source of information for developers when debugging into and understanding a build failure. Recently, attempts to partly automate this time-consuming, purely manual activity have come up, such as rule- or information-retrieval-based techniques.We believe that having a common data set to compare different build log analysis techniques will advance the research area. It will ultimately increase our understanding of CI build failures. In this paper, we present LogChunks, a collection of 797 annotated Travis CI build logs from 80 GitHub repositories in 29 programming languages. For each build log, LogChunks contains a manually labeled log part (chunk) describing why the build failed. We externally validated the data set with the developers who caused the original build failure.The width and depth of the LogChunks data set are intended to make it the default benchmark for automated build log analysis techniques.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {583–587},
numpages = {5},
keywords = {Build Failure, CI, Chunk Retrieval, Build Log Analysis},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/NOMS47738.2020.9110458,
author = {Granderath, Malte and Sch\"{o}nw\"{a}lder, J\"{u}rgen},
title = {A Resource Efficient Implementation of the RESTCONF Protocol for OpenWrt Systems},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NOMS47738.2020.9110458},
doi = {10.1109/NOMS47738.2020.9110458},
abstract = {In recent years, the open source operating system OpenWrt has become a popular option for replacing proprietary firmware on networking devices such as home routers or access points. In order to configure an OpenWrt system, like setting up firewall rules, the user has to either sign in to the web interface or use SSH to manually change configuration files on the device. While the current approach is sufficient for small home networks, it only allows for limited automation of management tasks and configuration management becomes time-consuming, for example, on larger campus networks where access control lists on OpenWrt access points need updates regularly.This paper describes our efforts to implement the RESTCONF configuration management protocol standardized by the IETF on OpenWrt systems that have limited CPU and memory resources. We detail our design choices that make our implementation resource efficient for the use cases we target and we compare our implementation against other similar solutions. Our implementation is available on GitHub under an open source license<sup>1</sup>.},
booktitle = {NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium},
pages = {1–6},
numpages = {6},
location = {Budapest, Hungary}
}

@inproceedings{10.1145/3159450.3159595,
author = {Heckman, Sarah and King, Jason},
title = {Developing Software Engineering Skills Using Real Tools for Automated Grading},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159595},
doi = {10.1145/3159450.3159595},
abstract = {Situated learning theory supports engaging students with materials and resources that reflect professional standards and best practices. Starting with our introductory courses, we incorporate situated learning to support student engagement in software engineering practices and processes through the use of industrial strength open-source tools in several classes throughout the undergraduate computer science curriculum at NC State University. Additionally, these tools support several logistical and educational needs in computer science classrooms, including assignment submission systems and automated grading. In this tools paper, we present our Canary Framework for supporting software engineering practices through the use of Eclipse for development; GitHub for submission and collaboration; and Jenkins for continuous integration and automated grading. These tools are used in five of ten core courses by more than 3000 students over ten semesters. While the use of these tools in education is not unique, we want to share our model of using professional tools in a classroom setting and our experiences on how this framework can support multiple courses throughout the curriculum and at scale.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {794–799},
numpages = {6},
keywords = {automated grading, software engineering best practice, version control, continuous integration},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.5555/3172077.3172324,
author = {Xiao, Jun and Ye, Hao and He, Xiangnan and Zhang, Hanwang and Wu, Fei and Chua, Tat-Seng},
title = {Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks},
year = {2017},
isbn = {9780999241103},
publisher = {AAAI Press},
abstract = {  Factorization Machines  (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named  Attentional Factorization Machine  (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&amp;Deep [Cheng  et al. , 2016] and Deep-Cross [Shan  et al. , 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github. com/hexiangnan/attentional_factorization_machine},
booktitle = {Proceedings of the 26th International Joint Conference on Artificial Intelligence},
pages = {3119–3125},
numpages = {7},
location = {Melbourne, Australia},
series = {IJCAI'17}
}

@inproceedings{10.1145/2901739.2903501,
author = {Sinha, Vinayak and Lazar, Alina and Sharif, Bonita},
title = {Analyzing Developer Sentiment in Commit Logs},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903501},
doi = {10.1145/2901739.2903501},
abstract = {The paper presents an analysis of developer commit logs for GitHub projects. In particular, developer sentiment in commits is analyzed across 28,466 projects within a seven year time frame. We use the Boa infrastructure's online query system to generate commit logs as well as files that were changed during the commit. We analyze the commits in three categories: large, medium, and small based on the number of commits using a sentiment analysis tool. In addition, we also group the data based on the day of week the commit was made and map the sentiment to the file change history to determine if there was any correlation. Although a majority of the sentiment was neutral, the negative sentiment was about 10% more than the positive sentiment overall. Tuesdays seem to have the most negative sentiment overall. In addition, we do find a strong correlation between the number of files changed and the sentiment expressed by the commits the files were part of. Future work and implications of these results are discussed.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {520–523},
numpages = {4},
keywords = {commit logs, Java projects, sentiment analysis},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3397536.3429335,
author = {Saxena, Nikita},
title = {Efficient Downscaling of Satellite Oceanographic Data With Convolutional Neural Networks},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3429335},
doi = {10.1145/3397536.3429335},
abstract = {Space-borne satellite radiometers measure Sea Surface Temperature (SST), which is pivotal to studies of air-sea interactions and ocean features. Under clear sky conditions, high resolution measurements are obtainable. But under cloudy conditions, data analysis is constrained to the available low resolution measurements. We assess the efficiency of Deep Learning (DL) architectures, particularly Convolutional Neural Networks (CNN) to downscale oceanographic data from low spatial resolution (SR) to high SR. With a focus on SST Fields of Bay of Bengal, this study proves that Very Deep Super Resolution CNN can successfully reconstruct SST observations from 15 km SR to 5km SR, and 5km SR to 1km SR. This outcome calls attention to the significance of DL models explicitly trained for the reconstruction of high SR SST fields by using low SR data. Inference on DL models can act as a substitute to the existing computationally expensive downscaling technique: Dynamical Downsampling. The complete code is available on this Github Repository.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {659–660},
numpages = {2},
keywords = {Single Image Super Resolution, Ocean Remote Sensing Data, Deep Convolutional Neural Networks},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.1145/3377813.3381364,
author = {Fischer-Nielsen, Anders and Fu, Zhoulai and Su, Ting and W\k{a}sowski, Andrzej},
title = {The Forgotten Case of the Dependency Bugs: On the Example of the Robot Operating System},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381364},
doi = {10.1145/3377813.3381364},
abstract = {A dependency bug is a software fault that manifests itself when accessing an unavailable asset. Dependency bugs are pervasive and we all hate them. This paper presents a case study of dependency bugs in the Robot Operating System (ROS), applying mixed methods: a qualitative investigation of 78 dependency bug reports, a quantitative analysis of 1354 ROS bug reports against 19553 reports in the top 30 GitHub projects, and a design of three dependency linters evaluated on 406 ROS packages.The paper presents a definition and a taxonomy of dependency bugs extracted from data. It describes multiple facets of these bugs and estimates that as many as 15% (!) of all reported bugs are dependency bugs. We show that lightweight tools can find dependency bugs efficiently, although it is challenging to decide which tools to build and difficult to build general tools. We present the research problem to the community, and posit that it should be feasible to eradicate it from software development practice.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {21–30},
numpages = {10},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/3338906.3342505,
author = {Kr\"{u}ger, Jacob},
title = {Tackling Knowledge Needs during Software Evolution},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342505},
doi = {10.1145/3338906.3342505},
abstract = {Developers use a large amount of their time to understand the system they work on, an activity referred to as program comprehension. Especially software evolution and forgetting over time lead to developers becoming unfamiliar with a system. To support them during program comprehension, we can employ knowledge recovery to reverse engineer implicit information from the system and the platform (e.g., GitHub) it is hosted on. However, to recover useful knowledge and to provide it in a useful way, we first need to understand what knowledge developers forget to what extent, what sources are reliable to recover knowledge, and how to trace knowledge to the features in a system. We tackle these three issues, aiming to provide empirical insights and tooling to support developers during software evolution and maintenance. The results help practitioners, as we support the analysis and understanding of systems, as well as researchers, showing opportunities to automate, for example, reverse-engineering techniques.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1244–1246},
numpages = {3},
keywords = {Software evolution, Feature traceability, Software maintenance, Forgetting, Program comprehension, Memory},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1109/MSR.2019.00041,
author = {Dietrich, Jens and Luczak-Roesch, Markus and Dalefield, Elroy},
title = {Man vs Machine: A Study into Language Identification of Stack Overflow Code Snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00041},
doi = {10.1109/MSR.2019.00041},
abstract = {Software engineers produce large amounts of publicly accessible data that enables researchers to mine knowledge, fostering a better understanding of the field. Knowledge extraction often relies on meta data. This meta data can either be harvested from user-provided tags, or inferred by algorithms from the respective data. The question arises to which extent either type of meta data can be trusted and relied upon.We study this problem in the context of language identification of code snippets posted on Stack Overflow. We analyse the consistency between user-provided tags and the classification obtained with GitHub linguist, an industry-strength automated language recognition tool. We find that the results obtained by both approaches are often not consistent. This indicates that both have to be used with great care. Our results also suggest that developers may not follow the evolutionary path of programming languages beyond one step when seeking or providing answers to software engineering challenges encountered.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {205–209},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3196398.3196405,
author = {Sanchez, Beatriz A. and Barmpis, Konstantinos and Neubauer, Patrick and Paige, Richard F. and Kolovos, Dimitrios S.},
title = {Restmule: Enabling Resilient Clients for Remote APIs},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196405},
doi = {10.1145/3196398.3196405},
abstract = {Mining data from remote repositories, such as GitHub and StackExchange, involves the execution of requests that can easily reach the limitations imposed by the respective APIs to shield their services from overload and abuse. Therefore, data mining clients are left alone to deal with such protective service policies which usually involves an extensive amount of manual implementation effort. In this work we present RestMule, a framework for handling various service policies, such as limited number of requests within a period of time and multi-page responses, by generating resilient clients that are able to handle request rate limits, network failures, response caching, and paging in a graceful and transparent manner. As a result, RestMule clients generated from OpenAPI specifications (i.e. standardized REST API descriptors), are suitable for intensive data-fetching scenarios. We evaluate our framework by reproducing an existing repository mining use case and comparing the results produced by employing a popular hand-written client and a RestMule client.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {537–541},
numpages = {5},
keywords = {HTTP API clients, openAPI specification, resilience},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1007/978-3-319-42061-5_1,
author = {Babur, \"{O}nder and Cleophas, Loek and Brand, Mark},
title = {Hierarchical Clustering of Metamodels for Comparative Analysis and Visualization},
year = {2016},
isbn = {9783319420608},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-42061-5_1},
doi = {10.1007/978-3-319-42061-5_1},
abstract = {Many applications in Model-Driven Engineering involve processing multiple models or metamodels. A good example is the comparison and merging of metamodel variants into a common metamodel in domain model recovery. Although there are many sophisticated techniques to process the input dataset, little attention has been given to the initial data analysis, visualization and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. In this paper we present a generic approach for metamodel comparison, analysis and visualization as an exploratory first step for domain model recovery. We propose representing metamodels in a vector space model, and applying hierarchical clustering techniques to compare and visualize them as a tree structure. We demonstrate our approach on two Ecore datasets: a collection of 50 state machine metamodels extracted from GitHub as top search results; and $$sim $$~100 metamodels from 16 different domains, obtained from AtlanMod Metamodel Zoo.},
booktitle = {Proceedings of the 12th European Conference on Modelling Foundations and Applications - Volume 9764},
pages = {3–18},
numpages = {16},
keywords = {Model comparison, R, Model-Driven Engineering, Vector space model, Hierarchical clustering}
}

@inproceedings{10.1145/2901739.2903497,
author = {Kery, Mary Beth and Le Goues, Claire and Myers, Brad A.},
title = {Examining Programmer Practices for Locally Handling Exceptions},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903497},
doi = {10.1145/2901739.2903497},
abstract = {Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed. A major complaint is that programmers often write minimal and low quality handlers. We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception. Programmers make heavy use of actions like Log, Print, Return, or Throw in catch blocks, and also frequently copy code between handlers. We found bad practices like empty catch blocks or catching Exception are indeed widespread. We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. Some of these issues might be addressed by future tools which autocomplete more complete handlers.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {484–487},
numpages = {4},
keywords = {Java exceptions, error handlers, Boa, GitHub},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/2660398.2660435,
author = {McDonald, Nora},
title = {Distributed Leadership in OSS},
year = {2014},
isbn = {9781450330435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660398.2660435},
doi = {10.1145/2660398.2660435},
abstract = {Open-source software (OSS) is software whose source code is available to view, change, and distribute without cost, and is typically developed in a collaborative manner that has captured the imagination of those who view the web as enabling more "democratic" models of governance. Researchers have, for years, debated the social structure of OSS projects -- in particular, the extent to which they represent decentralized forms of organization. Many have argued that the significant concentration of code development responsibility raises doubts about whether the level of power-sharing truly qualifies as "distributed" in the way early observers predicted. This research will investigate how changes in the technology that supports these projects -- specifically the greater visibility that characterizes the GitHub workspace may lead to a more broadly and quantifiably distributed leadership. Over the course of several studies employing several methodologies, it will examine leadership in OSS projects when visibility is a feature of the workspace.},
booktitle = {Proceedings of the 18th International Conference on Supporting Group Work},
pages = {261–262},
numpages = {2},
keywords = {distributed leadership, open-source software, social computing},
location = {Sanibel Island, Florida, USA},
series = {GROUP '14}
}

@inproceedings{10.1145/2568225.2568313,
author = {Subramanian, Siddharth and Inozemtseva, Laura and Holmes, Reid},
title = {Live API Documentation},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568313},
doi = {10.1145/2568225.2568313},
abstract = { Application Programming Interfaces (APIs) provide powerful abstraction mechanisms that enable complex functionality to be used by client programs. However, this abstraction does not come for free: understanding how to use an API can be difficult. While API documentation can help, it is often insufficient on its own. Online sites like Stack Overflow and Github Gists have grown to fill the gap between traditional API documentation and more example-based resources. Unfortunately, these two important classes of documentation are independent.  In this paper we describe an iterative, deductive method of linking source code examples to API documentation. We also present an implementation of this method, called Baker, that is highly precise (0.97) and supports both Java and JavaScript. Baker can be used to enhance traditional API documentation with up-to-date source code examples; it can also be used to incorporate links to the API documentation into the code snippets that use the API. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {643–652},
numpages = {10},
keywords = {documentation, Source code examples, source code search},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568260,
author = {Gousios, Georgios and Pinzger, Martin and Deursen, Arie van},
title = {An Exploratory Study of the Pull-Based Software Development Model},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568260},
doi = {10.1145/2568225.2568260},
abstract = { The advent of distributed version control systems has led to the development of a new paradigm for distributed software development; instead of pushing changes to a central repository, developers pull them from other repositories and merge them locally. Various code hosting sites, notably Github, have tapped on the opportunity to facilitate pull-based development by offering workflow support tools, such as code reviewing systems and integrated issue trackers. In this work, we explore how pull-based software development works, first on the GHTorrent corpus and then on a carefully selected sample of 291 projects. We find that the pull request model offers fast turnaround, increased opportunities for community engagement and decreased time to incorporate contributions. We show that a relatively small number of factors affect both the decision to merge a pull request and the time to process it. We also examine the reasons for pull request rejection and find that technical ones are only a small minority. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {345–355},
numpages = {11},
keywords = {pull request, empirical software engineering, Pull-based development, distributed software development},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.5555/2486788.2486844,
author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
title = {Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {422–431},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/2141512.2141583,
author = {Tsay, Jason T. and Dabbish, Laura and Herbsleb, James},
title = {Social Media and Success in Open Source Projects},
year = {2012},
isbn = {9781450310512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2141512.2141583},
doi = {10.1145/2141512.2141583},
abstract = {Social media are being integrated into work environments. They have the potential to provide essential context and awareness, and increase work performance as a result. However, the specific effects of social media that impact productivity are not well understood. We perform a quantitative analysis of project success of over 5,000 open source software projects hosted on GitHub, a website that provides extensive social media functionality. Adapted from the open source literature, we develop two measures of project success, Developer Attention and Work Contribution. We find that projects with highly socially connected developers are not necessarily the most active or popular projects. Oddly, projects with a high level of developer multitasking, i.e., splitting effort equally across multiple projects, tend to receive less Developer Attention, but greater Work Contribution. Success on both measures is strongly positively associated with greater concentration of work among a small number of developers. We discuss the implications of the findings for social media in online production.},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work Companion},
pages = {223–226},
numpages = {4},
keywords = {social media, open source, software development, success},
location = {Seattle, Washington, USA},
series = {CSCW '12}
}

@inproceedings{10.5555/3408207.3408219,
author = {Nobel, Parth},
title = {Auto_diff: An Automatic Differentiation Package for Python},
year = {2020},
isbn = {9781713812883},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {We present auto_diff, a package that performs automatic differentiation of numerical Python code. auto_diff overrides Python's NumPy package's functions, augmenting them with seamless automatic differentiation capabilities. Notably, auto_diff is non-intrusive, i.e., the code to be differentiated does not require auto_diff-specific alterations. We illustrate auto_diff on electronic devices, a circuit simulation, and a mechanical system simulation. In our evaluations so far, we found that running simulations with auto_diff takes less than 4 times as long as simulations with hand-written differentiation code. We believe that auto_diff, which was written after attempts to use existing automatic differentiation packages on our applications ran into difficulties, caters to an important need within the numerical Python community. We have attempted to write this paper in a tutorial style to make it accessible to those without prior background in automatic differentiation techniques and packages. We have released auto_diff as open source on GitHub.},
booktitle = {Proceedings of the 2020 Spring Simulation Conference},
articleno = {10},
numpages = {12},
keywords = {Python, implementation, automatic differentiation, numerical methods, library},
location = {Fairfax, Virginia},
series = {SpringSim '20}
}

@inproceedings{10.1145/3173574.3173606,
author = {Rule, Adam and Tabard, Aur\'{e}lien and Hollan, James D.},
title = {Exploration and Explanation in Computational Notebooks},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173606},
doi = {10.1145/3173574.3173606},
abstract = {Computational notebooks combine code, visualizations, and text in a single document. Researchers, data analysts, and even journalists are rapidly adopting this new medium. We present three studies of how they are using notebooks to document and share exploratory data analyses. In the first, we analyzed over 1 million computational notebooks on GitHub, finding that one in four had no explanatory text but consisted entirely of visualizations or code. In a second study, we examined over 200 academic computational notebooks, finding that although the vast majority described methods, only a minority discussed reasoning or results. In a third study, we interviewed 15 academic data analysts, finding that most considered computational notebooks personal, exploratory, and messy. Importantly, they typically used other media to share analyses. These studies demonstrate a tension between exploration and explanation in constructing and sharing computational notebooks. We conclude with opportunities to encourage explanation in computational media without hindering exploration.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {data analysis, computational notebook, jupyter notebook, data science, narrative},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1109/ESEM.2017.11,
author = {Hassan, Foyzul and Mostafa, Shaikh and Lam, Edmund S. L. and Wang, Xiaoyin},
title = {Automatic Building of Java Projects in Software Repositories: A Study on Feasibility and Challenges},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.11},
doi = {10.1109/ESEM.2017.11},
abstract = {Despite the advancement in software build tools such as Maven and Gradle, human involvement is still often required in software building. To enable large-scale advanced program analysis and data mining of software artifacts, software engineering researchers need to have a large corpus of built software, so automatic software building becomes essential to improve research productivity. In this paper, we present a feasibility study on automatic software building. Particularly, we first put state-of-the-art build automation tools (Ant, Maven and Gradle) to the test by automatically executing their respective default build commands on top 200 Java projects from GitHub. Next, we focus on the 86 projects that failed this initial automated build attempt, manually examining and determining correct build sequences to build each of these projects. We present a detailed build failure taxonomy from these build results and show that at least 57% build failures can be automatically resolved.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {38–47},
numpages = {10},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3368089.3409705,
author = {Lamba, Hemank and Trockman, Asher and Armanios, Daniel and K\"{a}stner, Christian and Miller, Heather and Vasilescu, Bogdan},
title = {Heard It through the Gitvine: An Empirical Study of Tool Diffusion across the Npm Ecosystem},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409705},
doi = {10.1145/3368089.3409705},
abstract = {Automation tools like continuous integration services, code coverage reporters, style checkers, dependency managers, etc. are all known to provide significant improvements in developer productivity and software quality. Some of these tools are widespread, others are not. How do these automation "best practices" spread? And how might we facilitate the diffusion process for those that have seen slower adoption? In this paper, we rely on a recent innovation in transparency on code hosting platforms like GitHub---the use of repository badges---to track how automation tools spread in open-source ecosystems through different social and technical mechanisms over time. Using a large longitudinal data set, multivariate network science techniques, and survival analysis, we study which socio-technical factors can best explain the observed diffusion process of a number of popular automation tools. Our results show that factors such as social exposure, competition, and observability affect the adoption of tools significantly, and they provide a roadmap for software engineers and researchers seeking to propagate best practices and tools.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {505–517},
numpages = {13},
keywords = {innovations, open source ecosystem, diffusion, software tools},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3243127.3243132,
author = {Gelman, Ben and Hoyle, Bryan and Moore, Jessica and Saxe, Joshua and Slater, David},
title = {A Language-Agnostic Model for Semantic Source Code Labeling},
year = {2018},
isbn = {9781450359726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243127.3243132},
doi = {10.1145/3243127.3243132},
abstract = {Code search and comprehension have become more difficult in recent years due to the rapid expansion of available source code. Current tools lack a way to label arbitrary code at scale while maintaining up-to-date representations of new programming languages, libraries, and functionalities. Comprehensive labeling of source code enables users to search for documents of interest and obtain a high-level understanding of their contents. We use Stack Overflow code snippets and their tags to train a language-agnostic, deep convolutional neural network to automatically predict semantic labels for source code documents. On Stack Overflow code snippets, we demonstrate a mean area under ROC of 0.957 over a long-tailed list of 4,508 tags. We also manually validate the model outputs on a diverse set of unlabeled source code documents retrieved from Github, and obtain a top-1 accuracy of 86.6%. This strongly indicates that the model successfully transfers its knowledge from Stack Overflow snippets to arbitrary source code documents.},
booktitle = {Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis},
pages = {36–44},
numpages = {9},
keywords = {source code, semantic labeling, natural language processing, multilabel classification, deep learning, crowdsourcing},
location = {Montpellier, France},
series = {MASES 2018}
}

@inproceedings{10.1145/3148055.3148071,
author = {Torres, Johnny and Vaca, Carmen and Abad, Cristina L.},
title = {What Ignites a Reply? Characterizing Conversations in Microblogs},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148071},
doi = {10.1145/3148055.3148071},
abstract = {Nowadays, microblog platforms provide a medium to share content and interact with other users. With the large-scale data generated on these platforms, the origin and reasons of users engagement in conversations has attracted the attention of the research community. In this paper, we analyze the factors that might spark conversations in Twitter, for the English and Spanish languages. Using a corpus of 2.7 million tweets, we reconstruct existing conversations, then extract several contextual and content features. Based on the features extracted, we train and evaluate several predictive models to identify tweets that will spark a conversation. Our findings show that conversations are more likely to be initiated by users with high activity level and popularity. For less popular users, the type of content generated is a more important factor. Experimental results shows that the best predictive model is able obtain an average score $F1=0.80$. We made available the dataset scripts and code used in this paper to the research community via Github.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {149–156},
numpages = {8},
keywords = {machine learning, social computing, big data},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3139903.3139916,
author = {Salgado, Ronie and Bergel, Alexandre},
title = {Pharo Git Thermite: A Visual Tool for Deciding to Weld a Pull Request},
year = {2017},
isbn = {9781450355544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139903.3139916},
doi = {10.1145/3139903.3139916},
abstract = {Collaborative software development platforms such as GitHub simplify the process of contributing into open source projects by the use of a pull request. The decision of accepting or rejecting a pull request has to be made by an integrator. Because reviewing a pull request can be time consuming, social factors are known to have an important effect on the acceptation of a pull request. This effect can be especially important for large and complicated pull request.In this paper we present Git Thermite, a tool to assess the internal structure of a pull request and simplifying the job of the integrator. Git Thermite details the structural changes made on the source code. In Git Thermite we use a pull request business card visual metaphor for describing a pull request. In this business card, we present the pull request metadata and describe the modified files, and the structural changes in the modified source code.},
booktitle = {Proceedings of the 12th Edition of the International Workshop on Smalltalk Technologies},
articleno = {11},
numpages = {6},
location = {Maribor, Slovenia},
series = {IWST '17}
}

@inproceedings{10.1109/MSR.2017.37,
author = {Gupta, Yash and Khan, Yusaira and Gallaba, Keheliya and McIntosh, Shane},
title = {The Impact of the Adoption of Continuous Integration on Developer Attraction and Retention},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.37},
doi = {10.1109/MSR.2017.37},
abstract = {Open-source projects rely on attracting new and retaining old contributors for achieving sustainable success. One may suspect that adopting new development practices like Continuous Integration (CI) should improve the attractiveness of a project. However, little is known about the impact that adoption of CI has on developer attraction and retention. To bridge this gap, we study how the introduction of Travis CI---a popular CI service provider---impacts developer attraction and retention in 217 GitHub repositories. Surprisingly, we find that heuristics that estimate the developer attraction and retention of a project are higher in the year before adopting Travis CI than they are in the year following Travis CI adoption. Moreover, the results are statistically significant (Wilcoxon signed rank test, α = 0.05), with small but non-negligible effect sizes (Cliff's delta). Although we do not suspect a causal link, our results are worrisome. More work is needed to ascertain the relationship between CI and developer attraction and retention.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {491–494},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1109/ICSE-C.2017.8,
author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
title = {DroidBot: A Lightweight UI-Guided Test Input Generator for Android},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.8},
doi = {10.1109/ICSE-C.2017.8},
abstract = {As many automated test input generation tools for Android need to instrument the system or the app, they cannot be used in some scenarios such as compatibility testing and malware analysis. We introduce DroidBot, a lightweight UI-guided test input generator, which is able to interact with an Android app on almost any device without instrumentation. The key technique behind DroidBot is that it can generate UI-guided test inputs based on a state transition model generated on-the-fly, and allow users to integrate their own strategies or algorithms. DroidBot is lightweight as it does not require app instrumentation, thus no need to worry about the inconsistency between the tested version and the original version. It is compatible to most Android apps, and able to run on almost all Android-based systems, including customized sandboxes and commodity devices. Droidbot is released as an open-source tool on GitHub [1], and the demo video can be found at https://youtu.be/3-aHGSazMY.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {23–26},
numpages = {4},
keywords = {dynamic analysis, compatibility testing, automated testing, Android, malware detection},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1109/BIBM.2015.7359904,
author = {Cui, Xiaodong and Zhen Wei and Zhang, Lin and Liu, Hui and Lei Sun and Zhang, Shao-Wu and Huang, Yufei and Meng, Jia},
title = {Sketching the Distribution of Transcriptomic Features on RNA Transcripts with Travis Coordinates},
year = {2015},
isbn = {9781467367998},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BIBM.2015.7359904},
doi = {10.1109/BIBM.2015.7359904},
abstract = {Biological features, such as, genes, transcription factor binding sites, SNPs, etc., are usually denoted with genome-based coordinates as the genomic features. While genome-based representation is usually very effective, it can be tedious to examine the distribution of RNA-related genomic features on RNA transcripts with existing tools due to the conversion and comparison between genome-based coordinates to RNA-based coordinates. We developed here an open source R package Travis for sketching the transcriptomic view of genomic features so as to facilitate the analysis of RNA-related but genome-based coordinates. Internally, Travis package extracts the coordinates relative to the landmarks of transcripts, with which the distribution of RNA-related genomic features can then be conveniently analyzed. We demonstrated the usage of Travis package in analyzing post-transcriptional RNA modifications (5-MethylCytosine and N6-MethylAdenosine) derived from high-throughput sequencing approaches (MeRIP-Seq and RNA BS-Seq). The Travis R package is now publicly available from GitHub: https://github.com/lzcyzm/Travis.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
pages = {1536–1542},
numpages = {7},
series = {BIBM '15}
}

@inproceedings{10.1145/2627508.2627512,
author = {Dai, Meixi and Shen, Beijun and Zhang, Tao and Zhao, Min},
title = {Impact of Consecutive Changes on Later File Versions},
year = {2014},
isbn = {9781450329651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627508.2627512},
doi = {10.1145/2627508.2627512},
abstract = { By analyzing histories of program versions, many researches have shown that software quality is associated with history-related metrics, such as code-related metrics, commit-related metrics, developer-related metrics, process-related metrics, and organizational metrics etc. It has also been revealed that consecutive changes on commit-level are strongly associated with software defects. In this paper, we introduce two novel concepts of consecutive changes: CFC (chain of consecutive bug-fixing file versions) and CAC (chain of consecutive file versions where each pair of adjacent versions are submitted by different developers). And then several experiments are conducted to explore the correlation between consecutive changes and software quality by using three open-source projects from Github. Our main findings include: 1) CFCs and CACs widely exist in file version histories; 2) Consecutive changes have a negative and strong impact on the later file versions in a short term, especially when the length of consecutive change chain is 4 or 5. },
booktitle = {Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies},
pages = {17–24},
numpages = {8},
keywords = {file version histories, Software quality, mining software repository, consecutive change},
location = {Nanjing, China},
series = {EAST 2014}
}

@inproceedings{10.1145/2595188.2595197,
author = {Vobl, Thorsten and Gotscharek, Annette and Reffle, Uli and Ringlstetter, Christoph and Schulz, Klaus U.},
title = {PoCoTo - an Open Source System for Efficient Interactive Postcorrection of OCRed Historical Texts},
year = {2014},
isbn = {9781450325882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2595188.2595197},
doi = {10.1145/2595188.2595197},
abstract = {When applied to historical texts, OCR engines often produce a non-negligible number of OCR errors. For research in the Humanities, text mining and retrieval, the option is important to improve the quality of OCRed historical texts using interactive postcorrection. We describe a system for interactive postcorrection of OCRed historical documents developed in the EU project IMPACT. Various advanced features of the system help to efficiently correct texts. Language technology used in the background takes orthographic variation in historical language into account. Using this knowledge, the tool visualizes possible OCR errors and series of similar possible OCR errors in a given input document. Error series can be corrected in one shot. Practical user tests in three major European libraries have shown that the system considerably reduces the time needed by human correctors to eliminate a certain number of OCR errors. The system has been published as an open source tool under GitHub.},
booktitle = {Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage},
pages = {57–61},
numpages = {5},
keywords = {user interfaces, decision support, error correction},
location = {Madrid, Spain},
series = {DATeCH '14}
}

@inproceedings{10.1145/3372297.3420015,
author = {Vu, Duc Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
title = {Towards Using Source Code Repositories to Identify Software Supply Chain Attacks},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3420015},
doi = {10.1145/3372297.3420015},
abstract = {Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100,000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (e.g., those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI) suggests that the technique is suitable for lightweight analysis of real-world packages.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2093–2095},
numpages = {3},
keywords = {source code repositories, package repositories, code injection, lightweight analysis, software supply chain attacks},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.1109/ICPC.2017.18,
author = {Beniamini, Gal and Gingichashvili, Sarah and Orbach, Alon Klein and Feitelson, Dror G.},
title = {Meaningful Identifier Names: The Case of Single-Letter Variables},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.18},
doi = {10.1109/ICPC.2017.18},
abstract = {It is widely accepted that variable names in computer programs should be meaningful, and that this aids program comprehension. "Meaningful" is commonly interpreted as favoring long descriptive names. However, there is at least some use of short and even single-letter names: using i in loops is very common, and we show (by extracting variable names from 1000 popular github projects in 5 languages) that some other letters are also widely used. In addition, controlled experiments with different versions of the same functions (specifically, different variable names) failed to show significant differences in ability to modify the code. Finally, an online survey showed that certain letters are strongly associated with certain types and meanings. This implies that a single letter can in fact convey meaning. The conclusion from all this is that single letter variables can indeed be used beneficially in certain cases, leading to more concise code.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {45–54},
numpages = {10},
keywords = {program comprehension, meaningful identifier names, single-letter names},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/2901739.2903500,
author = {Asaduzzaman, Muhammad and Ahasanuzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A.},
title = {How Developers Use Exception Handling in Java?},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903500},
doi = {10.1145/2901739.2903500},
abstract = {Exception handling is a technique that addresses exceptional conditions in applications, allowing the normal flow of execution to continue in the event of an exception and/or to report on such events. Although exception handling techniques, features and bad coding practices have been discussed both in developer communities and in the literature, there is a marked lack of empirical evidence on how developers use exception handling in practice. In this paper we use the Boa language and infrastructure to analyze 274k open source Java projects in GitHub to discover how developers use exception handling. We not only consider various exception handling features but also explore bad coding practices and their relation to the experience of developers. Our results provide some interesting insights. For example, we found that bad exception handling coding practices are common in open source Java projects and regardless of experience all developers use bad exception handling coding practices.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {516–519},
numpages = {4},
keywords = {language feature, source code mining, Java, exception},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.5555/2820518.2820539,
author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Bacchelli, Alberto},
title = {Will They like This? Evaluating Code Contributions with Language Models},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected changesets do contain code significantly less similar to the project than accepted ones; furthermore, the less similar changesets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {157–167},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3377811.3380410,
author = {Overney, Cassandra and Meinicke, Jens and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {How to Not Get Rich: An Empirical Study of Donations in Open Source},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380410},
doi = {10.1145/3377811.3380410},
abstract = {Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1209–1221},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3377813.3381358,
author = {Malavolta, Ivano and Lewis, Grace and Schmerl, Bradley and Lago, Patricia and Garlan, David},
title = {How Do You Architect Your Robots? State of the Practice and Guidelines for ROS-Based Systems},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381358},
doi = {10.1145/3377813.3381358},
abstract = {The Robot Operating System (ROS) is the de-facto standard for robotic software. If on one hand ROS is helping roboticists, e.g., by providing a standardized communication platform, on the other hand ROS-based systems are getting larger and more complex and could benefit from good software architecture practices. This paper presents an observational study aimed at (i) unveiling the state-of-the-practice for architecting ROS-based systems and (ii) providing guidance to roboticists about how to properly architect ROS-based systems. To achieve these goals, we (i) build a dataset of 335 GitHub repositories containing real open-source ROS-based systems, (ii) mine the repositories for extracting the state of the practice about how roboticists are architecting them, and (iii) synthesize a catalog of 49 evidence-based guidelines for architecting ROS-based systems. The guidelines have been validated by 77 roboticists working on real-world open-source ROS-based systems.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {31–40},
numpages = {10},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/3368555.3384461,
author = {Park, Yubin and Ho, Joyce C.},
title = {CaliForest: Calibrated Random Forest for Health Data},
year = {2020},
isbn = {9781450370462},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368555.3384461},
doi = {10.1145/3368555.3384461},
abstract = {Real-world predictive models in healthcare should be evaluated in terms of discrimination, the ability to differentiate between high and low risk events, and calibration, or the accuracy of the risk estimates. Unfortunately, calibration is often neglected and only discrimination is analyzed. Calibration is crucial for personalized medicine as they play an increasing role in the decision making process. Since random forest is a popular model for many healthcare applications, we propose CaliForest, a new calibrated random forest. Unlike existing calibration methodologies, CaliForest utilizes the out-of-bag samples to avoid the explicit construction of a calibration set. We evaluated CaliForest on two risk prediction tasks obtained from the publicly-available MIMIC-III database. Evaluation on these binary prediction tasks demonstrates that CaliForest can achieve the same discriminative power as random forest while obtaining a better-calibrated model evaluated across six different metrics. CaliForest will be published on the standard Python software repository and the code will be openly available on Github.},
booktitle = {Proceedings of the ACM Conference on Health, Inference, and Learning},
pages = {40–50},
numpages = {11},
keywords = {calibration, random forest, healthcare, Python},
location = {Toronto, Ontario, Canada},
series = {CHIL '20}
}

@inproceedings{10.1145/3338906.3338917,
author = {Zhang, Chen and Chen, Bihuan and Chen, Linlin and Peng, Xin and Zhao, Wenyun},
title = {A Large-Scale Empirical Study of Compiler Errors in Continuous Integration},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338917},
doi = {10.1145/3338906.3338917},
abstract = {Continuous Integration (CI) is a widely-used software development practice to reduce risks. CI builds often break, and a large amount of efforts are put into troubleshooting broken builds. Despite that compiler errors have been recognized as one of the most frequent types of build failures, little is known about the common types, fix efforts and fix patterns of compiler errors that occur in CI builds of open-source projects. To fill such a gap, we present a large-scale empirical study on 6,854,271 CI builds from 3,799 open-source Java projects hosted on GitHub. Using the build data, we measured the frequency of broken builds caused by compiler errors, investigated the ten most common compiler error types, and reported their fix time. We manually analyzed 325 broken builds to summarize fix patterns of the ten most common compiler error types. Our findings help to characterize and understand compiler errors during CI and provide practical implications to developers, tool builders and researchers.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {176–187},
numpages = {12},
keywords = {Continuous Integration, Compiler Errors, Build Failures},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3236024.3264600,
author = {Hua, Jinru and Zhang, Mengshi and Wang, Kaiyuan and Khurshid, Sarfraz},
title = {SketchFix: A Tool for Automated Program Repair Approach Using Lazy Candidate Generation},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264600},
doi = {10.1145/3236024.3264600},
abstract = {Manually locating and removing bugs in faulty program is often tedious and error-prone. A common automated program repair approach called generate-and-validate (G&amp;V) iteratively creates candidate fixes, compiles them, and runs these candidates against the given tests. This approach can be costly due to a large number of re-compilations and re-executions of the program. To tackle this limitation, recent work introduced the SketchFix approach that tightly integrates the generation and validation phases, and utilizes runtime behaviors to substantially prune a large amount of repair candidates. This tool paper describes our Java implementation of SketchFix, which is an open-source library that we released on Github. Our experimental evaluation using Defects4J benchmark shows that SketchFix can significantly reduce the number of re-compilations and re-executions compared to other approaches and work particularly well in repairing expression manipulation at the AST node-level granularity.The demo video is at: https://youtu.be/AO-YCH8vGzQ.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {Program Sketching, SketchFix, Program Repair, Program Synthesis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3077136.3080753,
author = {Jiang, Jyun-Yu and Cheng, Pu-Jen and Wang, Wei},
title = {Open Source Repository Recommendation in Social Coding},
year = {2017},
isbn = {9781450350228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077136.3080753},
doi = {10.1145/3077136.3080753},
abstract = {Social coding and open source repositories have become more and more popular. Software developers have various alternatives to contribute themselves to the communities and collaborate with others. However, nowadays there is no effective recommender suggesting developers appropriate repositories in both the academia and the industry. Although existing one-class collaborative filtering (OCCF) approaches can be applied to this problem, they do not consider particular constraints of social coding such as the programming languages, which, to some extent, associate the repositories with the developers. The aim of this paper is to investigate the feasibility of leveraging user programming language preference to improve the performance of OCCF-based repository recommendation. Based on matrix factorization, we propose language-regularized matrix factorization (LRMF), which is regularized by the relationships between user programming language preferences. Extensive experiments have been conducted on the real-world dataset of GitHub. The results demonstrate that our framework significantly outperforms five competitive baselines.},
booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {1173–1176},
numpages = {4},
keywords = {user programming language preference, manifold regularization, open source repository, repository recommendation},
location = {Shinjuku, Tokyo, Japan},
series = {SIGIR '17}
}

@inproceedings{10.1145/2995306.2995314,
author = {McMillion, Brendan and Sullivan, Nick},
title = {Attacking White-Box AES Constructions},
year = {2016},
isbn = {9781450345767},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2995306.2995314},
doi = {10.1145/2995306.2995314},
abstract = {A white-box implementation of the Advanced Encryption Standard (AES) is a software implementation which aims to prevent recovery of the block cipher's master secret key. This paper refines the design criteria for white-box AES constructions by describing new attacks on past proposals which are conceptually very simple and introduces a new family of white-box AES constructions. Our attacks have a decomposition phase, followed by a disambiguation phase. The decomposition phase applies an SASAS-style cryptanalysis to convert the implementation into a simpler form, while the disambiguation phase converts the simpler form into a unique canonical form. It's then trivial to recover the master secret key of the implementation from its canonical form. We move on to discuss the hardness of SPN disambiguation as a problem on its own, and how to construct white-boxes from it. Implementations of all described attacks and constructions are provided on GitHub at https://github.com/OpenWhiteBox/},
booktitle = {Proceedings of the 2016 ACM Workshop on Software PROtection},
pages = {85–90},
numpages = {6},
keywords = {white-box, AES, self-equivalences},
location = {Vienna, Austria},
series = {SPRO '16}
}

@inproceedings{10.1145/2884781.2884790,
author = {Nadi, Sarah and Kr\"{u}ger, Stefan and Mezini, Mira and Bodden, Eric},
title = {Jumping through Hoops: Why Do Java Developers Struggle with Cryptography APIs?},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884790},
doi = {10.1145/2884781.2884790},
abstract = {To protect sensitive data processed by current applications, developers, whether security experts or not, have to rely on cryptography. While cryptography algorithms have become increasingly advanced, many data breaches occur because developers do not correctly use the corresponding APIs. To guide future research into practical solutions to this problem, we perform an empirical investigation into the obstacles developers face while using the Java cryptography APIs, the tasks they use the APIs for, and the kind of (tool) support they desire. We triangulate data from four separate studies that include the analysis of 100 StackOverflow posts, 100 GitHub repositories, and survey input from 48 developers. We find that while developers find it difficult to use certain cryptographic algorithms correctly, they feel surprisingly confident in selecting the right cryptography concepts (e.g., encryption vs. signatures). We also find that the APIs are generally perceived to be too low-level and that developers prefer more task-based solutions.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {935–946},
numpages = {12},
keywords = {cryptography, empirical software engineering, API misuse},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2676723.2677287,
author = {Zadrozny, Wlodek W. and Gallagher, Sean and Shalaby, Walid and Avadhani, Adarsh},
title = {Simulating IBM Watson in the Classroom},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2677287},
doi = {10.1145/2676723.2677287},
abstract = {IBM Watson exemplifies multiple innovations in natural language processing and question answering. In addition, Watson uses most of the known techniques in these two domains as well as many methods from related domains. Hence, there is pedagogical value in a rigorous understanding of its function. The paper provides the description of a text analytics course focused on building a simulator of IBM Watson, conducted in Spring 2014 at UNC Charlotte. We believe this is the first time a simulation containing all the major Watson components was created in a university classroom. The system achieved a respectable (close to) 20% accuracy on Jeopardy! questions, and there remain many known and new avenues of improving performance that can be explored in the future. The code and documentation are available on GitHub. The paper is a joint effort of the teacher and some of the students who were leading teams implementing component technologies, and therefore deeply involved in making the class successful.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {72–77},
numpages = {6},
keywords = {ibm watson, qa, student research, question answering},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1109/APSEC.2014.57,
author = {Yu, Yue and Wang, Huaimin and Yin, Gang and Ling, Charles X.},
title = {Who Should Review This Pull-Request: Reviewer Recommendation to Expedite Crowd Collaboration},
year = {2014},
isbn = {9781479974269},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/APSEC.2014.57},
doi = {10.1109/APSEC.2014.57},
abstract = {Github facilitates the pull-request mechanism as an outstanding social coding paradigm by integrating with social media. The review process of pull-requests is a typical crowd sourcing job which needs to solicit opinions of the community. Recommending appropriate reviewers can reduce the time between the submission of a pull-request and the actual review of it. In this paper, we firstly extend the traditional Machine Learning (ML) based approach of bug triaging to reviewer recommendation. Furthermore, we analyze social relations between contributors and reviewers, and propose a novel approach to recommend highly relevant reviewers by mining comment networks (CN) of given projects. Finally, we demonstrate the effectiveness of these two approaches with quantitative evaluations. The results show that CN-based approach achieves a significant improvement over the ML-based approach, and on average it reaches a precision of 78% and 67% for top-1 and top-2 recommendation respectively, and a recall of 77% for top-10 recommendation.},
booktitle = {Proceedings of the 2014 21st Asia-Pacific Software Engineering Conference - Volume 01},
pages = {335–342},
numpages = {8},
keywords = {Comment Network, Reviewer Recommendation, Pull-request, Social Coding},
series = {APSEC '14}
}

@inproceedings{10.1145/2647868.2654889,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
year = {2014},
isbn = {9781450330633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647868.2654889},
doi = {10.1145/2647868.2654889},
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments.Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
booktitle = {Proceedings of the 22nd ACM International Conference on Multimedia},
pages = {675–678},
numpages = {4},
keywords = {parallel computation, open source, neural networks, computer vision, machine learning},
location = {Orlando, Florida, USA},
series = {MM '14}
}

@inproceedings{10.1145/3405656.3418709,
author = {Tourani, Reza and Torres, George and Misra, Satyajayant},
title = {PERSIA: A PuzzlE-Based InteReSt FloodIng Attack Countermeasure},
year = {2020},
isbn = {9781450380409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3405656.3418709},
doi = {10.1145/3405656.3418709},
abstract = {With the proliferation of smart and connected mobile, wireless devices at the edge, Distributed Denial of Service (DDoS) attacks are increasing. Weak security, improper commissioning, and the fast, non-standardized growth of the IoT industry are the major contributors to the recent DDoS attacks, e.g., Mirai Botnet attack on Dyn and Memcached attack on GitHub. Similar to UDP/TCP flooding (common DDoS attack vector), request flooding attack is the primary DDoS vulnerability in the Named-Data Networking (NDN) architecture.In this paper, we propose PERSIA, a distributed request flooding prevention and mitigation framework for NDN-enabled ISPs, to ward-off attacks at the edge. PERSIA's edge-centric attack prevention mechanism eliminates the possibility of successful attacks from malicious end hosts. In the presence of compromised infrastructure (routers), PERSIA dynamically deploys an in-network mitigation strategy to minimize the attack's magnitude. Our experimentation demonstrates PERSIA's resiliency and effectiveness in preventing and mitigating DDoS attacks while maintaining legitimate users' quality of experience (&gt; 99.92% successful packet delivery rate).},
booktitle = {Proceedings of the 7th ACM Conference on Information-Centric Networking},
pages = {117–128},
numpages = {12},
keywords = {edge security, Information-centric networking, in-network mitigation, distributed denial of service attack, network edge, edge-centric prevention},
location = {Virtual Event, Canada},
series = {ICN '20}
}

@inproceedings{10.1145/3379597.3387441,
author = {Nakamaru, Tomoki and Matsunaga, Tomomasa and Yamazaki, Tetsuro and Akiyama, Soramichi and Chiba, Shigeru},
title = {An Empirical Study of Method Chaining in Java},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387441},
doi = {10.1145/3379597.3387441},
abstract = {While some promote method chaining as a good practice for improving code readability, others refer to it as a bad practice that worsens code quality. In this paper, we first investigate whether method chaining is a programming style accepted by real-world programmers. To answer this question, we collected 2,814 Java repositories on GitHub and analyzed historical trends in the frequency of method chaining. The results of our analysis revealed the increasing use of method chaining; 23.1% of method invocations were part of method chains in 2018, whereas only 16.0% were such invocations in 2010. We then explore language features that are helpful to the method-chaining style but have not been supported yet in Java. For this aim, we conducted manual inspections of method chains that are randomly sampled from the collected repositories. We also estimated how effective they are to encourage the method-chaining style if they are adopted in Java.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {93–102},
numpages = {10},
keywords = {Quantitative analysis, Method chaining, Repository mining},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3361242.3362774,
author = {Liu, Bohong and Wang, Tao and Zhang, Xunhui and Fan, Qiang and Yin, Gang and Deng, Jinsheng},
title = {A Neural-Network Based Code Summarization Approach by Using Source Code and Its Call Dependencies},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3362774},
doi = {10.1145/3361242.3362774},
abstract = {Code summarization aims at generating natural language abstraction for source code, and it can be of great help for program comprehension and software maintenance. The current code summarization approaches have made progress with neural-network. However, most of these methods focus on learning the semantic and syntax of source code snippets, ignoring the dependency of codes. In this paper, we propose a novel method based on neural-network model using the knowledge of the call dependency between source code and its related codes. We extract call dependencies from the source code, transform it as a token sequence of method names, and leverage the Seq2Seq model for code summarization using the combination of source code and call dependency information. About 100,000 code data is collected from 1,000 open source Java proejects on github for experiment. The large-scale code experiment shows that by considering not only the code itself but also the codes it called, the code summarization model can be improved with the BLEU score to 33.08.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {12},
numpages = {10},
keywords = {Call Dependency, Code Summarization, Open Source, Neural Network},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3301506.3301532,
author = {Li, Renqiang and Liu, Hong and Wang, Xiangdong and Qian, Yueliang},
title = {DSBI: Double-Sided Braille Image Dataset and Algorithm Evaluation for Braille Dots Detection},
year = {2018},
isbn = {9781450366137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301506.3301532},
doi = {10.1145/3301506.3301532},
abstract = {Braille is an effective way for the visually impaired to learn knowledge and obtain information. Braille image recognition aims to automatically detect Braille dots in the whole Braille image. There is no available public datasets for Braille image recognition to push relevant research and evaluate algorithms. This paper constructs a large-scale Double-Sided Braille Image dataset DSBI with detailed Braille recto dots, verso dots and Braille cells annotation. To quickly annotate Braille images, an auxiliary annotation strategy is proposed, which adopts initial automatic detection of Braille dots and modifies annotation results by convenient human-computer interaction method. This labeling strategy can averagely increase label efficiency by six times for recto dots annotation in one Braille image. Braille dots detection is the core and basic step for Braille image recognition. This paper also evaluates some Braille dots detection methods on our dataset DSBI and gives the benchmark performance of recto dots detection. We have released our Braille images dataset on the GitHub website.},
booktitle = {Proceedings of the 2018 the 2nd International Conference on Video and Image Processing},
pages = {65–69},
numpages = {5},
keywords = {auxiliary annotation, Braille dots detection, benchmark, dataset, Braille image},
location = {Hong Kong, Hong Kong},
series = {ICVIP 2018}
}

@inproceedings{10.1145/3231644.3231673,
author = {Murthy, Sean and Figueroa, Andrew and Rollo, Steven},
title = {Toward a Large-Scale Open Learning System for Data Management},
year = {2018},
isbn = {9781450358866},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3231644.3231673},
doi = {10.1145/3231644.3231673},
abstract = {This paper describes ClassDB, a free and open source system to enable large-scale learning of data management. ClassDB is different from existing solutions in that the same system supports a wide range of data-management topics from introductory SQL to advanced "native analytics" where code in SQL and non-SQL languages (Python and R) run inside a database management system. Each student/team maintains their own sandbox which instructors can read and provide feedback. Both students and instructors can review activity logs to analyze progress and determine future course of action. ClassDB is currently in its second pilot and is scheduled for a larger trial later this year. After the trials, ClassDB will be made available to about 4,000 students in the university system, which comprises four universities and 12 community colleges. ClassDB is built in collaboration with students employing modern DevOps processes. Its source code and documentation are available in a public GitHub repository. ClassDB is work in progress.},
booktitle = {Proceedings of the Fifth Annual ACM Conference on Learning at Scale},
articleno = {16},
numpages = {4},
keywords = {scalability, classDB, data management, learning, FOSS},
location = {London, United Kingdom},
series = {L@S '18}
}

@inproceedings{10.1145/3183440.3183471,
author = {Wang, Cong and Jiang, Yu and Zhao, Xibin and Song, Xiaoyu and Gu, Ming and Sun, Jiaguang},
title = {Weak-Assert: A Weakness-Oriented Assertion Recommendation Toolkit for Program Analysis},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183471},
doi = {10.1145/3183440.3183471},
abstract = {Assertions are helpful in program analysis, such as software testing and verification. The most challenging part of automatically recommending assertions is to design the assertion patterns and to insert assertions in proper locations. In this paper, we develop Weak-Assert1, a weakness-oriented assertion recommendation toolkit for program analysis of C code. A weakness-oriented assertion is an assertion which can help to find potential program weaknesses. Weak-Assert uses well-designed patterns to match the abstract syntax trees of source code automatically. It collects significant messages from trees and inserts assertions into proper locations of programs. These assertions can be checked by using program analysis techniques. The experiments are set up on Juliet test suite and several actual projects in Github. Experimental results show that Weak-Assert helps to find 125 program weaknesses in 26 actual projects. These weaknesses are confirmed manually to be triggered by some test cases.The address of the abstract demo video is: https://youtu.be/_RWC4GJvRWc},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {69–72},
numpages = {4},
keywords = {assertion recommendation, formal program verification, program testing, program weakness},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1109/ICCPS.2018.00050,
author = {Lukina, Anna and Kumar, Arjun and Schmittle, Matt and Singh, Abhijeet and Das, Jnaneshwar and Rees, Stephen and Buskirk, Christopher P. and Sztipanovits, Janos and Grosu, Radu and Kumar, Vijay},
title = {Formation Control and Persistent Monitoring in the OpenUAV Swarm Simulator on the NSF CPS-VO},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCPS.2018.00050},
doi = {10.1109/ICCPS.2018.00050},
abstract = {Simulation tools offer a low barrier to entry and enable testing and validation before field trials. However, most of the well-known simulators today are challenging to use at scale due to the need for powerful computers and the time required for initial set up. The OpenUAV Swarm Simulator was developed to address these challenges, enabling multi-UAV simulations on the cloud through the NSF CPS-VO. We leverage the Containers as a Service (CaaS) technology to enable students and researchers carry out simulations on the cloud on demand. We have based our framework on open-source tools including ROS, Gazebo, Docker, and the PX4 flight stack, and we designed the simulation framework so that it has no special hardware requirements. The demo and poster will showcase UAV swarm trajectory optimization, and multi-UAV persistent monitoring on the CPS-VO. The code for the simulator is available on GitHub: https://github.com/Open-UAV.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {353–354},
numpages = {2},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@inproceedings{10.1145/3082031.3083236,
author = {Xu, Guanshuo},
title = {Deep Convolutional Neural Network to Detect J-UNIWARD},
year = {2017},
isbn = {9781450350617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3082031.3083236},
doi = {10.1145/3082031.3083236},
abstract = {This paper presents an empirical study on applying convolutional neural networks (CNNs) to detecting J-UNIWARD -- one of the most secure JPEG steganographic method. Experiments guiding the architectural design of the CNNs have been conducted on the JPEG compressed BOSSBase containing 10,000 covers of size 512\texttimes{}512. Results have verified that both the pooling method and the depth of the CNNs are critical for performance. Results have also proved that a 20-layer CNN, in general, outperforms the most sophisticated feature-based methods, but its advantage gradually diminishes on hard-to-detect cases. To show that the performance generalizes to large-scale databases and to different cover sizes, one experiment has been conducted on the CLS-LOC dataset of ImageNet containing more than one million covers cropped to unified size of 256\texttimes{}256. The proposed 20-layer CNN has cut the error achieved by a CNN recently proposed for large-scale JPEG steganalysis by 35%. Source code is available via GitHub: https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis},
booktitle = {Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security},
pages = {67–73},
numpages = {7},
keywords = {j-uniward, deep cnn, steganalysis, deep learning, cnn},
location = {Philadelphia, Pennsylvania, USA},
series = {IH&amp;MMSec '17}
}

@inproceedings{10.1145/2976749.2978403,
author = {Wressnegger, Christian and Yamaguchi, Fabian and Maier, Alwin and Rieck, Konrad},
title = {Twice the Bits, Twice the Trouble: Vulnerabilities Induced by Migrating to 64-Bit Platforms},
year = {2016},
isbn = {9781450341394},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976749.2978403},
doi = {10.1145/2976749.2978403},
abstract = {Subtle flaws in integer computations are a prime source for exploitable vulnerabilities in system code. Unfortunately, even code shown to be secure on one platform can be vulnerable on another, making the migration of code a notable security challenge. In this paper, we provide the first study on how code that works as expected on 32-bit platforms can become vulnerable on 64-bit platforms. To this end, we systematically review the effects of data model changes between platforms. We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable ("Jessie") and 200 popular open-source projects hosted on GitHub. Moreover, we discuss 64-bit migration vulnerabilities that have been discovered as part of our study, including vulnerabilities in Chromium, the Boost C++ Libraries, libarchive, the Linux Kernel, and zlib.},
booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
pages = {541–552},
numpages = {12},
keywords = {integer-based vulnerabilities, data models, software security},
location = {Vienna, Austria},
series = {CCS '16}
}

@inproceedings{10.1109/ICSME.2014.76,
author = {Yan, Yan and Menarini, Massimiliano and Griswold, William},
title = {Mining Software Contracts for Software Evolution},
year = {2014},
isbn = {9781479961467},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSME.2014.76},
doi = {10.1109/ICSME.2014.76},
abstract = {Maintenance and evolution are important parts for all successful software projects. In recent years, version control systems have played a key role in software development process. Not only do they provide a means to coordinate programmers, organize and manage source code, but they also persist the evolution history of the source code into their software repositories. Mining software repositories has provided many insights on the evolution of software, both for researchers and practitioners. In this paper we propose that versioned software contracts--mined from software repositories--can be a powerful tool for better understanding and supporting software evolution. Tooling support is critical, due to the complexities of configuring, compiling, and running the software to produce meaningful inferred contracts. This paper contributes both techniques and tool support for downloading, building, and analyzing open source software from social coding sites like GitHub. The tool automatically produces a description of software evolution represented by versions of program invariants.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution},
pages = {471–475},
numpages = {5},
keywords = {version control, program analysis, software evolution, software testing, contracts},
series = {ICSME '14}
}

@inproceedings{10.1145/2568225.2568305,
author = {Singer, Leif and Figueira Filho, Fernando and Storey, Margaret-Anne},
title = {Software Engineering at the Speed of Light: How Developers Stay Current Using Twitter},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568305},
doi = {10.1145/2568225.2568305},
abstract = { The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {211–221},
numpages = {11},
keywords = {Learning, Awareness, Microblogging, Twitter, Social Media},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3306446.3340825,
author = {Gasparini, Mattia and Izquierdo, Javier Luis C\'{a}novas and Claris\'{o}, Robert and Brambilla, Marco and Cabot, Jordi},
title = {Analyzing Rich-Club Behavior in Open Source Projects},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340825},
doi = {10.1145/3306446.3340825},
abstract = {The network of collaborations in an open source project can reveal relevant emergent properties that influence its prospects of success. In this work, we analyze open source projects to determine whether they exhibit a rich-club behavior, i.e., a phenomenon where contributors with a high number of collaborations (i.e., strongly connected within the collaboration network) are likely to cooperate with other well-connected individuals. The presence or absence of a rich-club has an impact on the sustainability and robustness of the project.For this analysis, we build and study a dataset with the 100 most popular projects in GitHub, exploiting connectivity patterns in the graph structure of collaborations that arise from commits, issues and pull requests. Results show that rich-club behavior is present in all the projects, but only few of them have an evident club structure. We compute coefficients both for single source graphs and the overall interaction graph, showing that rich-club behavior varies across different layers of software development. We provide possible explanations of our results, as well as implications for further analysis.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {6},
numpages = {9},
keywords = {network analysis, GitHub, rich-club coefficient, open source},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1145/3322790.3330596,
author = {David, Andrea and Souppe, Mariette and Jimenez, Ivo and Obraczka, Katia and Mansfield, Sam and Veenstra, Kerry and Maltzahn, Carlos},
title = {Reproducible Computer Network Experiments: A Case Study Using Popper},
year = {2019},
isbn = {9781450367561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322790.3330596},
doi = {10.1145/3322790.3330596},
abstract = {Computer network research experiments can be broadly grouped in three categories: simulated, controlled, and real-world experiments. Simulation frameworks, experiment testbeds and measurement tools, respectively, are commonly used as the platforms for carrying out network experiments. In many cases, given the nature of computer network experiments, properly configuring these platforms is a complex and time-consuming task, which makes replicating and validating research results quite challenging. This complexity can be reduced by leveraging tools that enable experiment reproducibility. In this paper, we show how a recently proposed reproducibility tool called Popper facilitates the reproduction of networking experiments. In particular, we detail the steps taken to reproduce results in two published articles that rely on simulations. The outcome of this exercise is a generic workflow for carrying out network simulation experiments. In addition, we briefly present two additional Popper workflows for running experiments on controlled testbeds, as well as studies that gather real-world metrics (all code is publicly available on Github). We close by providing a list of lessons we learned throughout this process.},
booktitle = {Proceedings of the 2nd International Workshop on Practical Reproducible Evaluation of Computer Systems},
pages = {29–34},
numpages = {6},
keywords = {network experiment simulation, popper, reproducible network experiments, software automation},
location = {Phoenix, AZ, USA},
series = {P-RECS '19}
}

@inproceedings{10.5555/3155562.3155583,
author = {Jiang, Siyuan and Armaly, Ameer and McMillan, Collin},
title = {Automatically Generating Commit Messages from Diffs Using Neural Machine Translation},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically ``translate'' diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {135–146},
numpages = {12},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1109/MSR.2017.31,
author = {Manglaviti, Marco and Coronado-Montoya, Eduardo and Gallaba, Keheliya and McIntosh, Shane},
title = {An Empirical Study of the Personnel Overhead of Continuous Integration},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.31},
doi = {10.1109/MSR.2017.31},
abstract = {Continuous Integration (CI) is a software development practice where changes to the codebase are compiled and automatically checked for software quality issues. Like any software artifact (e.g., production code, build specifications), CI systems require an investment of development resources in order to keep them running smoothly.In this paper, we examine the human resources that are associated with developing and maintaining CI systems. Through the analysis of 1,279 GitHub repositories that adopt Travis CI (a popular CI service provider), we observe that: (i) there are 0 to 6 unique contributors to CI-related development in any 30-day period, regardless of project size; and (ii) the total number of CI developers has an upper bound of 15 for 99.2% of the studied projects, regardless of overall team size. These results indicate that service-based CI systems only require a small proportion of the development team to contribute. These costs are almost certainly outweighed by the reported benefits of CI (e.g., team communication and time-to-market for new content).},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {471–474},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3076246.3076252,
author = {Miao, Hui and Li, Ang and Davis, Larry S. and Deshpande, Amol},
title = {On Model Discovery For Hosted Data Science Projects},
year = {2017},
isbn = {9781450350266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3076246.3076252},
doi = {10.1145/3076246.3076252},
abstract = {Alongside developing systems for scalable machine learning and collaborative data science activities, there is an increasing trend toward publicly shared data science projects, hosted in general or dedicated hosting services, such as GitHub and DataHub. The artifacts of the hosted projects are rich and include not only text files, but also versioned datasets, trained models, project documents, etc. Under the fast pace and expectation of data science activities, model discovery, i.e., finding relevant data science projects to reuse, is an important task in the context of data management for end-to-end machine learning. In this paper, we study the task and present the ongoing work on ModelHub Discovery, a system for finding relevant models in hosted data science projects. Instead of prescribing a structured data model for data science projects, we take an information retrieval approach by decomposing the discovery task into three major steps: project query and matching, model comparison and ranking, and processing and building ensembles with returned models. We describe the motivation and desiderata, propose techniques, and present opportunities and challenges for model discovery for hosted data science projects.},
booktitle = {Proceedings of the 1st Workshop on Data Management for End-to-End Machine Learning},
articleno = {6},
numpages = {4},
location = {Chicago, IL, USA},
series = {DEEM'17}
}

@inproceedings{10.1145/2989238.2989244,
author = {Shakiba, Abbas and Green, Robert and Dyer, Robert},
title = {FourD: Do Developers Discuss Design? Revisited},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989244},
doi = {10.1145/2989238.2989244},
abstract = {Software repositories contain a variety of information that can be mined and utilized to enhance software engineering processes. Patterns stored in software repository meta-data can provide useful and informative information about different aspects of a project, particularly those that may not be obvious for developers. One such aspect is the role of software design in a project. The messages connected to each commit in the repository note not only what changes have been made to project files, but potentially if those changes have somehow manipulated the design of the software.In this paper, a sample of commit messages from a random sample of projects on GitHub and SourceForge are manually classified as "design" or "non-design" based on a survey. The resulting data is then used to train multiple machine learning algorithms in order to determine if it is possible to predict whether or not a single commit is discussing software design. Our results show the Random Forest classifier performed best on our combined data set with a G-mean of 75.01.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {43–46},
numpages = {4},
keywords = {software design, machine-learning, Boa, mining},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/3412569.3412576,
author = {Setia, Simran and Iyengar, S. R.S. and Verma, Amit Arjun},
title = {QWiki: Need for QnA &amp; Wiki to Co-Exist},
year = {2020},
isbn = {9781450387798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412569.3412576},
doi = {10.1145/3412569.3412576},
abstract = {Access to knowledge has never been as easy and quick as it has been in the 21st century. With the advent of the Internet and crowd-sourced knowledge building portals such as Wikipedia, Stack Exchange, Quora, and GitHub, information is just a click away. It is interesting to observe that the crowd builds these information repositories and not the experts. Information accumulation on a wiki-like portal and discussions on the QnA forum function independently as collaborative knowledge building practices. There is a need to understand the best possible practices to acquire and maintain knowledge in such crowdsourced portals. In this paper, we introduce QWiki, a novel approach of integrating a wiki-like portal and a QnA forum, seeking the union of aforementioned independent collaborative practices. The experimental analysis demonstrates that QWiki helps in knowledge acquisition and knowledge building process. The proposed model highlights the importance of interaction between a wiki-like portal and a QnA forum.},
booktitle = {Proceedings of the 16th International Symposium on Open Collaboration},
articleno = {9},
numpages = {12},
keywords = {crowd, Wiki Portal, QnA Forum, knowledge building, triggering},
location = {Virtual conference, Spain},
series = {OpenSym 2020}
}

@inproceedings{10.1145/3407197.3407211,
author = {Lotfi Rezaabad, Ali and Vishwanath, Sriram},
title = {Long Short-Term Memory Spiking Networks and Their Applications},
year = {2020},
isbn = {9781450388511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407197.3407211},
doi = {10.1145/3407197.3407211},
abstract = { Recent advances in event-based neuromorphic systems have resulted in significant interest in the use and development of spiking neural networks (SNNs). However, the non-differentiable nature of spiking neurons makes SNNs incompatible with conventional backpropagation techniques. In spite of the significant progress made in training conventional deep neural networks (DNNs), training methods for SNNs still remain relatively poorly understood. In this paper, we present a novel framework for training recurrent SNNs. Analogous to the benefits presented by recurrent neural networks (RNNs) in learning time series models within DNNs, we develop SNNs based on long short-term memory (LSTM) networks. We show that LSTM spiking networks learn the timing of the spikes and temporal dependencies. We also develop a methodology for error backpropagation within LSTM-based SNNs. The developed architecture and method for backpropagation within LSTM-based SNNs enable them to learn long-term dependencies with comparable results to conventional LSTMs. Code is available on github; https://github.com/AliLotfi92/SNNLSTM },
booktitle = {International Conference on Neuromorphic Systems 2020},
articleno = {3},
numpages = {9},
location = {Oak Ridge, TN, USA},
series = {ICONS 2020}
}

@inproceedings{10.1145/3368926.3369733,
author = {Nguyen, Ngoc-Hoa and Le, Viet-Ha and Phung, Van-On and Du, Phuong-Hanh},
title = {Toward a Deep Learning Approach for Detecting PHP Webshell},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369733},
doi = {10.1145/3368926.3369733},
abstract = {The most efficient way of securing Web applications is searching and eliminating threats therein (from both malwares and vulnerabilities). In case of having Web application source codes, Web security can be improved by performing the task to detecting malicious codes, such as Web shells. In this paper, we proposed a model using a deep learning approach to detect and identify the malicious codes inside PHP source files. Our method relies on (i) pattern matching techniques by applying Yara rules to build a malicious and benign datasets, (ii) converting the PHP source codes to a numerical sequence of PHP opcodes and (iii) applying the Convolutional Neural Network model to predict a PHP file whether embedding a malicious code such as a webshell. Thus, we validate our approach with different webshell collections from reliable source published in Github. The experiment results show that the proposed method achieved the accuracy of 99.02% with 0.85% false positive rate.},
booktitle = {Proceedings of the Tenth International Symposium on Information and Communication Technology},
pages = {514–521},
numpages = {8},
keywords = {webshell detection, deep learning, yara rules, opcode sequence, pattern matching, CNN},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT 2019}
}

@inproceedings{10.1145/3362789.3362877,
author = {Guerrero-Higueras, \'{A}ngel Manuel and S\'{a}nchez-Gonz\'{a}lez, Lidia and Conde, Miguel \'{A}ngel and Lera, Francisco J. Rodr\'{\i}guez and Castej\'{o}n-Limas, Manuel and Petkov, Nicolai},
title = {Facilitating the Learning Process in Parallel Computing by Using Instant Messaging},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362877},
doi = {10.1145/3362789.3362877},
abstract = {Parallel Programming skills may require long time to acquire. "Think in parallel" is a skill which requires time, effort, and experience. In this work, we propose to facilitate the learning process in parallel programming by using instant messaging by students. Our aim is to find out if students' interaction through instant messaging is beneficial for the learning process. We asked several students of an HPC course of the Master's degree in Computer Science to develop a specific parallel application, each of them using a different application program interface: OpenMP, MPI, CUDA, or OpenCL. Even though the used APIs are different, there are common points in the design process. We proposed to these students to interact with each other by using Gitter, an instant messaging tool for GitHub users. Our analysis of the communications and results demonstrate that the direct interaction of students through the Gitter tool has a positive impact on the learning process.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {558–563},
numpages = {6},
keywords = {Instant Messaging, High-performance Computing, Parallel Programming},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/3307339.3343174,
author = {Tillquist, Richard C.},
title = {Low-Dimensional Representation of Biological Sequence Data},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343174},
doi = {10.1145/3307339.3343174},
abstract = {Systems of interest in bioinformatics and computational biology tend to be large, complex, interdependent, and stochastic. As our ability to collect sequence data at finer resolutions improves, we can better understand and predict system behavior under different conditions. Machine learning algorithms are a powerful set of tools designed to help with this understanding. However, many of the most effective of these algorithms are not immediately amenable to application on symbolic data. It is often necessary to map biological symbols to real vectors before performing analysis or prediction using sequence data. This tutorial will cover several techniques for embedding sequence data. Common methods utilizing k-mer count and binary vector representations will be addressed along with state of the art methods based on neural networks, like BioVec, and graph embeddings, like Node2Vec and multilateration. This tutorial is in collaboration with M. Lladser. Slides, datasets, and code from the tutorial will be made freely available for future use on GitHub.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {555},
numpages = {1},
keywords = {sequence data, multilateration, graph embeddings, neural networks, symbolic data science},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/3195538.3195543,
author = {Freudenstein, Dietmar and Radduenz, Jeannette and Junker, Maximilian and Eder, Sebastian and Hauptmann, Benedikt},
title = {Automated Test-Design from Requirements: The Specmate Tool},
year = {2018},
isbn = {9781450357494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195538.3195543},
doi = {10.1145/3195538.3195543},
abstract = {Designing a small set of tests that nonetheless cover the requirements sufficiently is tantamount to keep costs for testing at bay while still maintaining the necessary quality. Engineering an optimal test-suite requires both, insight into the domain and the system under test, but also carefully examining the combinatorics inherent in the requirements. Especially the second part is a cognitive challenge and systematic methods are cumbersome when performed by hand. In this paper, we present Specmate, a tool that supports and partly automates the design of tests from requirements. It provides light-weight modeling techniques to capture requirements, test generation facilities to create test specifications and further supporting functions to derive test procedures or test-scripts from specifications. Specmate has been developed and evaluated in the context of one of the core business systems of Allianz Deutschland, a large insurance company. The source code is freely available at GitHub and an online-demo of Specmate is available at http://specmate.in.tum.de.},
booktitle = {Proceedings of the 5th International Workshop on Requirements Engineering and Testing},
pages = {5–8},
numpages = {4},
location = {Gothenburg, Sweden},
series = {RET '18}
}

@inproceedings{10.1109/CCGRID.2018.00090,
author = {Beineke, Kevin and Nothaas, Stefan and Sch\"{o}ttner, Michael},
title = {Efficient Messaging for Java Applications Running in Data Centers},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00090},
doi = {10.1109/CCGRID.2018.00090},
abstract = {Big data and large-scale Java applications often aggregate the resources of many servers. Low-latency and high-throughput network communication is important, if the applications have to process many concurrent interactive queries. We designed DXNet to address these challenges providing fast object de-/serialization, automatic connection management and zero-copy messaging. The latter includes sending of asynchronous messages as well as synchronous requests/responses and an event-driven message receiving approach. DXNet is optimized for small messages (&lt; 64 bytes) in order to support highly interactive web applications, e.g., graph-based information retrieval, but works well with larger messages (e.g., 8 MB) as well. DXNet is available as standalone component on Github and its modular design is open for different transports currently supporting Ethernet and InfiniBand. The evaluation with micro benchmarks and YCSB using Ethernet and InfiniBand shows request-response latencies sub 10 μs (round-trip) including object de-/serialization, as well as a maximum throughput of more than 9 GByte/s.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {589–598},
numpages = {10},
keywords = {ethernet networks, data centers, cloud computing, Java, message passing, InfiniBand},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/3379597.3387495,
author = {Spinellis, Diomidis and Kotti, Zoe and Kravvaritis, Konstantinos and Theodorou, Georgios and Louridas, Panos},
title = {A Dataset of Enterprise-Driven Open Source Software},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387495},
doi = {10.1145/3379597.3387495},
abstract = {We present a dataset of open source software developed mainly by enterprises rather than volunteers. This can be used to address known generalizability concerns, and, also, to perform research on open source business software development. Based on the premise that an enterprise's employees are likely to contribute to a project developed by their organization using the email account provided by it, we mine domain names associated with enterprises from open data sources as well as through white- and blacklisting, and use them through three heuristics to identify 17 264 enterprise GitHub projects. We provide these as a dataset detailing their provenance and properties. A manual evaluation of a dataset sample shows an identification accuracy of 89%. Through an exploratory data analysis we found that projects are staffed by a plurality of enterprise insiders, who appear to be pulling more than their weight, and that in a small percentage of relatively large projects development happens exclusively through enterprise insiders.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {533–537},
numpages = {5},
keywords = {EDGAR, open source software in business, Software engineering economics, SEC 10-K, Fortune Global 500, software ecosystems, dataset, SEC 20-F},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3384772.3385125,
author = {Frid-Jimenez, Amber and Carson, Jesi and Scott, Alanna and Khantidhara, Paninee and Elza, Dethe},
title = {Designing Participedia: A Collaborative Research Platform},
year = {2020},
isbn = {9781450376068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384772.3385125},
doi = {10.1145/3384772.3385125},
abstract = {A transformation of democratic governance is occurring as public participation empowers citizens to have their voices heard beyond the vote.&nbsp;Participedia is a research community and online crowdsourcing platform designed to document and share emerging knowledge about participatory democracy. Participedia's women-led Design &amp; Technology (D&amp;T) team used participatory design (PD) and feminist human computer interaction (HCI) strategies to evaluate Participedia's formerly proprietary website and design and build a new, open source platform.By shifting Participedia to an open source technological approach, the D&amp;T team deliberately created opportunities for women and students and initiated new collaborations through channels like Github. Key design improvements, such as improved accessibility and reducing bias in the data model of Participedia, further align the project with feminist values of equity, diversity and inclusion (EDI). The D&amp;T team is part of a new generation of designers and developers contributing to interdisciplinary research through design and technology for social good.},
booktitle = {Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 2},
pages = {21–25},
numpages = {5},
keywords = {Participatory Democracy, Open Source, Equity, Diversity, Inclusion, Participatory Design, Public Participation, Feminist Human Computer Interaction},
location = {Manizales, Colombia},
series = {PDC '20}
}

@inproceedings{10.5555/3370272.3370301,
author = {Grichi, Manel and Abidi, Mouna and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Khomh, Foutse},
title = {State of Practices of Java Native Interface},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {The use of the Java Native Interface (JNI) allows taking advantage of the existing libraries written in different programming languages for code reuse, performance, and security. Despite the importance of JNI in development, practices on its usages are not well studied yet. In this paper, we investigated the usage of JNI in 100 open source systems collected from OpenHub and Github, around 8k of source code files combined between Java and C/C++, including the Java class libraries part of the JDK v9. We identified the state of the practice in JNI systems by semi-automatically and manually analyzing the source code.Our qualitative analysis shows eleven JNI practices where they are mainly related to loading libraries, implementing native methods, exception management, return types, and local/global references management. Basing on our findings, we provided some suggestions and recommendations to developers to facilitate the debugging tasks of JNI in multi-language systems, which can also help them to deal with the Java and C memory.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {274–283},
numpages = {10},
keywords = {Java development kit, practices, usages, Java native interface, code analysis, multi-language systems},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3359591.3359735,
author = {Allamanis, Miltiadis},
title = {The Adverse Effects of Code Duplication in Machine Learning Models of Code},
year = {2019},
isbn = {9781450369954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359591.3359735},
doi = {10.1145/3359591.3359735},
abstract = {The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research.},
booktitle = {Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {143–153},
numpages = {11},
keywords = {machine learning, code naturalness, dataset collection, duplication, big code},
location = {Athens, Greece},
series = {Onward! 2019}
}

@inproceedings{10.1145/3335203.3335736,
author = {Lin, Yuzhen and Wang, Rangding and Yan, Diqun and Dong, Li and Zhang, Xueyuan},
title = {Audio Steganalysis with Improved Convolutional Neural Network},
year = {2019},
isbn = {9781450368216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3335203.3335736},
doi = {10.1145/3335203.3335736},
abstract = {Deep learning, especially the convolutional neural network (CNN), has enjoyed significant success in many fields, e.g., image recognition. Recently, CNN has successfully applied to multimedia steganalysis. However, the detection performance is still unsatisfactory. In this work, we propose an improved CNN-based method for audio steganalysis. Specifically, a special convolutional layer is first carefully designed, which could capture the minor steganographic noise. Then, a truncated linear unit is adapted to activate the output of shallow convolutional layer. In addition, we employ the average pooling to minimize the over-fitting risk. Finally, a parameter transfer strategy is adopted, aiming to boost the detection performance for the low embedding-rate cases. The experimental results evaluated on 30,000 audio clips verify the effectiveness of our method for a variety of embedding rates. Compared with the existing CNN-based steganalysis methods, our proposed method could achieve superior performance. To facilitate the reproducible research, the source code will be released at GitHub.},
booktitle = {Proceedings of the ACM Workshop on Information Hiding and Multimedia Security},
pages = {210–215},
numpages = {6},
keywords = {convolutional neural network, deep learning, audio steganalysis},
location = {Paris, France},
series = {IH&amp;MMSec'19}
}

@inproceedings{10.1109/ICSE.2019.00109,
author = {Nguyen, Phuong T. and Di Rocco, Juri and Di Ruscio, Davide and Ochoa, Lina and Degueule, Thomas and Di Penta, Massimiliano},
title = {FOCUS: A Recommender System for Mining API Function Calls and Usage Patterns},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00109},
doi = {10.1109/ICSE.2019.00109},
abstract = {Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate, accuracy, and execution time. Results indicate the suitability of context-aware collaborative-filtering recommender systems to provide API usage patterns.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1050–1060},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1109/MSR.2017.34,
author = {Soto, Mauricio and Coker, Zack and Goues, Claire Le},
title = {Analyzing the Impact of Social Attributes on Commit Integration Success},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.34},
doi = {10.1109/MSR.2017.34},
abstract = {As the software development community makes it easier to contribute to open source projects, the number of commits and pull requests keep increasing. However, this exciting growth renders it more difficult to only accept quality contributions. Recent research has found that both technical and social factors predict the success of project contributions on GitHub. We take this question a step further, focusing on predicting continuous integration build success based on technical and social factors involved in a commit. Specifically, we investigated if social factors (such as being a core member of the development team, having a large number of followers, or contributing a large number of commits) improve predictions of build success. We found that social factors cause a noticeable increase in predictive power (12%), core team members are more likely to pass the build tests (10%), and users with 1000 or more followers are more likely to pass the build tests (10%).},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {483–486},
numpages = {4},
keywords = {GitHub, Travis CI, social coding, predicting integration success, social attributes, social networks},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1109/ASEW.2015.21,
author = {Bogart, Christopher and Kastner, Christian and Herbsleb, James},
title = {When It Breaks, It Breaks: How Ecosystem Developers Reason about the Stability of Dependencies},
year = {2015},
isbn = {9781467397759},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASEW.2015.21},
doi = {10.1109/ASEW.2015.21},
abstract = {Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems. Negotiating change can be tricky: changes to one module may cause ripple effects to many other modules that depend on it, yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change. We argue that awareness mechanisms based on various notions of stability can enable developers to make decisions that are independent yet wise and provide stewardship rather than disruption to the ecosystem. In ongoing interviews with developers in two software ecosystems (CRAN and Node.js), we are finding that developers in fact struggle with change, that they often use adhoc mechanisms to negotiate change, and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload. We study the state of the art and current information needs and outline a vision toward a change-based awareness system.},
booktitle = {Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)},
pages = {86–89},
numpages = {4},
series = {ASEW '15}
}

@inproceedings{10.1145/2786805.2786827,
author = {Treude, Christoph and Figueira Filho, Fernando and Kulesza, Uir\'{a}},
title = {Summarizing and Measuring Development Activity},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786827},
doi = {10.1145/2786805.2786827},
abstract = { Software developers pursue a wide range of activities as part of their work, and making sense of what they did in a given time frame is far from trivial as evidenced by the large number of awareness and coordination tools that have been developed in recent years. To inform tool design for making sense of the information available about a developer's activity, we conducted an empirical study with 156 GitHub users to investigate what information they would expect in a summary of development activity, how they would measure development activity, and what factors influence how such activity can be condensed into textual summaries or numbers. We found that unexpected events are as important as expected events in summaries of what a developer did, and that many developers do not believe in measuring development activity. Among the factors that influence summarization and measurement of development activity, we identified development experience and programming languages. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {625–636},
numpages = {12},
keywords = {development activity, Summarization, empirical study},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1109/SEAA.2014.63,
author = {Paolo, Giampiero Di and Malavolta, Ivano and Muccini, Henry},
title = {How Do You Feel Today? Buggy!},
year = {2014},
isbn = {9781479957958},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2014.63},
doi = {10.1109/SEAA.2014.63},
abstract = {It is well-known that moods and emotions strongly affect our performances. Clearly, this holds also for software developers. Thus, modern managers, trainers, and coaches should be aware of moods and emotions of software developers in their teams. In this context, mining software repositories and social networks in combination can be an invaluable instrument for understanding how the moods and emotions of software developers impact their performance, even in real-time. In this paper, we propose our first steps in mining software repositories for (i) getting information about developers' moods and emotion throughout the development process, and (ii) investigating on the existence of the correlation between software developers' performance (in terms of their commits bugginess) and mood. For what concerns data sources, we use publicly-available information on GitHub for getting insights about the performance of software developers, while we semantically analyse developers' posts on Twitter for extracting their moods during the whole duration of the project.},
booktitle = {Proceedings of the 2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
pages = {391},
numpages = {1},
series = {SEAA '14}
}

@inproceedings{10.1145/3368089.3417942,
author = {Escobar-Vel\'{a}squez, Camilo and Riveros, Diego and Linares-V\'{a}squez, Mario},
title = {MutAPK 2.0: A Tool for Reducing Mutation Testing Effort of Android Apps},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417942},
doi = {10.1145/3368089.3417942},
abstract = {Mutation testing is a time consuming process because large sets of fault-injected-versions of an original app are generated and executed with the purpose of evaluating the quality of a given test suite. In the case of Android apps, recent studies even suggest that mutant generation and mutation testing effort could be greater when the mutants are generated at the APK level. To reduce that effort, useless (e.g., equivalent) mutants should be avoided and mutant selection techniques could be used to reduce the set of mutants used with mutation testing. However, despite the existence of mutation testing tools, none of those tools provides features for removing useless mutants and sampling mutant sets. In this paper, we present MutAPK 2.0, an improved version of our open source mutant generation tool (MutAPK) for Android apps at APK level. To the best of our knowledge, MutAPK 2.0 is the first tool that enables the removal of dead-code mutants, provides a set of mutant selection strategies, and removes automatically equivalent and duplicate mutants. MutAPK 2.0 is publicly available at GitHub: https://thesoftwaredesignlab.github.io/MutAPK/ VIDEO: https://thesoftwaredesignlab.github.io/MutAPK/video.html},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1611–1615},
numpages = {5},
keywords = {Dead code, Equivalent, Duplicate, Mutation Testing, Mutant Selection},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3341105.3373924,
author = {Mandal, Amit and Ferrara, Pietro and Khlyebnikov, Yuliy and Cortesi, Agostino and Spoto, Fausto},
title = {Cross-Program Taint Analysis for IoT Systems},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373924},
doi = {10.1145/3341105.3373924},
abstract = {Cross-program propagation of tainted data (such as sensitive information or user input) in an interactive IoT system is listed among the OWASP IoT top 10 most critical security risks. When programs run on distinct devices, as it occurs in IoT systems, they communicate through different channels in order to implement some functionality. Hence, in order to prove the overall system secure, an analysis must consider how these components interact. Standard taint analyses detect if a value coming from a source (such as methods that retrieve user input or sensitive data) flows into a sink (typically, methods that execute SQL queries or send data into the Internet), unsanitized (that is, not properly escaped). This work devises a cross-program taint analysis that leverages an existing intra-program taint analysis to detect security vulnerabilities in multiple communicating programs. The proposed framework has been implemented above the intra-program taint analysis of the Julia static analyzer. Preliminary experimental results on multi-program IoT systems, publicly available on GitHub, show that the technique is effective and detects inter-program flows of tainted data that could not be discovered by analyzing each program in isolation.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1944–1952},
numpages = {9},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3275219.3275223,
author = {Wang, Tao and Zhang, Yang and Yin, Gang and Yu, Yue and Wang, Huaimin},
title = {Who Will Become a Long-Term Contributor? A Prediction Model Based on the Early Phase Behaviors},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275223},
doi = {10.1145/3275219.3275223},
abstract = {The continuous contribution from peripheral participants is crucial for the success of open source projects. Thus, how to identify the potential Long-Term Contributors (LTC) early and retain them is of great importance. We propose a prediction model to measure the chance for an individual to become a LTC contributor through his capacity, willingness, and the opportunity to contribute at the time of joining. Using data of Rails hosted on GitHub, we find that the probability for a new joiner to become a LTC is associated with his willingness and environment. Specifically, future LTCs tend to be more active and show more community-oriented attitude than other joiners during their first month. This implies that the interaction between individual's attitude and project's climate are associated with the odds that an individual would become a valuable contributor or disengage from the project. We evaluated our prediction model by using the 10 cross-validation method. Results show that our model archives the mean AUC as 0.807, which is valuable for OSS projects to identify potential long-term contributors and adopt better strategies to retain them for continuous contribution.},
booktitle = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
articleno = {9},
numpages = {10},
keywords = {Developer behavior, GitHub, Long Term Contributor, Open Source},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3149457.3149480,
author = {Takizawa, Ryota and Kawashima, Hideyuki and Mitsuhashi, Ryuya and Tatebe, Osamu},
title = {Performing External Join Operator on PostgreSQL with Data Transfer Approach},
year = {2018},
isbn = {9781450353724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149457.3149480},
doi = {10.1145/3149457.3149480},
abstract = {With the development of sensing devices, the size of data managed by human being has been rapidly increasing. To manage such huge data, relational database management system (RDBMS) plays a key role. RDBMS models the real world data as n-ary relational tables. Join operator is one of the most important relational operators, and its acceleration has been studied widely and deeply. How can an RDBMS provide such an efficient join operator? The performance improvement of join operator has been deeply studied for a decade, and many techniques are proposed already. The problem that we face is how to actually use such excellent techniques in real RDBMSs. We propose to implement an efficient join technique by the data transfer approach. The approach makes a hook point inside an RDBMS internal, and pulls data streams from the operator pipeline in the RDBMS, and applies our original join operator to the data, and finally returns the result to the operator pipeline in the RDBMS. The result of the experiment showed that our proposed method achieved 1.42x speedup compared with PostgreSQL. Our code is available on GitHub.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {271–277},
numpages = {7},
keywords = {Relational Database, Parallel Hash Join, PostgreSQL},
location = {Chiyoda, Tokyo, Japan},
series = {HPC Asia 2018}
}

@inproceedings{10.1109/CloudCom.2015.26,
author = {Zhuang, Hao and Rahman, Rameez and Hui, Pan and Aberer, Karl},
title = {StoreSim: Optimizing Information Leakage in Multicloud Storage Services},
year = {2015},
isbn = {9781467395601},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CloudCom.2015.26},
doi = {10.1109/CloudCom.2015.26},
abstract = {Many schemes have been recently advanced for storing data on multiple clouds. Distributing data over different cloud storage providers (CSPs) automatically provides users with a certain degree of information leakage control, as no single point of attack can leak all user's information. However, unplanned distribution of data chunks can lead to high information disclosure even while using multiple clouds. In this paper, to address this problem we present StoreSim, an information leakage aware storage system in multicloud. StoreSim aims to store syntactically similar data on the same cloud, thus minimizing the user's information leakage across multiple clouds. We design an approximate algorithm to efficiently generate similarity-preserving signatures for data chunks based on MinHash and Bloom filter, and also design a function to compute the information leakage based on these signatures. Next, we present an effective storage plan generation algorithm based on clustering for distributing data chunks with minimal information leakage across multiple clouds. Finally, we evaluate our scheme using two real datasets from Wikipedia and GitHub. We show that our scheme can reduce the information leakage by up to 60% compared to unplanned placement.},
booktitle = {Proceedings of the 2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)},
pages = {379–386},
numpages = {8},
series = {CLOUDCOM '15}
}

@inproceedings{10.1145/3183713.3183726,
author = {Zhou, Yang and Yang, Tong and Jiang, Jie and Cui, Bin and Yu, Minlan and Li, Xiaoming and Uhlig, Steve},
title = {Cold Filter: A Meta-Framework for Faster and More Accurate Stream Processing},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183726},
doi = {10.1145/3183713.3183726},
abstract = {Approximate stream processing algorithms, such as Count-Min sketch, Space-Saving, etc., support numerous applications in databases, storage systems, networking, and other domains. However, the unbalanced distribution in real data streams poses great challenges to existing algorithms. To enhance these algorithms, we propose a meta-framework, called Cold Filter (CF), that enables faster and more accurate stream processing.Different from existing filters that mainly focus on hot items, our filter captures cold items in the first stage, and hot items in the second stage. Also, existing filters require two-direction communication - with frequent exchanges between the two stages; our filter on the other hand is one-direction - each item enters one stage at most once. Our filter can accurately estimate both cold and hot items, giving it a genericity that makes it applicable to many stream processing tasks. To illustrate the benefits of our filter, we deploy it on three typical stream processing tasks and experimental results show speed improvements of up to 4.7 times, and accuracy improvements of up to 51 times. All source code is made publicly available at Github.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {741–756},
numpages = {16},
keywords = {sketch, data streams, data structures, approximate algorithms},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1109/ICSE-C.2017.11,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K. and Lo, David},
title = {RACK: Code Search in the IDE Using Crowdsourced Knowledge},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.11},
doi = {10.1109/ICSE-C.2017.11},
abstract = {Traditional code search engines often do not perform well with natural language queries since they mostly apply keyword matching. These engines thus require carefully designed queries containing information about programming APIs for code search. Unfortunately, existing studies suggest that preparing an effective query for code search is both challenging and time consuming for the developers. In this paper, we propose a novel code search tool-RACK-that returns relevant source code for a given code search query written in natural language text. The tool first translates the query into a list of relevant API classes by mining keyword-API associations from the crowdsourced knowledge of Stack Overflow, and then applies the reformulated query to GitHub code search API for collecting relevant results. Once a query related to a programming task is submitted, the tool automatically mines relevant code snippets from thousands of open-source projects, and displays them as a ranked list within the context of the developer's programming environment-the IDE. Tool page: http://www.usask.ca/?masud.rahman/rack},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {51–54},
numpages = {4},
keywords = {crowdsourced knowledge, keyword-API association, code search, query reformulation, stack overflow},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1109/eScience.2014.54,
author = {Aierken, Ailifan and Davis, Delmar B. and Zhang, Qi and Gupta, Kriti and Wong, Alex and Asuncion, Hazeline U.},
title = {A Multi-Level Funneling Approach to Data Provenance Reconstruction},
year = {2014},
isbn = {9781479942879},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/eScience.2014.54},
doi = {10.1109/eScience.2014.54},
abstract = {When data are retrieved from a file storage system or the Internet, is there information about their provenance (i.e., their origin or history)? It is possible that data could have been copied from another source and then transformed. Often, provenance is not readily available for data sets created in the past. Solving such a problem is the motivation behind the 2014 Provenance Reconstruction Challenge. This challenge is aimed at recovering lost provenance for two data sets: one data set (WikiNews articles) in which a list of possible sources has been provided, and another data set (files from GitHub repositories) in which the file sources are not provided. To address this challenge, we present a multi-level funneling approach to provenance reconstruction, a technique that incorporates text processing techniques from different disciplines to approximate the provenance of a given data set. We built three prototypes using this technique and evaluated them using precision and recall metrics. Our preliminary results indicate that our technique is capable of reconstructing some of the lost provenance.},
booktitle = {Proceedings of the 2014 IEEE 10th International Conference on E-Science - Volume 02},
pages = {71–74},
numpages = {4},
keywords = {topic modeling, similarity metrics, semantic analysis, longest common subsequence, data provenance reconstruction, vector space model},
series = {E-SCIENCE '14}
}

@inproceedings{10.1145/2502081.2502228,
author = {Aamulehto, Rami and Kuhna, Mikko and Tarvainen, Jussi and Oittinen, Pirkko},
title = {Stage Framework: An HTML5 and CSS3 Framework for Digital Publishing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502228},
doi = {10.1145/2502081.2502228},
abstract = {In this paper we present Stage Framework, an HTML5 and CSS3 framework for digital book, magazine and newspaper publishing. The framework offers publishers the means and tools for publishing editorial content in the HTML5 format using a single web application. The approach is cross-platform and is based on open web standards. Stage Framework serves as an alternative for platform-specific native publications using pure HTML5 to deliver book, magazine and newspaper content while retaining the familiar gesture interaction of native applications. Available gesture actions include for example the page swipe and kinetic scrolling. The magazine browsing view relies entirely on CSS3 3D Transforms and Transitions, thus utilizing hardware acceleration in most devices and platforms. The web application also features a magazine stand which, can be used to offer issues of multiple publications. Developed as a part of master's thesis research, the framework has been published under the GPL and MIT licenses and is available to everyone via the framework website (http://stageframework.com) and the GitHub repository (http://github.com/ralatalo/stage).},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {851–854},
numpages = {4},
keywords = {tablet magazine, browser technology, HTML5, CSS3, web application, digital publishing},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3368089.3417928,
author = {Song, Yang and Chaparro, Oscar},
title = {BEE: A Tool for Structuring and Analyzing Bug Reports},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417928},
doi = {10.1145/3368089.3417928},
abstract = {This paper introduces BEE, a tool that automatically analyzes user-written bug reports and provides feedback to reporters and developers about the system’s observed behavior (OB), expected behavior (EB), and the steps to reproduce the bug (S2R). BEE employs machine learning to (i) detect if an issue describes a bug, an enhancement, or a question; (ii) identify the structure of bug descriptions by automatically labeling the sentences that correspond to the OB, EB, or S2R; and (iii) detect when bug reports fail to provide these elements. BEE is integrated with GitHub and offers a public web API that researchers can use to investigate bug management tasks based on bug reports. We evaluated BEE’s underlying models on more than 5k existing bug reports and found they can correctly detect OB, EB, and S2R sentences as well as missing information in bug reports. BEE is an open-source project that can be found at <a>https://git.io/JfFnN</a>. A screencast showing the full capabilities of BEE can be found at <a>https://youtu.be/8pC48f_hClw</a>.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1551–1555},
numpages = {5},
keywords = {bug report structure, Bug reporting, text analysis, bug report quality},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3394171.3414536,
author = {Zhang, Huaizheng and Li, Yuanming and Ai, Qiming and Luo, Yong and Wen, Yonggang and Jin, Yichao and Ta, Nguyen Binh Duong},
title = {Hysia: Serving DNN-Based Video-to-Retail Applications in Cloud},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3414536},
doi = {10.1145/3394171.3414536},
abstract = {Combining video streaming and online retailing (V2R) has been a growing trend recently. In this paper, we provide practitioners and researchers in multimedia with a cloud-based platform named Hysia for easy development and deployment of V2R applications. The system consists of: 1) a back-end infrastructure providing optimized V2R related services including data engine, model repository, model serving and content matching; and 2) an application layer which enables rapid V2R application prototyping. Hysia addresses industry and academic needs in large-scale multimedia by: 1) seamlessly integrating state-of-the-art libraries including NVIDIA video SDK, Facebook faiss, and gRPC; 2) efficiently utilizing GPU computation; and 3) allowing developers to bind new models easily to meet the rapidly changing deep learning (DL) techniques. On top of that, we implement an orchestrator for further optimizing DL model serving performance. Hysia has been released as an open source project on GitHub, and attracted considerable attention. We have published Hysia to DockerHub as an official image for seamless integration and deployment in current cloud environments.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4457–4460},
numpages = {4},
keywords = {video analysis, multimedia system, advertising, video shopping, open source software, cloud platform},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3238147.3240482,
author = {Hariri, Farah and Shi, August},
title = {SRCIROR: A Toolset for Mutation Testing of C Source Code and LLVM Intermediate Representation},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240482},
doi = {10.1145/3238147.3240482},
abstract = {We present SRCIROR (pronounced “sorcerer“), a toolset for performing mutation testing at the levels of C/C++ source code (SRC) and the LLVM compiler intermediate representation (IR). At the SRC level, SRCIROR identifies program constructs for mutation by pattern-matching on the Clang AST. At the IR level, SRCIROR directly mutates the LLVM IR instructions through LLVM passes. Our implementation enables SRCIROR to (1) handle any program that Clang can handle, extending to large programs with a minimal overhead, and (2) have a small percentage of invalid mutants that do not compile. SRCIROR enables performing mutation testing using the same classes of mutation operators at both the SRC and IR levels, and it is easily extensible to support more operators. In addition, SRCIROR can collect coverage to generate mutants only for covered code elements. Our tool is publicly available on GitHub (https://github.com/TestingResearchIllinois/srciror). We evaluate SRCIROR on Coreutils subjects. Our evaluation shows interesting differences between SRC and IR, demonstrating the value of SRCIROR in enabling mutation testing research across different levels of code representation.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {860–863},
numpages = {4},
keywords = {Software testing, mutation testing},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3377644.3377667,
author = {Duong-Trung, Nghia and Son, Ha Xuan and Le, Hai Trieu and Phan, Tan Tai},
title = {Smart Care: Integrating Blockchain Technology into the Design of Patient-Centered Healthcare Systems},
year = {2020},
isbn = {9781450377447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377644.3377667},
doi = {10.1145/3377644.3377667},
abstract = {Cross-institutional sharing of medical data is essential to provide e.ective collaborative treatment and clinical decisions for patients. Medical data privacy involves ensuring only authorized parties may access the health records under the awareness and approval of patients in any circumstances. This is crucial to any healthcare system because the protection of patients' clinical data is not only an ethical responsibility but also a legal mandate. Despite the importance of medical data sharing, today's healthcare systems have not provided enough protection of patients' sensitive information to be utilized deliberately or unintentionally. Hence, there is an urgent demand for a clinical transaction mechanism that allows patients to access, trace and control their health records. In this paper, the authors focus on several limitations in the literature and propose appropriate improvement in healthcare systems by (i) addressing information security and privacy, (ii) solving the lack of trust between providers, and (iii) encouraging scalability of healthcare interoperability. Building upon these key insights, we introduce several components of a patient-centered healthcare system using smart contracts via blockchain technology. A complete code solution is publicized on the authors' GitHub repository to engage further reproducibility and improvement.},
booktitle = {Proceedings of the 2020 4th International Conference on Cryptography, Security and Privacy},
pages = {105–109},
numpages = {5},
keywords = {Blockchain, Smart Contract, Healthcare System},
location = {Nanjing, China},
series = {ICCSP 2020}
}

@inproceedings{10.1109/ICSE-SEIS.2019.17,
author = {Ford, Denae and Behroozi, Mahnaz and Serebrenik, Alexander and Parnin, Chris},
title = {Beyond the Code Itself: How Programmers <i>Really</i> Look at Pull Requests},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS.2019.17},
doi = {10.1109/ICSE-SEIS.2019.17},
abstract = {Developers in open source projects must make decisions on contributions from other community members, such as whether or not to accept a pull request. However, secondary factors---beyond the code itself---can influence those decisions. For example, signals from GitHub profiles, such as a number of followers, activity, names, or gender can also be considered when developers make decisions. In this paper, we examine how developers use these signals (or not) when making decisions about code contributions. To evaluate this question, we evaluate how signals related to perceived gender identity and code quality influenced decisions on accepting pull requests. Unlike previous work, we analyze this decision process with data collected from an eye-tracker. We analyzed differences in what signals developers said are important for themselves versus what signals they actually used to make decisions about others. We found that after the code snippet (x = 57%), the second place programmers spent their time ixating is on supplemental technical signals (x = 32%), such as previous contributions and popular repositories. Diverging from what participants reported themselves, we also found that programmers ixated on social signals more than recalled.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Society},
pages = {51–60},
numpages = {10},
keywords = {code contributions, socio-technical ecosystems, transparency, open source software development, eye-tracking},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIS '19}
}

@inproceedings{10.1145/2964284.2973797,
author = {Rossetto, Luca and Giangreco, Ivan and Tanase, Claudiu and Schuldt, Heiko},
title = {Vitrivr: A Flexible Retrieval Stack Supporting Multiple Query Modes for Searching in Multimedia Collections},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973797},
doi = {10.1145/2964284.2973797},
abstract = {vitrivr is an open source full-stack content-based multimedia retrieval system with focus on video. Unlike the majority of the existing multimedia search solutions, vitrivr is not limited to searching in metadata, but also provides content-based search and thus offers a large variety of different query modes which can be seamlessly combined: Query by sketch, which allows the user to draw a sketch of a query image and/or sketch motion paths, Query by example, keyword search, and relevance feedback. The vitrivr architecture is self-contained and addresses all aspects of multimedia search, from offline feature extraction, database management to frontend user interaction. The system is composed of three modules: a web-based frontend which allows the user to input the query (e.g., add a sketch) and browse the retrieved results (vitrivr-ui), a database system designed for interactive search in large-scale multimedia collections (ADAM), and a retrieval engine that handles feature extraction and feature-based retrieval (Cineast). The vitrivr source is available on GitHub under the MIT open source (and similar) licenses and is currently undergoing several upgrades as part of the Google Summer of Code 2016.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1183–1186},
numpages = {4},
keywords = {content-based multimedia retrieval, multimedia search},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.5555/3021955.3022007,
author = {Oliveira, Johnatan and Fernandes, Eduardo and Souza, Mauricio and Figueiredo, Eduardo},
title = {A Method Based on Naming Similarity to Identify Reuse Opportunities},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Software reuse is a development strategy in which existing software components, called reusable assets, are used in the development of new software systems. There are many advantages of reuse in software development, such as minimization of development efforts and improvement of software quality. New methods for reusable asset extraction are essential to achieve these advantages. Extraction methods may be used in different contexts including software product lines derivation. However, few methods have been proposed in literature for reusable asset extraction and recommendation of these reuse opportunities. In this paper, we propose a method for extraction of reuse opportunities based on naming similarity of two types of object-oriented entities: classes and methods. Our method, called JReuse, computes a similarity function to identify similarly named classes and methods from a set of software systems from a domain. These classes and methods compose a repository with reuse opportunities. We also present a prototype tool to support the extraction by applying our method. We evaluate the method with 38 e-commerce information systems mined from GitHub. As a result, we observe that our method is able to identify classes and methods that are relevant in the e-commerce domain.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {305–312},
numpages = {8},
keywords = {reusable assets, tool, Software reuse, naming similarity},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1109/ICSM.2015.7332486,
author = {Linares-Vasquez, Mario and Vendome, Christopher and Luo, Qi and Poshyvanyk, Denys},
title = {How Developers Detect and Fix Performance Bottlenecks in Android Apps},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332486},
doi = {10.1109/ICSM.2015.7332486},
abstract = {Performance of rapidly evolving mobile apps is one of the top concerns for users and developers nowadays. Despite the efforts of researchers and mobile API designers to provide developers with guidelines and best practices for improving the performance of mobile apps, performance bottlenecks are still a significant and frequent complaint that impacts the ratings and apps' chances for success. However, little research has been done into understanding actual developers' practices for detecting and fixing performance bottlenecks in mobile apps. In this paper, we present the results of an empirical study aimed at studying and understanding these practices by surveying 485 open source Android app and library developers, and manually analyzing performance bugs and fixes in their app repositories hosted on GitHub. The paper categorizes actual practices and tools used by real developers while dealing with performance issues. In general, our findings indicate that developers heavily rely on user reviews and manual execution of the apps for detecting performance bugs. While developers also use available tools to detect performance bottlenecks, these tools are mostly for profiling and do not help in detecting and fixing performance issues automatically.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {352–361},
numpages = {10},
series = {ICSME '15}
}

@inproceedings{10.1145/3368089.3418539,
author = {Wessel, Mairieli},
title = {Enhancing Developers’ Support on Pull Requests Activities with Software Bots},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418539},
doi = {10.1145/3368089.3418539},
abstract = {Software bots are employed to support developers' activities, serving as conduits between developers and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save development cost, time, and effort, the bots' presence can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers enhance existing bots. Toward this end, we are interviewing maintainers, contributors, and bot developers to understand the problems in the human-bot interaction and how they affect the collaboration in a project. Afterward, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1674–1677},
numpages = {4},
keywords = {Open-source Software, GitHub Bots, Software Bots},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3395027.3419596,
author = {Tian, Shi and Kashyap, Abhinav Ramesh and Kan, Min-Yen},
title = {ServiceMarq: Extracting Service Contributions from Call for Papers},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419596},
doi = {10.1145/3395027.3419596},
abstract = {In an era, where large numbers of academic research papers are submitted to conferences and journals, the voluntary services of academicians to manage them, is indispensable. The call for contributions of research papers -- through an e-mail or as a webpage, not only solicits research works from scientists, but also lists the names of the researchers and their roles in managing the conference. Tracking such information which showcases the researchers' leadership qualities is becoming increasingly important. Here we present ServiceMarq - a system which proactively tracks service contributions to conferences. It performs focused crawling for website-based call for papers, and integrates archival and natural language processing libraries to achieve both high precision and recall in extracting information. Our results indicate that aggregated service contribution gives an alternative but correlated picture of institutional quality compared against standard bibliometrics. In addition, we have developed a proof of concept website to track service contributions and is available at https://cfp-mining-fe.herokuapp.com and our github repo is available at https://github.com/shitian007/cfp-mining},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {20},
numpages = {4},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/3379597.3387487,
author = {Claes, Ma\"{e}lick and M\"{a}ntyl\"{a}, Mika V.},
title = {20-MAD: 20 Years of Issues and Commits of Mozilla and Apache Development},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387487},
doi = {10.1145/3379597.3387487},
abstract = {Data of long-lived and high profile projects is valuable for research on successful software engineering in the wild. Having a dataset with different linked software repositories of such projects, enables deeper diving investigations. This paper presents 20-MAD, a dataset linking the commit and issue data of Mozilla and Apache projects. It includes over 20 years of information about 765 projects, 3.4M commits, 2.3M issues, and 17.3M issue comments, and its compressed size is over 6 GB. The data contains all the typical information about source code commits (e.g., lines added and removed, message and commit time) and issues (status, severity, votes, and summary). The issue comments have been pre-processed for natural language processing and sentiment analysis. This includes emoticons and valence and arousal scores. Linking code repository and issue tracker information, allows studying individuals in two types of repositories and provide more accurate time zone information for issue trackers as well. To our knowledge, this the largest linked dataset in size and in project lifetime that is not based on GitHub.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {503–507},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3379597.3387442,
author = {Pfeiffer, Rolf-Helge},
title = {What Constitutes Software? An Empirical, Descriptive Study of Artifacts},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387442},
doi = {10.1145/3379597.3387442},
abstract = {The term software is ubiquitous, however, it does not seem as if we as a community have a clear understanding of what software actually is. Imprecise definitions of software do not help other professions, in particular those acquiring and sourcing software from third-parties, when deciding what precisely are potential deliverables. In this paper we investigate which artifacts constitute software by analyzing 23 715 repositories from Github, we categorize the found artifacts into high-level categories, such as, code, data, and documentation (and into 19 more concrete categories) and we can confirm the notion of others that software is more than just source code or programs, for which the term is often used synonymously. With this work we provide an empirical study of more than 13 million artifacts, we provide a taxonomy of artifact categories, and we can conclude that software most often consists of variously distributed amounts of code in different forms, such as source code, binary code, scripts, etc., data, such as configuration files, images, databases, etc., and documentation, such as user documentation, licenses, etc.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {481–491},
numpages = {11},
keywords = {software, artifacts, empirical study},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3349266.3351392,
author = {White, Laurie and Engelke, Charles},
title = {Serverless Distributed Architecture by Incremental Examples},
year = {2019},
isbn = {9781450369213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349266.3351392},
doi = {10.1145/3349266.3351392},
abstract = {Cloud computing has the potential to be a great equalizer, providing powerful and sophisticated computing tools to anyone with an internet connection and even a minimal budget. In fact, educational institutions can often get cloud grants providing access at no cost. Although any traditional computing paradigm can be implemented with cloud computing, there are now new and potentially more useful options available. This workshop is a hands-on exploration of several of those new paradigms in a single gamifiable system. Participants will explore cloud computing through a game-playing case study. Concepts covered include serverless computing, distributed systems, event-driven software, message passing, asynchronous communication, and non-relational databases. Each new idea will be used to advance the example system a step at a time. The workshop will emphasize ways to incorporate the case study in class and possibilities for extending it. All code for the system is available on GitHub for participants to work with to implement their own games or extend the concepts in other ways, such as a programming assignment submission and scoring system.},
booktitle = {Proceedings of the 20th Annual SIG Conference on Information Technology Education},
pages = {138–139},
numpages = {2},
keywords = {serverless computing, functions as a service, event driven, cloud, case study},
location = {Tacoma, WA, USA},
series = {SIGITE '19}
}

@inproceedings{10.1109/ICSE.2019.00078,
author = {Qiu, Huilian Sophie and Nolte, Alexander and Brown, Anita and Serebrenik, Alexander and Vasilescu, Bogdan},
title = {Going Farther Together: The Impact of Social Capital on Sustained Participation in Open Source},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00078},
doi = {10.1109/ICSE.2019.00078},
abstract = {Sustained participation by contributors in open-source software is critical to the survival of open-source projects and can provide career advancement benefits to individual contributors. However, not all contributors reap the benefits of open-source participation fully, with prior work showing that women are particularly underrepresented and at higher risk of disengagement. While many barriers to participation in open-source have been documented in the literature, relatively little is known about how the social networks that open-source contributors form impact their chances of long-term engagement. In this paper we report on a mixed-methods empirical study of the role of social capital (i.e., the resources people can gain from their social connections) for sustained participation by women and men in open-source GitHub projects. After combining survival analysis on a large, longitudinal data set with insights derived from a user survey, we confirm that while social capital is beneficial for prolonged engagement for both genders, women are at disadvantage in teams lacking diversity in expertise.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {688–699},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1109/ICSE-Companion.2019.00036,
author = {Moreno, David and Due\~{n}as, Santiago and Cosentino, Valerio and Fernandez, Miguel Angel and Zerouali, Ahmed and Robles, Gregorio and Gonzalez-Barahona, Jesus M.},
title = {SortingHat: Wizardry on Software Project Members},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00036},
doi = {10.1109/ICSE-Companion.2019.00036},
abstract = {Nowadays, software projects and in particular open source ones heavily rely on a plethora of tools (e.g., Git, GitHub) to support and coordinate development activities. Despite their paramount value, they foster to fragment members' contribution, since members can access them with different identities (e.g., email, username). Thus, researchers and practitioners willing to evaluate individual members contributions are often forced to develop ad-hoc scripts or perform manual work to merge identities. This comes at the risk of obtaining wrong results and hindering replication of their work. In this demo we present SortingHat, which helps to track unique identities of project members and their related information such as gender, country and organization enrollments. It allows to manipulate identities interactively as well as to load bulks of identities via batch files (useful for projects with large communities). SortingHat is a component of GrimoireLab, an industry strong free platform developed by Bitergia, which offers commercial software analytics and is part of the CHAOSS project of the Linux Foundation. A video showing SortingHat is available at https://youtu.be/724I1XcQV6c.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {51–54},
numpages = {4},
keywords = {software development, software mining, open source software, identity merging, empirical software engineering},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3183440.3195071,
author = {Kochhar, Pavneet Singh and Swierc, Stanislaw and Carnahan, Trevor and Sajnani, Hitesh and Nagappan, Meiyappan},
title = {Understanding the Role of Reporting in Work Item Tracking Systems for Software Development: An Industrial Case Study},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195071},
doi = {10.1145/3183440.3195071},
abstract = {Work item tracking systems such as Visual Studio Team Services, JIRA, and GitHub issue tracker are widely used by software engineers. They help in managing different kinds of deliverables (e.g.\^{A}\u{a}features, user stories, bugs), plan sprints, distribute tasks across the team and prioritize the work. While these tools provide reporting capabilities there has been little research into the role these reports play in the overall software development process.In this study, we conduct an empirical investigation on the usage of Analytics Service - a reporting service provided by Visual Studio Team Services (VSTS) to build dashboards and reports out of their work item tracking data. In particular, we want to understand why and how users interact with Analytics Service and what are the outcomes and business decisions taken by stakeholders from reports built using Analytics Service. We perform semi-structured interviews and survey with users of Analytics Service to understand usage and challenges. Our report on qualitative and quantitative analysis can help organizations and engineers building similar tools or services.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {133–134},
numpages = {2},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3180155.3180260,
author = {Zhang, Tianyi and Upadhyaya, Ganesha and Reinhardt, Anastasia and Rajan, Hridesh and Kim, Miryung},
title = {Are Code Examples on an Online Q&amp;A Forum Reliable? A Study of API Misuse on Stack Overflow},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180260},
doi = {10.1145/3180155.3180260},
abstract = {Programmers often consult an online Q&amp;A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons---missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {886–896},
numpages = {11},
keywords = {API usage pattern, online Q&amp;A forum, code example assessment},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3301551.3301579,
author = {Sandanayake, T. C. and Limesha, G. A. I. and Madhumali, T. S. S. and Mihirani, W. P. I. and Peiris, M. S. A.},
title = {Automated CV Analyzing and Ranking Tool to Select Candidates for Job Positions},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301579},
doi = {10.1145/3301551.3301579},
abstract = {Processing of CVs to find a suitable candidate is a challenging task for many organisations. Most of the time identifying potential candidates for a job post is a time consuming and costly task for HR Divisions. Different CV s consists of information in a many formats. This research study extracts the information from the CV and ranks the CVs according to a given criteria to screen them out from the vast number of received applications in an organization. This research solution also recommends the most appropriate job category for an applicant according to his/ her CV information. At the same time the study creates candidate profiles using data from external professional web sources like Stack overflow, GitHub and Blogs. This research has basically designed for the domain of Information Technology related job postings such as Software Engineering, Quality Assurance etc. This system can rank the CVs according to various aspects presented in the CV, thus saving an enormous amount of time and effort that is required for manual scanning by the recruiters.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {13–18},
numpages = {6},
keywords = {Curriculum vitae, machine learning, job recommendation, information extraction, natural language processing},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/3183519.3183540,
author = {Urli, Simon and Yu, Zhongxing and Seinturier, Lionel and Monperrus, Martin},
title = {How to Design a Program Repair Bot? Insights from the Repairnator Project},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183540},
doi = {10.1145/3183519.3183540},
abstract = {Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a "program repair bot" and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 523 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 15 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {95–104},
numpages = {10},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1145/3406865.3418333,
author = {Das, Dipto and Semaan, Bryan},
title = {Quoras: A Python API for Quora Data Collection to Increase Multi-Language Social Science Research},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418333},
doi = {10.1145/3406865.3418333},
abstract = {Quora is a fast growing crowdsourced Q/A site that also creates online social networks and community practices among the users. Operating in several regional languages, it catalyzes more contextual discussions on local incidents and issues. To understand how language-specific social communities conduct Q/A-based discussions on online forums, we need to study Quora platform. As the first step to that, we need a data collection API. We introduce quoras, a Python API for collecting data from Quora. The API relies on Selenium, which is an open-source cross platform web automation framework. The API operates by creating custom HTTPS requests to Quora and parsing responses from it. It has the ability to perform many types of advanced searches that are otherwise only available on the Quora website, and not through any other existing APIs. The quoras API is released under an open-source MIT license and available along with the full API reference on GitHub. The latest stable release is also available on Python Package Index (PyPI).},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {251–256},
numpages = {6},
keywords = {q/a sites, quora, social networking sites, api, data collection},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3387940.3391481,
author = {Brown, Chris and Parnin, Chris},
title = {Comparing Different Developer Behavior Recommendation Styles},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391481},
doi = {10.1145/3387940.3391481},
abstract = {Research shows that one of the most effective ways software engineers discover useful developer behaviors, or tools and practices designed to help developers complete programming tasks, is through human-to-human recommendations from coworkers during work activities. However, due to the increasingly distributed nature of the software industry and development teams, opportunities for these peer interactions are in decline. To overcome the deprecation of peer interactions in software engineering, we explore the impact of several system-to-human recommendation systems, including the recently introduced suggested changes feature on GitHub which allows users to propose code changes to developers on contributions to repositories, to discover their impact on developer recommendations. In this work, we aim to study the effectiveness of suggested changes for recommending developer behaviors by performing a user study with professional software developers to compare static analysis tool recommendations from emails, pull requests, issues, and suggested changes. Our results provide insight into creating systems for recommendations between developers and design implications for improving automated recommendations to software engineers.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {78–85},
numpages = {8},
keywords = {tool adoption, developer behavior, developer recommendations, software engineering},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3395027.3419581,
author = {Cuculovic, Milos and Fondement, Frederic and Devanne, Maxime and Weber, Jonathan and Hassenforder, Michel},
title = {Change Detection on JATS Academic Articles: An XML Diff Comparison Study},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419581},
doi = {10.1145/3395027.3419581},
abstract = {XML is currently a well established and widely used document format. It is used as a core data container in collaborative writing suites and other modern information architectures. The extraction and analysis of differences between two XML document versions is an attractive topic, and has already been tackled by several research groups. The goal of this study is to compare 12 existing state-of-the-art and commercial XML diff algorithms by applying them to JATS documents in order to extract and evaluate changes between two versions of the same academic article. Understanding changes between two article versions is important not only regarding data, but also semantics. Change information consumers in our case are editorial teams, and thus they are more generally interested in change semantics than in the exact data changes. The existing algorithms are evaluated on the following aspects: their edit detection suitability for both text and tree changes, execution speed, memory usage and delta file size. The evaluation process is supported by a Python tool available on Github.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {5},
numpages = {10},
keywords = {JATS, document comparison, change control, XML diff, semantic diff, academic publishing},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/3379597.3387439,
author = {Jia, Ang and Fan, Ming and Xu, Xi and Cui, Di and Wei, Wenying and Yang, Zijiang and Ye, Kai and Liu, Ting},
title = {From Innovations to Prospects: What Is Hidden Behind Cryptocurrencies?},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387439},
doi = {10.1145/3379597.3387439},
abstract = {The great influence of Bitcoin has promoted the rapid development of blockchain-based digital currencies, especially the altcoins, since 2013. However, most altcoins share similar source codes, resulting in concerns about code innovations. In this paper, an empirical study on existing altcoins is carried out to offer a thorough understanding of various aspects associated with altcoin innovations. Firstly, we construct the dataset of altcoins, including source code repository, GitHub fork relation, and market capitalization (cap). Then, we analyze the altcoin innovations from the perspective of source code similarities. The results demonstrate that more than 85% of altcoin repositories present high code similarities. Next, a temporal clustering algorithm is proposed to mine the inheritance relationship among various altcoins. The family pedigrees of altcoin are constructed, in which the altcoin presents similar evolution features as biology, such as power-law in family size, variety in family evolution, etc. Finally, we investigate the correlation between code innovations and market capitalization. Although we fail to predict the price of altcoins based on their code similarities, the results show that altcoins with higher innovations reflect better market prospects.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {288–299},
numpages = {12},
keywords = {Prospects, Innovations, Altcoins, Relation},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3383219.3383255,
author = {Rao, A. Eashaan and Chimalakonda, Sridhar},
title = {An Exploratory Study Towards Understanding Lambda Expressions in Python},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383255},
doi = {10.1145/3383219.3383255},
abstract = {Lambda expressions are anonymous functions in Python. It is one of the alternatives to write a function definition. Syntactically, it is a single expression and defined using the keyword lambda. Lambda expression is a functional programming feature, currently in use, in many mainstream programming languages such as Python, Java8, C++11. There are few studies in C++ and Java to understand the impact of lambda expressions on programmers. These studies are focusing on the developer's adaptability to use a functional style of construct and the benefit they gain from using it. However, we are not aware of any literature on the use of lambda expressions in Python. Thus, there is a need to study lambda expressions in Python projects. In this paper, we examine 15 GitHub repositories out of 760 from our dataset, that are using Python as their primary language. In this study, we are classifying the uses of lambda expressions based on varying scenarios. We identified 13 different usages of lambda expressions from these Python repositories. This catalog is an attempt to support programmers to use lambda expressions more effectively and efficiently.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {318–323},
numpages = {6},
keywords = {Empirical Study, Python, Programming Language Constructs, Lambda Expressions},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1109/ASE.2019.00098,
author = {Tavares, Alberto Trindade and Borba, Paulo and Cavalcanti, Guilherme and Soares, S\'{e}rgio},
title = {Semistructured Merge in JavaScript Systems},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00098},
doi = {10.1109/ASE.2019.00098},
abstract = {Industry widely uses unstructured merge tools that rely on textual analysis to detect and resolve conflicts between code contributions. Semistructured merge tools go further by partially exploring the syntactic structure of code artifacts, and, as a consequence, obtaining significant merge accuracy gains for Java-like languages. To understand whether semistructured merge and the observed gains generalize to other kinds of languages, we implement two semistructured merge tools for JavaScript, and compare them to an unstructured tool. We find that current semistructured merge algorithms and frameworks are not directly applicable for scripting languages like JavaScript. By adapting the algorithms, and studying 10,345 merge scenarios from 50 JavaScript projects on GitHub, we find evidence that our JavaScript tools report fewer spurious conflicts than unstructured merge, without compromising the correctness of the merging process. The gains, however, are much smaller than the ones observed for Java-like languages, suggesting that semistructured merge advantages might be limited for languages that allow both commutative and non-commutative declarations at the same syntactic level.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1014–1025},
numpages = {12},
keywords = {software merging, collaborative development, semistructured merge, JavaScript, version control systems},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3369255.3369300,
author = {Raibulet, Claudia and Fontana, Francesca Arcelli and Pigazzini, Ilaria},
title = {Teaching Software Engineering Tools to Undergraduate Students},
year = {2019},
isbn = {9781450372541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369255.3369300},
doi = {10.1145/3369255.3369300},
abstract = {Today, software development is characterized by keywords such as collaborative, teamwork, distributed, agile, dynamic, qualitative and tool-supported among many others. In this paper, we present our experience in teaching three software development tools often used in industry in a software engineering course for undergraduate students: GitHub, SonarQube, and Microsoft Project. The main reasons behind the use of these tools during the development of a software project were: (1) students become familiar with examples of tools adopted in industry and academia, (2) students are enabled to collaborate in teams for the achievement of a common goal, and (3) students become aware of the management tasks needed by a project developed in teams. We exploited these tools in the software engineering course in the last three academic years. The students feedback on using these tools gathered through a questionnaire was positive. Students were enthusiastic in learning about new tools and their support for software development and management. In this paper we summarize the students feedback during three academic years and the lessons we have learned from their feedback.},
booktitle = {Proceedings of the 2019 11th International Conference on Education Technology and Computers},
pages = {262–267},
numpages = {6},
keywords = {SonarQube, GitHub, Software engineering, Microsoft Project, education, tool},
location = {Amsterdam, Netherlands},
series = {ICETC 2019}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ Unleashed: An Open Implementation of the SZZ Algorithm - Featuring Example Usage in a Study of Just-in-Time Bug Prediction for the Jenkins Project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {mining software repositories, SZZ, issue tracking, defect prediction},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1109/MSR.2019.00056,
author = {Liu, Qin and Liu, Zihe and Zhu, Hongming and Fan, Hongfei and Du, Bowen and Qian, Yu},
title = {Generating Commit Messages from Diffs Using Pointer-Generator Network},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00056},
doi = {10.1109/MSR.2019.00056},
abstract = {The commit messages in source code repositories are valuable but not easy to be generated manually in time for tracking issues, reporting bugs, and understanding codes. Recently published works indicated that the deep neural machine translation approaches have drawn considerable attentions on automatic generation of commit messages. However, they could not deal with out-of-vocabulary (OOV) words, which are essential context-specific identifiers such as class names and method names in code diffs. In this paper, we propose PtrGNCMsg, a novel approach which is based on an improved sequence-to-sequence model with the pointer-generator network to translate code diffs into commit messages. By searching the smallest identifier set with the highest probability, PtrGNCMsg outperforms recent approaches based on neural machine translation, and first enables the prediction of OOV words. The experimental results based on the corpus of diffs and manual commit messages from the top 2,000 Java projects in GitHub show that PtrGNCMsg outperforms the state-of-the-art approach with improved BLEU by 1.02, ROUGE-1 by 4.00 and ROUGE-L by 3.78, respectively.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {299–309},
numpages = {11},
keywords = {sequence-to-sequence model, pointer-generator network, code change pattern recognition, automatic commit message generation},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3239235.3267432,
author = {Nepomuceno, Vilmar and Soares, Sergio},
title = {Maintaining Systematic Literature Reviews: Benefits and Drawbacks},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3267432},
doi = {10.1145/3239235.3267432},
abstract = {Background: Maintenance and traceability (versioning) are constant concerns in Software Engineering (SE), however, few works related to these topics in Systematic Literature Reviews (SLR) were found. Goal: The goal of this research is to elucidate how SLRs can be maintained and what are the benefits and drawbacks in this process. Method: This work presents a survey where experienced researchers that conducted SLRs between 2011 and 2015 answered questions about maintenance and traceability and, using software maintenance concepts, it addresses the SLRs maintenance process. From the 79 e-mails sent we reach 28 answers. Results: 19 of surveyed researchers have shown interest in keeping their SLRs up-to-date, but they have expressed concerns about the effort to be made to accomplish it. It was also observed that 20 participants would be willing to share their SLRs in common repositories, such as GitHub. Conclusions: There is a need to perform maintenance on SLRs. Thus, we are proposing a SLR maintenance process, taking into account some benefits and drawbacks identified during our study and presented through the paper.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {47},
numpages = {4},
keywords = {evidence based software engineering, traceability, maintenance, systematic literature review},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3196398.3196437,
author = {Accioly, Paola and Borba, Paulo and Silva, L\'{e}uson and Cavalcanti, Guilherme},
title = {Analyzing Conflict Predictors in Open-Source Java Projects},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196437},
doi = {10.1145/3196398.3196437},
abstract = {In collaborative development environments integration conflicts occur frequently. To alleviate this problem, different awareness tools have been proposed to alert developers about potential conflicts before they become too complex. However, there is not much empirical evidence supporting the strategies used by these tools. Learning about what types of changes most likely lead to conflicts might help to derive more appropriate requirements for early conflict detection, and suggest improvements to existing conflict detection tools. To bring such evidence, in this paper we analyze the effectiveness of two types of code changes as conflict predictors. Namely, editions to the same method, and editions to directly dependent methods. We conduct an empirical study analyzing part of the development history of 45 Java projects from GitHub and Travis CI, including 5,647 merge scenarios, to compute the precision and recall for the conflict predictors aforementioned. Our results indicate that the predictors combined have a precision of 57.99% and a recall of 82.67%. Moreover, we conduct a manual analysis which provides insights about strategies that could further increase the precision and the recall.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {576–586},
numpages = {11},
keywords = {awareness tools, collaborative development, precision and recall, conflict predictors},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.5555/3155562.3155616,
author = {Chapman, Carl and Wang, Peipei and Stolee, Kathryn T.},
title = {Exploring Regular Expression Comprehension},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 of pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluated the understandability of various regex language features. We further analyzed regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {405–416},
numpages = {12},
keywords = {Regular expression comprehension, equivalence class, regex representations},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3097983.3098101,
author = {Maurus, Samuel and Plant, Claudia},
title = {Let's See Your Digits: Anomalous-State Detection Using Benford's Law},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098101},
doi = {10.1145/3097983.3098101},
abstract = {Benford's Law explains a curious phenomenon in which the leading digits of "naturally-occurring" numerical data are distributed in a precise fashion. In this paper we begin by showing that system metrics generated by many modern information systems like Twitter, Wikipedia, YouTube and GitHub obey this law. We then propose a novel unsupervised approach called BenFound that exploits this property to detect anomalous system events. BenFound tracks the "Benfordness" of key system metrics, like the follower counts of tweeting Twitter users or the change deltas in Wikipedia page edits. It then applies a novel Benford-conformity test in real-time to identify "non-Benford events". We investigate a variety of such events, showing that they correspond to unnatural and often undesirable system interactions like spamming, hashtag-hijacking and denial-of-service attacks. The result is a technically-uncomplicated and effective "red flagging" technique that can be used to complement existing anomaly-detection approaches. Although not without its limitations, it is highly efficient and requires neither obscure parameters, nor text streams, nor natural-language processing.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {977–986},
numpages = {10},
keywords = {anomaly detection, time series data, nonparametric statistical tests, data streams, benford's law},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1007/978-3-319-43659-3_9,
author = {S\^{\i}rbu, Alina and Babaoglu, Ozalp},
title = {Power Consumption Modeling and Prediction in a Hybrid CPU-GPU-MIC Supercomputer},
year = {2016},
isbn = {9783319436586},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-43659-3_9},
doi = {10.1007/978-3-319-43659-3_9},
abstract = {Power consumption is a major obstacle for High Performance Computing HPC systems in their quest towards the holy grail of ExaFLOP performance. Significant advances in power efficiency have to be made before this goal can be attained and accurate modeling is an essential step towards power efficiency by optimizing system operating parameters to match dynamic energy needs. In this paper we present a study of power consumption by jobs in Eurora, a hybrid CPU-GPU-MIC system installed at the largest Italian data center. Using data from a dedicated monitoring framework, we build a data-driven model of power consumption for each user in the system and use it to predict the power requirements of future jobs. We are able to achieve good prediction results for over 80\"{\i} undefined% of the users in the system. For the remaining users, we identify possible reasons why prediction performance is not as good. Possible applications for our predictive modeling results include scheduling optimization, power-aware billing and system-scale power modeling. All the scripts used for the study have been made available on GitHub.},
booktitle = {Proceedings of the 22nd International Conference on Euro-Par 2016: Parallel Processing - Volume 9833},
pages = {117–130},
numpages = {14},
keywords = {Job power prediction, Support vector regression, Hybrid system, Job power modeling, High performance computing}
}

@inproceedings{10.1145/2901739.2903498,
author = {D\'{e}sarmeaux, Casimir and Pecatikov, Andrea and McIntosh, Shane},
title = {The Dispersion of Build Maintenance Activity across Maven Lifecycle Phases},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903498},
doi = {10.1145/2901739.2903498},
abstract = {Build systems describe how source code is translated into deliverables. Developers use build management tools like Maven to specify their build systems. Past work has shown that while Maven provides invaluable features (e.g., incremental building), it introduces an overhead on software development. Indeed, Maven build systems require maintenance. However, Maven build systems follow the build lifecycle, which is comprised of validate, compile, test, packaging, install, and deploy phases. Little is known about how build maintenance activity is dispersed among these lifecycle phases. To bridge this gap, in this paper, we analyze the dispersion of build maintenance activity across build lifecycle phases. Through analysis of 1,181 GitHub repositories that use Maven, we find that: (1) the compile phase accounts for 24% more of the build maintenance activity than the other phases; and (2) while the compile phase generates a consistent amount of maintenance activity over time, the other phases tend to generate peaks and valleys of maintenance activity. Software teams that use Maven should plan for these shifts in the characteristics of build maintenance activity.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {492–495},
numpages = {4},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/2858036.2858479,
author = {Xu, Pingmei and Sugano, Yusuke and Bulling, Andreas},
title = {Spatio-Temporal Modeling and Prediction of Visual Attention in Graphical User Interfaces},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858479},
doi = {10.1145/2858036.2858479},
abstract = {We present a computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces. Like existing models of bottom-up visual attention in computer vision, our model does not require any eye tracking equipment. Instead, it predicts attention solely using information available to the interface, specifically users' mouse and keyboard input as well as the UI components they interact with. To study our model in a principled way, we further introduce a method to synthesize user interface layouts that are functionally equivalent to real-world interfaces, such as from Gmail, Facebook, or GitHub. We first quantitatively analyze attention allocation and its correlation with user input and UI components using ground-truth gaze, mouse, and keyboard data of 18 participants performing a text editing task. We then show that our model predicts attention maps more accurately than state-of-the-art methods. Our results underline the significant potential of spatio-temporal attention modeling for user interface evaluation, optimization, or even simulation.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3299–3310},
numpages = {12},
keywords = {saliency, spatio-temporal modeling, graphical user interfaces, visual attention, interactive environment, physical action},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/2855667.2855678,
author = {Mustyatsa, Vadim},
title = {BDD by Example: Russian Bylina Written in Gherkin Language},
year = {2015},
isbn = {9781450341301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2855667.2855678},
doi = {10.1145/2855667.2855678},
abstract = {In this paper is presented the User Stories technique within the Behaviour-Driven Development process by example of the famous Russian bylina (epic poem) "Three trips of Ilya Muromets" written in Gherkin language. In the first part are given explanations about the choice of the bylina as a good example for presentation of this technique and about the choice of GitHub as a good environment for this presentation. In the second part is placed the text of the presentation divided by the stages of the Story development and fitted with the links to the corresponding commits and file versions in the educational repository. A distinct advantage of this presentation is that it reflects a Story in its evolution, as a process. It reflects a more complex and changeable behaviour than in static trivial examples, which are usually used. Also, there are presented the main features of using the User Stories technique in Russian language, which have not been previously covered nowhere. Besides that, the using of the educational repository can significantly increase the possibilities for further spreading and development of the example.},
booktitle = {Proceedings of the 11th Central &amp; Eastern European Software Engineering Conference in Russia},
articleno = {10},
numpages = {15},
keywords = {byliny, Agile, Scrum, cucumber, JBehave, behaviour-driven development, extreme programming, examples, Gherkin, user stories, GitHub, epic poetry},
location = {Moscow, Russia},
series = {CEE-SECR '15}
}

@inproceedings{10.5555/2820518.2820554,
author = {Burlet, Gregory and Hindle, Abram},
title = {An Empirical Study of End-User Programmers in the Computer Music Community},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and interviews with individuals in this end-user community. Based on this analysis and feedback from computer musicians we find that there are many avenues where software engineering can be applied to help aid this community of end-user programmers.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {292–302},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3406865.3418368,
author = {Wessel, Mairieli},
title = {Leveraging Software Bots to Enhance Developers' Collaboration in Online Programming Communities},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418368},
doi = {10.1145/3406865.3418368},
abstract = {Software bots are applications that are integrated into human communication channels, serving as an interface between users and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save developers' costs, time, and effort, the interaction of these bots can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers to enhance existing bots, thereby improving the partnership with contributors and maintainers. Toward this end, we are interviewing developers to understand what are the problems on the human-bot interaction and how they affect human collaboration. Afterwards, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {183–188},
numpages = {6},
keywords = {github bots, software bots, open source software, software engineering},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3382494.3422166,
author = {Hazhirpasand, Mohammadreza and Ghafari, Mohammad and Nierstrasz, Oscar},
title = {Java Cryptography Uses in the Wild},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422166},
doi = {10.1145/3382494.3422166},
abstract = {[Background] Previous research has shown that developers commonly misuse cryptography APIs. [Aim] We have conducted an exploratory study to find out how crypto APIs are used in open-source Java projects, what types of misuses exist, and why developers make such mistakes. [Method] We used a static analysis tool to analyze hundreds of open-source Java projects that rely on Java Cryptography Architecture, and manually inspected half of the analysis results to assess the tool results. We also contacted the maintainers of these projects by creating an issue on the GitHub repository of each project, and discussed the misuses with developers. [Results] We learned that 85% of Cryptography APIs are misused, however, not every misuse has severe consequences. Developer feedback showed that security caveats in the documentation of crypto APIs are rare, developers may overlook misuses that originate in third-party code, and the context where a Crypto API is used should be taken into account. [Conclusion] We conclude that using Crypto APIs is still problematic for developers but blindly blaming them for such misuses may lead to erroneous conclusions.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {40},
numpages = {6},
keywords = {Java cryptography, security, empirical study},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3387940.3391489,
author = {Schumacher, Max Eric Henry and Le, Kim Tuyen and Andrzejak, Artur},
title = {Improving Code Recommendations by Combining Neural and Classical Machine Learning Approaches},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391489},
doi = {10.1145/3387940.3391489},
abstract = {Code recommendation systems for software engineering are designed to accelerate the development of large software projects. A classical example is code completion or next token prediction offered by modern integrated development environments. A particular challenging case for such systems are dynamic languages like Python due to limited type information at editing time. Recently, researchers proposed machine learning approaches to address this challenge. In particular, the Probabilistic Higher Order Grammar technique (Bielik et al., ICML 2016) uses a grammar-based approach with a classical machine learning schema to exploit local context. A method by Li et al., (IJCAI 2018) uses deep learning methods, in detail a Recurrent Neural Network coupled with a Pointer Network. We compare these two approaches quantitatively on a large corpus of Python files from GitHub. We also propose a combination of both approaches, where a neural network decides which schema to use for each prediction. The proposed method achieves a slightly better accuracy than either of the systems alone. This demonstrates the potential of ensemble-like methods for code completion and recommendation tasks in dynamically typed languages.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {476–482},
numpages = {7},
keywords = {machine learning, neural networks, code recommendations},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/3167132.3167314,
author = {Benni, Benjamin and Mosser, S\'{e}bastien and Collet, Philippe and Riveill, Michel},
title = {Supporting Micro-Services Deployment in a Safer Way: A Static Analysis and Automated Rewriting Approach},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167314},
doi = {10.1145/3167132.3167314},
abstract = {The SOA ecosystem has drastically evolved since its childhood in the early 2000s. From monolithic services, micro-services now cooperate together in ultra-large scale systems. In this context, there is a tremendous need to deploy frequently new services, or new version of existing services. Container-based technologies (e.g., Docker) emerged recently to tool such deployments, promoting a black-box reuse mechanism to support off-the-shelf deployments. Unfortunately, from the service deployment point of view, such form of black-box reuse prevent to ensure what is really shipped inside the container with the service to deploy. In this paper, we propose a formalism to model and statically analyze service deployment artifacts based on state of the art deployment platforms. The static analysis mechanism leverages the hierarchy of deployment descriptors to verify a given deployment, as well as rewrite it to automatically fix common errors. The approach is validated through the automation of the guidelines provided by the user community associated to the reference Docker engine, and the analysis of 20,000 real deployment descriptors (hosted on GitHub).},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1706–1715},
numpages = {10},
keywords = {container, docker, microservice, static analysis},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/3017680.3017822,
author = {Brown, Neil C.C. and Altadmri, Amjad},
title = {What's New in BlueJ 4: Git, Stride and More (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3017822},
doi = {10.1145/3017680.3017822},
abstract = {BlueJ is a beginner's IDE for Java which has been in popular use for over ten years. But it continues to improve and evolve: BlueJ 4.0.0 was recently released with several new features. Git support has been added in a user-friendly way, and the support for writing JavaFX GUI applications has been improved. BlueJ 4 also includes the frame-based Stride editor (previously seen in Greenfoot), which allows for block-like programming. BlueJ 4 also retains all its existing functionality such as interactive object creation and method invocation, a "REPL"-like code pad, a debugger and testing support. This workshop, run by the developers of BlueJ, will take the participants, whether new to BlueJ and Java or long-time users, through the new features while also providing an introduction/refresher on the existing capabilities of the software. Participants will learn how to share BlueJ projects via Github, create a new JavaFX application, dabble with Stride and get a tour of the existing BlueJ functionality. A laptop with BlueJ 4.0 installed is required.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {734},
numpages = {1},
keywords = {stride, java, Git, javafx, BlueJ},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/2901739.2901743,
author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
title = {A Look at the Dynamics of the JavaScript Package Ecosystem},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901743},
doi = {10.1145/2901739.2901743},
abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node. js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {351–361},
numpages = {11},
keywords = {JavaScript, software ecosystem analysis, Node.js},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1109/FRUCT-ISPIT.2016.7561528,
author = {Masek, Pavel and Fujdiak, Radek and Zeman, Krystof and Hosek, Jiri and Muthanna, Ammar},
title = {Remote Networking Technology for IoT: Cloud-Based Access for AllJoyn-Enabled Devices},
year = {2016},
publisher = {FRUCT Oy},
address = {Helsinki, Uusimaa, FIN},
url = {https://doi.org/10.1109/FRUCT-ISPIT.2016.7561528},
doi = {10.1109/FRUCT-ISPIT.2016.7561528},
abstract = {The Internet of Things (IoT) represents a vision of a future communication between users, systems, and daily objects performing sensing and actuating capabilities with the goal to bring unprecedented convenience and economical benefits. Today, a wide variety of developed solutions for IoT can be seen through the all industry fields. Each of the developed systems is based on the proprietary SW implementation unable (in most cases) to share collected data with others. Trying to offer common communication platform for IoT, AllSeen Alliance introduced Alljoyn framework - interoperable platform for devices (sensors, actuators, etc.) and applications to communicate among themselves regardless of brands, transport technologies, and operating systems. In this paper, we discuss an application for remote management of light systems built as an extension of Alljoyn Framework - developed application is independent on communication technologies (e.g., ZigBee or WiFi). Besides provided communication independence, the presented framework can run on both major SoC architectures ARM and MIPS. To this end, we believe that our application (available as open source on GitHub) can serve as building block in future IoT / Smart home implementations.},
booktitle = {Proceedings of the 18th Conference of Open Innovations Association FRUCT},
pages = {200–205},
numpages = {6},
keywords = {IoT, Remote Control, M2M Communication, OSGi Framework, AllJoyn Framework},
location = {Saint-Petersburg, Russia},
series = {FRUCT '18}
}

@inproceedings{10.1109/ICSM.2015.7332461,
author = {Stanciulescu, Stefan and Schulze, Sandro and Wasowski, Andrzej},
title = {Forked and Integrated Variants in an Open-Source Firmware Project},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332461},
doi = {10.1109/ICSM.2015.7332461},
abstract = {Code cloning has been reported both on small (code fragments) and large (entire projects) scale. Cloning-in-the-large, or forking, is gaining ground as a reuse mechanism thanks to availability of better tools for maintaining forked project variants, hereunder distributed version control systems and interactive source management platforms such as Github. We study advantages and disadvantages of forking using the case of Marlin, an open source firmware for 3D printers. We find that many problems and advantages of cloning do translate to forking. Interestingly, the Marlin community uses both forking and integrated variability management (conditional compilation) to create variants and features. Thus, studying it increases our understanding of the choice between integrated and clone-based variant management. It also allows us to observe mechanisms governing source code maturation, in particular when, why and how feature implementations are migrated from forks to the main integrated platform. We believe that this understanding will ultimately help development of tools mixing clone-based and integrated variant management, combining the advantages of both.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {151–160},
numpages = {10},
series = {ICSME '15}
}

@inproceedings{10.5555/2486788.2486869,
author = {Hosek, Petr and Cadar, Cristian},
title = {Safe Software Updates via Multi-Version Execution},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Software systems are constantly evolving, with new versions and patches being released on a continuous basis. Unfortunately, software updates present a high risk, with many releases introducing new bugs and security vulnerabilities.  We tackle this problem using a simple but effective multi-version based approach. Whenever a new update becomes available, instead of upgrading the software to the new version, we run the new version in parallel with the old one; by carefully coordinating their executions and selecting the behaviour of the more reliable version when they diverge, we create a more secure and dependable multi-version application.  We implemented this technique in Mx, a system targeting Linux applications running on multi-core processors, and show that it can be applied successfully to several real applications such as Coreutils, a set of user-level UNIX applications; Lighttpd, a popular web server used by several high-traffic websites such as Wikipedia and YouTube; and Redis, an advanced key-value data structure server used by many well-known services such as GitHub and Flickr. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {612–621},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/2890046.2890056,
author = {Vassiliadis, Vangelis and Wielemaker, Jan and Mungall, Chris},
title = {Processing OWL2 Ontologies Using Thea: An Application of Logic Programming},
year = {2009},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {Traditional object-oriented programming languages can be difficult to use when working with ontologies, leading to the creation of domain-specific languages designed specifically for ontology processing. Prolog, with its logic-based, declarative semantics offers many advantages as a host programming language for querying and processing OWL2 ontologies. The SWI-Prolog semweb library provides some support for OWL but until now there has been a lack of any library providing direct and comprehensive support for OWL2.We have developed Thea, a library based directly on the OWL2 functional-style syntax, allowing storage and manipulation of axioms as a Prolog database. Thea can translate ontologies to Description Logic programs but the emphasis is on using Prolog as an application programming and processing language rather than a reasoning engine. Thea offers the ability to seamless connect to the java OWL API and OWLLink servers. Thea also includes support for SWRL.In this paper we provide examples of using Thea for processing ontologies, and compare the results to alternative methods. Thea is available from GitHub: http://github.com/vangelisv/thea.},
booktitle = {Proceedings of the 6th International Conference on OWL: Experiences and Directions - Volume 529},
pages = {89–98},
numpages = {10},
location = {Chantilly, VA},
series = {OWLED'09}
}

@inproceedings{10.1145/3426422.3426981,
author = {Rak-amnouykit, Ingkarat and McCrevan, Daniel and Milanova, Ana and Hirzel, Martin and Dolby, Julian},
title = {Python 3 Types in the Wild: A Tale of Two Type Systems},
year = {2020},
isbn = {9781450381758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426422.3426981},
doi = {10.1145/3426422.3426981},
abstract = {Python 3 is a highly dynamic language, but it has introduced a syntax for expressing types with PEP484. This paper explores how developers use these type annotations, the type system semantics provided by type checking and inference tools, and the performance of these tools. We evaluate the types and tools on a corpus of public GitHub repositories. We review MyPy and PyType, two canonical static type checking and inference tools, and their distinct approaches to type analysis. We then address three research questions: (i) How often and in what ways do developers use Python 3 types? (ii) Which type errors do developers make? (iii) How do type errors from different tools compare?  Surprisingly, when developers use static types, the code rarely type-checks with either of the tools. MyPy and PyType exhibit false positives, due to their static nature, but also flag many useful errors in our corpus. Lastly, MyPy and PyType embody two distinct type systems, flagging different errors in many cases. Understanding the usage of Python types can help guide tool-builders and researchers. Understanding the performance of popular tools can help increase the adoption of static types and tools by practitioners, ultimately leading to more correct and more robust Python code.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Symposium on Dynamic Languages},
pages = {57–70},
numpages = {14},
keywords = {Python, type checking, type inference},
location = {Virtual, USA},
series = {DLS 2020}
}

@inproceedings{10.1145/3412569.3412575,
author = {Verma, Amit Arjun and Iyengar, S. R.S. and Setia, Simran and Dubey, Neeru},
title = {KDAP: An Open Source Toolkit to Accelerate Knowledge Building Research},
year = {2020},
isbn = {9781450387798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412569.3412575},
doi = {10.1145/3412569.3412575},
abstract = {With the success of crowdsourced portals, such as Wikipedia, Stack Overflow, Quora, and GitHub, a class of researchers is driven towards understanding the dynamics of knowledge building on these portals. Even though collaborative knowledge building portals are known to be better than expert-driven knowledge repositories, limited research has been performed to understand the knowledge building dynamics in the former. This is mainly due to two reasons; first, unavailability of the standard data representation format, second, lack of proper tools and libraries to analyze the knowledge building dynamics.We describe Knowledge Data Analysis and Processing Platform (KDAP), a programming toolkit that is easy to use and provides high-level operations for analysis of knowledge data. We propose Knowledge Markup Language (Knol-ML), a standard representation format for the data of collaborative knowledge building portals. KDAP can process the massive data of crowdsourced portals like Wikipedia and Stack Overflow efficiently. As a part of this toolkit, a data-dump of various collaborative knowledge building portals is published in Knol-ML format. The combination of Knol-ML and the proposed open-source library will help the knowledge building community to perform benchmark analysis.URL:https://github.com/descentis/kdapSupplementary Material: https://bit.ly/2Z3tZK5},
booktitle = {Proceedings of the 16th International Symposium on Open Collaboration},
articleno = {1},
numpages = {11},
keywords = {open-source library, datasets, Q&amp;A, Wikipedia, Knowledge Building},
location = {Virtual conference, Spain},
series = {OpenSym 2020}
}

@inproceedings{10.1109/MSR.2019.00030,
author = {Pietri, Antoine and Spinellis, Diomidis and Zacchiroli, Stefano},
title = {The Software Heritage Graph Dataset: Public Software Development under One Roof},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00030},
doi = {10.1109/MSR.2019.00030},
abstract = {Software Heritage is the largest existing public archive of software source code and accompanying development history: it currently spans more than five billion unique source code files and one billion unique commits, coming from more than 80 million software projects.This paper introduces the Software Heritage graph dataset: a fully-deduplicated Merkle DAG representation of the Software Heritage archive. The dataset links together file content identifiers, source code directories, Version Control System (VCS) commits tracking evolution over time, up to the full states of VCS repositories as observed by Software Heritage during periodic crawls. The dataset's contents come from major development forges (including GitHub and GitLab), FOSS distributions (e.g., Debian), and language-specific package managers (e.g., PyPI). Crawling information is also included, providing timestamps about when and where all archived source code artifacts have been observed in the wild.The Software Heritage graph dataset is available in multiple formats, including downloadable CSV dumps and Apache Parquet files for local use, as well as a public instance on Amazon Athena interactive query service for ready-to-use powerful analytical processing.Source code file contents are cross-referenced at the graph leaves, and can be retrieved through individual requests using the Software Heritage archive API.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {138–142},
numpages = {5},
keywords = {open source software, development history graph, dataset, source code, mining software repositories, free software, digital preservation},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3236024.3236072,
author = {Wang, Peipei and Stolee, Kathryn T.},
title = {How Well Are Regular Expressions Tested in the Wild?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236072},
doi = {10.1145/3236024.3236072},
abstract = {Developers report testing their regular expressions less than the rest of their code. In this work, we explore how thoroughly tested regular expressions are by examining open source projects.  Using standard metrics of coverage, such as line and branch coverage, gives an incomplete picture of the test coverage of regular expressions. We adopt graph-based coverage metrics for the DFA representation of regular expressions, providing fine-grained test coverage metrics. Using over 15,000 tested regular expressions in 1,225 Java projects on GitHub, we measure node, edge, and edge-pair coverage. Our results show that only 17% of the regular expressions in the repositories are tested at all. For those that are tested, the median number of test inputs is two. For nearly 42% of the tested regular expressions, only one test input is used. Average node and edge coverage levels on the DFAs for tested regular expressions are 59% and 29%, respectively. Due to the lack of testing of regular expressions, we explore whether a string generation tool for regular expressions, Rex, achieves high coverage levels. With some exceptions, we found that tools such as Rex can be used to write test inputs with similar coverage to the developer tests.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {668–678},
numpages = {11},
keywords = {Deterministic Finite Automaton, Regular expressions, Test coverage metrics},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ICCPS.2018.00021,
author = {Schmittle, Matt and Lukina, Anna and Vacek, Lukas and Das, Jnaneshwar and Buskirk, Christopher P. and Rees, Stephen and Sztipanovits, Janos and Grosu, Radu and Kumar, Vijay},
title = {OpenUAV: A UAV Testbed for the CPS and Robotics Community},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCPS.2018.00021},
doi = {10.1109/ICCPS.2018.00021},
abstract = {Multirotor Unmanned Aerial Vehicles (UAV) have grown in popularity for research and education, overcoming challenges associated with fixed wing and ground robots. Unfortunately, extensive physical testing can be expensive and time consuming because of short flight times due to battery constraints and safety precautions. Simulation tools offer a low barrier to entry and enable testing and validation before field trials. However, most of the well-known simulators today have a high barrier to entry due to the need for powerful computers and the time required for initial set up. In this paper, we present OpenUAV, an open source test bed for UAV education and research that overcomes these barriers. We leverage the Containers as a Service (CaaS) technology to enable students and researchers carry out simulations on the cloud. We have based our framework on open-source tools including ROS, Gazebo, Docker, PX4, and Ansible, we designed the simulation framework so that it has no special hardware requirements. Two use-cases are presented. First, we show how a UAV can navigate around obstacles, and second, we test a multi-UAV swarm formation algorithm. To our knowledge, this is the first open-source, cloud-enabled testbed for UAVs. The code is available on GitHub: https://github.com/Open-UAV.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {130–139},
numpages = {10},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@inproceedings{10.1145/2642937.2643002,
author = {Campos, Jos\'{e} and Arcuri, Andrea and Fraser, Gordon and Abreu, Rui},
title = {Continuous Test Generation: Enhancing Continuous Integration with Automated Test Generation},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643002},
doi = {10.1145/2642937.2643002},
abstract = {In object oriented software development, automated unit test generation tools typically target one class at a time. A class, however, is usually part of a software project consisting of more than one class, and these are subject to changes over time. This context of a class offers significant potential to improve test generation for individual classes. In this paper, we introduce Continuous Test Generation (CTG), which includes automated unit test generation during continuous integration (i.e., infrastructure that regularly builds and tests software projects). CTG offers several benefits: First, it answers the question of how much time to spend on each class in a project. Second, it helps to decide in which order to test them. Finally, it answers the question of which classes should be subjected to test generation in the first place. We have implemented CTG using the EvoSuite unit test generation tool, and performed experiments using eight of the most popular open source projects available on GitHub, ten randomly selected projects from the SF100 corpus, and five industrial projects. Our experiments demonstrate improvements of up to +58% for branch coverage and up to +69% for thrown undeclared exceptions, while reducing the time spent on test generation by up to +83%.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {55–66},
numpages = {12},
keywords = {unit testing, automated test generation, continuous integration, continuous testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3394171.3414535,
author = {Zhang, Huaizheng and Li, Yuanming and Huang, Yizheng and Wen, Yonggang and Yin, Jianxiong and Guan, Kyle},
title = {MLModelCI: An Automatic Cloud Platform for Efficient MLaaS},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3414535},
doi = {10.1145/3394171.3414535},
abstract = {MLModelCI provides multimedia researchers and developers with a one-stop platform for efficient machine learning (ML) services. The system leverages DevOps techniques to optimize, test, and manage models. It also containerizes and deploys these optimized and validated models as cloud services (MLaaS). In its essence, MLModelCI serves as a housekeeper to help users publish models. The models are first automatically converted to optimized formats for production purpose and then profiled under different settings (e.g., batch size and hardware). The profiling information can be used as guidelines for balancing the trade-off between performance and cost of MLaaS. Finally, the system dockerizes the models for ease of deployment to cloud environments. A key feature of MLModelCI is the implementation of a controller, which allows elastic evaluation which only utilizes idle workers while maintaining online service quality. Our system bridges the gap between current ML training and serving systems and thus free developers from manual and tedious work often associated with service deployment. We release the platform as an open-source project on GitHub under Apache 2.0 license, with the aim that it will facilitate and streamline more large-scale ML applications and research projects.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4453–4456},
numpages = {4},
keywords = {inference serving, conversion, model deployment, profiling},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3379597.3387463,
author = {Meinicke, Jens and Hoyos, Juan and Vasilescu, Bogdan and K\"{a}stner, Christian},
title = {Capture the Feature Flag: Detecting Feature Flags in Open-Source},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387463},
doi = {10.1145/3379597.3387463},
abstract = {Feature flags (a.k.a feature toggles) are a mechanism to keep new features hidden behind a boolean option during development. Flags are used for many purposes, such as A/B testing and turning off a feature more easily in case of failures. While software engineering research on feature flags is burgeoning, examples of software projects using flags rarely come from outside commercial and private projects, stifling academic progress. To address this gap, in this paper we present a novel semi-automated mining software repositories approach to detect feature flags in open-source projects, based on analyzing the projects' commit messages and other project characteristics. With our approach, we search over all open-source GitHub projects, finding multiple thousand plausible and active candidate feature flagging projects. We manually validate projects and assemble a dataset of 100 confirmed feature flagging projects. To demonstrate the benefits of our detection technique, we report on an initial analysis of feature flags in the validated sample of 100 projects, investigating practices that correlate with shorter flag lifespans (typically desirable to reduce technical debt), such as using the issue tracker and having a flag owner.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {169–173},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3378936.3378951,
author = {Arora, Ritu and Wani, Anand and Vineet, Ankur and Dhandhalya, Bhavik and Sharma, Yashvardhan and Goel, Sanjay},
title = {Continuous Conflict Prediction during Collaborative Software Development: A Step-before Continuous Integration},
year = {2020},
isbn = {9781450376907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3378936.3378951},
doi = {10.1145/3378936.3378951},
abstract = {Concurrent activities of collaborative developers over shared project repositories might lead to direct and indirect conflicts. Software Configuration Management systems are designed to capture direct or merge conflicts which arise due to concurrent editing of same shared artifact. However, inconsistencies caused owing to indirect conflicts which arise because of concurrent editing of related artifacts might enter the codebase, since SCM systems have limited capabilities to capture these. Although, Continuous Integration process which is deployed to build the entire codebase with every commit, is quite effective in capturing several type of inconsistencies. However, still few categories of behavioral semantic inconsistencies might evade the build process and penetrate into codebase. In this paper, we propose the Continuous Conflict Prediction Framework which describes a cyclic, real-time, continuous process for conflict prediction which is executed during the process of code creation by collaborative developers. This framework entails a critical conflict-prediction and awareness-generation process which helps in capturing conflicts during development process itself and hence minimizes the number of conflicts entering the project codebase. The proposed framework is realized through implementation of the tool named Collaboration Over GitHub.},
booktitle = {Proceedings of the 3rd International Conference on Software Engineering and Information Management},
pages = {105–109},
numpages = {5},
keywords = {Continuous Integration, Direct Conflicts, Eclipse Plugin Development, Indirect Conflicts, Collaborative Software Development, Continuous Conflict Prediction},
location = {Sydney, NSW, Australia},
series = {ICSIM '20}
}

@inproceedings{10.5555/3304889.3304970,
author = {He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
title = {Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {This paper proposed a Soft Filter Pruning (SFP) method to accelerate the inference procedure of deep Convolutional Neural Networks (CNNs). Specifically, the proposed SFP enables the pruned filters to be updated when training the model after pruning. SFP has two advantages over previous works: (1) Larger model capacity. Updating previously pruned filters provides our approach with larger optimization space than fixing the filters to zero. Therefore, the network trained by our method has a larger model capacity to learn from the training data. (2) Less dependence on the pretrained model. Large capacity enables SFP to train from scratch and prune the model simultaneously. In contrast, previous filter pruning methods should be conducted on the basis of the pre-trained model to guarantee their performance. Empirically, SFP from scratch outperforms the previous filter pruning methods. Moreover, our approach has been demonstrated effective for many advanced CNN architectures. Notably, on ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/soft-filter-pruning},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {2234–2240},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{10.1145/3107411.3107455,
author = {Trieu, Tuan and Cheng, Jianlin},
title = {3D Genome Structure Modeling by Lorentzian Objective Function},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107455},
doi = {10.1145/3107411.3107455},
abstract = {Reconstructing 3D structure of a genome from chromosomal conformation capturing data such as Hi-C data has emerged as an important problem in bioinformatics and computational biology in the recent years. In this talk, I will present our latest method that uses Lorentzian function to describe distance restraints between chromosomal regions, which will be used to guide the reconstruction of 3D structures of individual chromosomes and an entire genome. The method is more robust against noisy distance restraints derived from Hi-C data than traditional objective functions such as squared error function and Gaussian probabilistic function. The method can handle both intra- and inter-chromosomal contacts effectively to build 3D structures of a big genome such as the human genome consisting of a number of chromosomes, which are not possible with most existing methods. We have released the Java source code that implements the method (called LorDG) at GitHub (https://github.com/BDM-Lab/LorDG), which is being used by the community to model 3D genome structures. We are currently further improving the method to build very high-resolution (e.g. 1KB base pair) 3D genome and chromosome models.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {641},
numpages = {1},
keywords = {lorentzian function, optimization, chromosomal conformation capturing, hi-c, 3d genome, modeling},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.5555/2819009.2819091,
author = {Lyons, Kelly and Oh, Christie},
title = {SOA4DM: Applying an SOA Paradigm to Coordination in Humanitarian Disaster Response},
year = {2015},
publisher = {IEEE Press},
abstract = {Despite efforts to achieve a sustainable state of control over the management of global crises, disasters are occurring with greater frequency, intensity, and affecting many more people than ever before while the resources to deal with them do not grow apace. As we enter 2015, with continued concerns that mega-crises may become the new normal, we need to develop novel methods to improve the efficiency and effectiveness of our management of disasters. Software engineering as a discipline has long had an impact on society beyond its role in the development of software systems. In fact, software engineers have been described as the developers of prototypes for future knowledge workers; tools such as Github and Stack Overflow have demonstrated applications beyond the domain of software engineering. In this paper, we take the potential influence of software engineering one-step further and propose using the software service engineering paradigm as a new approach to managing disasters. Specifically, we show how the underlying principles of service-oriented architectures (SOA) can be applied to the coordination of disaster response operations. We describe key challenges in coordinating disaster response and discuss how an SOA approach can address those challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {519–522},
numpages = {4},
keywords = {SOA, disaster response},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1109/HICSS.2014.405,
author = {Squire, Megan},
title = {Forge++: The Changing Landscape of FLOSS Development},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.405},
doi = {10.1109/HICSS.2014.405},
abstract = {Software forges are centralized online systems that provide useful tools to help distributed development teams work together, especially in free, libre, and open source software (FLOSS). Forge-provided tools may include web space, version control systems, mailing lists and communication forums, bug tracking systems, file downloads, wikis, and the like. Empirical software engineering researchers can mine the artifacts from these tools to better understand how FLOSS is made. As the landscape of distributed software development has grown and changed, the tools needed to make FLOSS have changed as well. There are three newer tools at the center of FLOSS development today: distributed version control based forges (like Github), programmer question-and-answer communities (like Stack Overflow), and paste bin tools (like Gist or Pastebin.com). These tools are extending and changing the toolset used for FLOSS development, and redefining what a software forge looks like. The main contributions of this paper are to describe each of these tools, to identify the data and artifacts available for mining from these tools, and to outline some of the ways researchers can use these artifacts to continue to understand how FLOSS is made.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {3266–3275},
numpages = {10},
keywords = {stack overflow, repositories, pastebin, github, open source software, software development, forges},
series = {HICSS '14}
}

@inproceedings{10.1145/3416506.3423576,
author = {Phan, Hung and Jannesari, Ali},
title = {Statistical Machine Translation Outperforms Neural Machine Translation in Software Engineering: Why and How},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423576},
doi = {10.1145/3416506.3423576},
abstract = {Neural Machine Translation (NMT) is the current trend approach in Natural Language Processing (NLP) to solve the problem of auto- matically inferring the content of target language given the source language. The ability of NMT is to learn deep knowledge inside lan- guages by deep learning approaches. However, prior works show that NMT has its own drawbacks in NLP and in some research problems of Software Engineering (SE). In this work, we provide a hypothesis that SE corpus has inherent characteristics that NMT will confront challenges compared to the state-of-the-art translation engine based on Statistical Machine Translation. We introduce a problem which is significant in SE and has characteristics that challenges the abil- ity of NMT to learn correct sequences, called Prefix Mapping. We implement and optimize the original SMT and NMT to mitigate those challenges. By the evaluation, we show that SMT outperforms NMT for this research problem, which provides potential directions to optimize the current NMT engines for specific classes of parallel corpus. By achieving the accuracy from 65% to 90% for code tokens generation of 1000 Github code corpus, we show the potential of using MT for code completion at token level.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {3–12},
numpages = {10},
keywords = {Neural Machine Translation, Statistical Machine Translation},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.1145/3402942.3409789,
author = {Stephens, Conor and Exton, Dr. Chris},
title = {Assessing Multiplayer Level Design Using Deep Learning Techniques},
year = {2020},
isbn = {9781450388078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402942.3409789},
doi = {10.1145/3402942.3409789},
abstract = { This paper proposes a new framework to measure the fairness of asymmetric level-design in multiplayer games. This work achieves real time prediction of the degree to which asymmetric levels are balanced using deep learning. The proposed framework provides both cost and time savings, by removing the requirement of numerous designed levels and the need to gather player data samples. This advancement with the field is possible through the combination of deep reinforcement learning (made accessible to developers with Unity’s ML-Agents framework), and Procedural Content Generation (PCG). The result of this merger is the acquisition of accelerated training data, which is established using parallel simulations. This paper showcases the proposed approach on a simple two player top-down -shooter game implemented using MoreMountains: Top Down Engine an extension to Unity 3D a popular game engine. Levels are generated using the same PCG approaches found in ’Nuclear Throne’ a popular cross platform Roguelike published by Vlambeer. This approach is accessible and easy to implement allowing games developers to test human-designed content in real time using the predictions. This research is open source and available on Github: https://github.com/Taikatou/top-down-shooter.},
booktitle = {International Conference on the Foundations of Digital Games},
articleno = {16},
numpages = {3},
keywords = {Procedural Content Generation, Level Design, Reinforcement Learning, Game Balance},
location = {Bugibba, Malta},
series = {FDG '20}
}

@inproceedings{10.1145/3359061.3361082,
author = {Estep, Samuel},
title = {Gradual Program Analysis},
year = {2019},
isbn = {9781450369923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359061.3361082},
doi = {10.1145/3359061.3361082},
abstract = {The designers of static analyses for null safety often try to reduce the number of false positives reported by the analysis through increased engineering effort, user-provided annotations, and/or weaker soundness guarantees. To produce a null-pointer analysis with little engineering effort, reduced false positives, and strong soundness guarantees in a principled way, we adapt the “Abstracting Gradual Typing” framework to the abstract-interpretation based program analysis setting. In particular, a simple static dataflow analysis that relies on user-provided annotations and has nullability lattice N ⊑ ⊤ (where N means “definitely not null” and ⊤ means “possibly null”) is gradualized producing a new lattice N ⊑ ? ⊑ ⊤. Question mark explicitly represents “optimistic uncertainty” in the analysis itself, supporting a formal soundness property and the “gradual guarantees” laid out in the gradual typing literature. We then implement a prototype of our gradual null-pointer analysis as a Facebook Infer checker, and compare it to existing null-pointer analyses via a suite of GitHub repositories used originally by Uber to evaluate their NullAway tool. Our prototype has architecture and output very similar to these existing tools, suggesting the value of applying our approach to more sophisticated program analyses in the future.},
booktitle = {Proceedings Companion of the 2019 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {52–53},
numpages = {2},
keywords = {null safety, gradual typing, abstract interpretation, program analysis},
location = {Athens, Greece},
series = {SPLASH Companion 2019}
}

@inproceedings{10.1145/2950290.2983932,
author = {Gyori, Alex and Lambeth, Ben and Shi, August and Legunsen, Owolabi and Marinov, Darko},
title = {NonDex: A Tool for Detecting and Debugging Wrong Assumptions on Java API Specifications},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983932},
doi = {10.1145/2950290.2983932},
abstract = { We present NonDex, a tool for detecting and debugging wrong assumptions on Java APIs. Some APIs have underdetermined specifications to allow implementations to achieve different goals, e.g., to optimize performance. When clients of such APIs assume stronger-than-specified guarantees, the resulting client code can fail. For example, HashSet’s iteration order is underdetermined, and code assuming some implementation-specific iteration order can fail. NonDex helps to proactively detect and debug such wrong assumptions. NonDex performs detection by randomly exploring different behaviors of underdetermined APIs during test execution. When a test fails during exploration, NonDex searches for the invocation instance of the API that caused the failure. NonDex is open source, well-integrated with Maven, and also runs from the command line. During our experiments with the NonDex Maven plugin, we detected 21 new bugs in eight Java projects from GitHub, and, using the debugging feature of NonDex, we identified the underlying wrong assumptions for these 21 new bugs and 54 previously detected bugs. We opened 13 pull requests; developers already accepted 12, and one project changed the continuous-integration configuration to run NonDex on every push. The demo video is at: https://youtu.be/h3a9ONkC59c },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {993–997},
numpages = {5},
keywords = {underdetermined API, flaky tests, NonDex},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/2635868.2635901,
author = {Allamanis, Miltiadis and Sutton, Charles},
title = {Mining Idioms from Source Code},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635901},
doi = {10.1145/2635868.2635901},
abstract = { We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q&amp;A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {472–483},
numpages = {12},
keywords = {code idioms, naturalness of source code, syntactic code patterns},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1109/IPDPSW.2013.161,
author = {Zhu, Zhaomeng and Zhang, Gongxuan and Zhang, Yongping and Guo, Jian and Xiong, Naixue},
title = {Briareus: Accelerating Python Applications with Cloud},
year = {2013},
isbn = {9780769549798},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPSW.2013.161},
doi = {10.1109/IPDPSW.2013.161},
abstract = {Briareus provides convenient tools to make use of computing resources provided by cloud to accelerate Python applications. In this paper, three techniques are presented. First, some of the functions in a Python program can be migrated to cloud and be evaluated using the hardware and software provided by that cloud platform, while the other parts still running locally. Second, Briareus can automatically parallelize specified loops in a program to accelerate it. And third, specified functions can be called asynchronously after being patched, so that two or more functions can be evaluated simultaneously. By combining these three methods, a Python application can make part of itself to run in a remote cloud platform in parallel. To use Briareus, developers do not need to modify the existing source much, but only need to insert some descriptive comments and invoke a patching function at the beginning. Experiments show that Briareus can significantly speed up the running of programs written by Python, especially for those for scientific and engineering computing. The early beta version of Briareus has been developed for testing and all sources are accessible to public via GitHub and installable via PyPI.},
booktitle = {Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing Workshops and PhD Forum},
pages = {1449–1456},
numpages = {8},
keywords = {Distributed computing, Parallel architectures, Computer languages, Clouds, Software tools},
series = {IPDPSW '13}
}

@inproceedings{10.1145/3387904.3389288,
author = {Tushev, Miroslav and Mahmoud, Anas},
title = {Linguistic Documentation of Software History},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389288},
doi = {10.1145/3387904.3389288},
abstract = {Open Source Software (OSS) projects start with an initial vocabulary, often determined by the first generation of developers. This vocabulary, embedded in code identifier names and internal code comments, goes through multiple rounds of change, influenced by the interrelated patterns of human (e.g., developers joining and departing) and system (e.g., maintenance activities) interactions. Capturing the dynamics of this change is crucial for understanding and synthesizing code changes over time. However, existing code evolution analysis tools, available in modern version control systems such as GitHub and SourceForge, often overlook the linguistic aspects of code evolution. To bridge this gap, in this paper, we propose to study code evolution in OSS projects through the lens of developers' language, also known as code lexicon. Our analysis is conducted using 32 OSS projects sampled from a broad range of application domains. Our results show that different maintenance activities impact code lexicon differently. These insights lay out a preliminary foundation for modeling the linguistic history of OSS projects. In the long run, this foundation will be utilized to provide support for basic program comprehension tasks and help researchers gain new insights into the complex interplay between linguistic change and various system and human aspects of OSS development.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {386–390},
numpages = {5},
keywords = {Linguistic Change, Code Lexicon, Open Source Software},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.5555/3327144.3327341,
author = {Kazemi, Seyed Mehran and Poole, David},
title = {SimplE Embedding for Link Prediction in Knowledge Graphs},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at https://github.com/Mehran-k/SimplE.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {4289–4300},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@inproceedings{10.1109/MSR.2017.2,
author = {Zampetti, Fiorella and Scalabrino, Simone and Oliveto, Rocco and Canfora, Gerardo and Di Penta, Massimiliano},
title = {How Open Source Projects Use Static Code Analysis Tools in Continuous Integration Pipelines},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.2},
doi = {10.1109/MSR.2017.2},
abstract = {Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a "softer" mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {334–344},
numpages = {11},
keywords = {empirical study, static analysis tools, continuous integration, open source projects},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3017680.3022383,
author = {Bart, Austin Cory and Kafura, Dennis},
title = {BlockPy Interactive Demo: Dual Text/Block Python Programming Environment for Guided Practice and Data Science (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3022383},
doi = {10.1145/3017680.3022383},
abstract = {Introductory non-major learners face the challenge of mastering programming fundamentals while remaining sufficiently motivated to engage with the computing discipline. In particular, multi-disciplinary students struggle to find relevance in traditional computing curricula that tend to either emphasize abstract concepts, focus on entertainment (e.g., game and animation design), or rely on decontextualized settings. To address these issues, this demo introduces BlockPy, a web-based environment for Python (https://blockpy.com). The most powerful feature of BlockPy is a dual text/block view that beginners can freely move between, using advanced Mutual Language Translation techniques. The environment contextualizes introductory programming with data science by integrating real-world data including weather reports, classic book statistics, and historical crime data. A fusion of Blockly and Skulpt, the entire interface runs locally with no need for server sandboxing. BlockPy is also a platform for interactive, guided practice problems with automatic feedback that scaffolds learners. This demo will walk through the novel features of BlockPy's environment, including the instructor's perspective of creating new problems and how BlockPy can be embedded in modern LTI-compatible learning management systems. BlockPy is available online for free and is open-sourced on GitHub. This material is based on work supported by the NSF under Grants No. DGE-0822220, DUE-1444094, and DUE-1624320.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {639–640},
numpages = {2},
keywords = {data science, python, intelligent tutoring, LTI, introductory programming, blocks, scaffolding, mutual language translation, web environments, guided feedback},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/3132847.3133065,
author = {Sathanur, Arun V. and Choudhury, Sutanay and Joslyn, Cliff and Purohit, Sumit},
title = {When Labels Fall Short: Property Graph Simulation via Blending of Network Structure and Vertex Attributes},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133065},
doi = {10.1145/3132847.3133065},
abstract = {Property graphs can be used to represent heterogeneous networks with labeled (attributed) vertices and edges. Given a property graph, simulating another graph with same or greater size with the same statistical properties with respect to the labels and connectivity is critical for privacy preservation and benchmarking purposes. In this work we tackle the problem of capturing the statistical dependence of the edge connectivity on the vertex labels and using the same distribution to regenerate property graphs of the same or expanded size in a scalable manner. However, accurate simulation becomes a challenge when the attributes do not completely explain the network structure. We propose the Property Graph Model (PGM) approach that uses a label augmentation strategy to mitigate the problem and preserve the vertex label and the edge connectivity distributions as well as their correlation, while also replicating the degree distribution. Our proposed algorithm is scalable with a linear complexity in the number of edges in the target graph. We illustrate the efficacy of the PGM approach in regenerating and expanding the datasets by leveraging two distinct illustrations. Our open-source implementation is available on GitHub.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2287–2290},
numpages = {4},
keywords = {label-topology correlation, graph generation, label augmentation, attributed graphs, property graphs, joint distribution},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/2904081.2904085,
author = {Kozma, Viktor and Broman, David},
title = {MORAP: A Modular Robotic Arm Platform for Teaching and Experimenting with Equation-Based Modeling Languages},
year = {2016},
isbn = {9781450342025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2904081.2904085},
doi = {10.1145/2904081.2904085},
abstract = {Equation-based object-oriented (EOO) modeling and simulation techniques have in the last decades gained significant attention both in academia and industry. One of the key properties of EOO languages is modularity, where different components can be developed independently and then connected together to form a complete acausal model. However, extensive modeling without explicit model validation together with a real physical system can result in incorrect assumptions and false conclusions. In particular, in an educational and research setting, it is vital that students experiment both with equation-based models and the real system that is being modeled. In this work-in-progress paper, we present a physical experimental robotic arm platform that is designed for teaching and research. Similar to EOO models, the robotic arm is modular, meaning that its parts can be reconfigured and composed together in various settings, and used for different experiments. The platform is completely open source, where electronic schematics, CAD models for 3D printing, controller software, and component specifications are available on GitHub. The vision is to form a community, where new open source components are continuously added, to enable an open and freely available physical experimental platform for EOO languages.},
booktitle = {Proceedings of the 7th International Workshop on Equation-Based Object-Oriented Modeling Languages and Tools},
pages = {27–30},
numpages = {4},
keywords = {modeling, simulations, equations, robotic arm},
location = {Milano, Italy},
series = {EOOLT '16}
}

@inproceedings{10.1145/2460625.2460716,
author = {Kirton, Travis},
title = {C4: Creative Coding for IOS},
year = {2013},
isbn = {9781450318983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460625.2460716},
doi = {10.1145/2460625.2460716},
abstract = {C4 is a new creative coding framework that focuses on interactivity, visualization and the relationship between various media. Designed for iOS, C4 makes it extremely easy to create apps for iPad, iPhone and iPod devices. Initially developed as a platform for quickly creating interactive artistic works, C4 is developing into a more broad-based language for other areas such as music and data visualization.In this workshop, participants will rapidly prototype interactive animated interfaces on iOS devices. Participants will have the opportunity to learn how to easily create dynamic animations, using all kinds of media including audio, video, shapes, OpenGL objects and more. In addition to this, participants will learn how to easily add the full suite of iOS gestural interaction to their applications and objects. Furthermore, C4 provides easy access to the camera, as well as access to the compass, motion, acceleration, proximity and light sensors. Along the way, participants will be introduced to the larger C4 community through their participation on various social networks, the Stackoverflow community-moderated Q&amp;A forum, and will also be shown how to access and share code on Github.},
booktitle = {Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction},
pages = {411–413},
numpages = {3},
keywords = {media, application programming interface, mobile, creative coding, multitouch, first-class objects},
location = {Barcelona, Spain},
series = {TEI '13}
}

@inproceedings{10.1145/3395363.3404367,
author = {Thompson, George and Sullivan, Allison K.},
title = {ProFL: A Fault Localization Framework for Prolog},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404367},
doi = {10.1145/3395363.3404367},
abstract = {Prolog is a declarative, first-order logic that has been used in a variety of domains to implement heavily rules-based systems. However, it is challenging to write a Prolog program correctly. Fortunately, the SWI-Prolog environment supports a unit testing framework, plunit, which enables developers to systematically check for correctness. However, knowing a program is faulty is just the first step. The developer then needs to fix the program which means the developer needs to determine what part of the program is faulty. ProFL is a fault localization tool that adapts imperative-based fault localization techniques to Prolog’s declarative environment. ProFL takes as input a faulty Prolog program and a plunit test suite. Then, ProFL performs fault localization and returns a list of suspicious program clauses to the user. Our toolset encompasses two different techniques: ProFLs, a spectrum-based technique, and ProFLm, a mutation-based technique. This paper describes our Python implementation of ProFL, which is a command-line tool, released as an open-source project on GitHub (https://github.com/geoorge1d127/ProFL). Our experimental results show ProFL is accurate at localizing faults in our benchmark programs.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {561–564},
numpages = {4},
keywords = {Prolog, Fault localization, Declarative programming},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1109/MSR.2019.00068,
author = {Zhu, Jiaxin and Wei, Jun},
title = {An Empirical Study of Multiple Names and Email Addresses in OSS Version Control Repositories},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00068},
doi = {10.1109/MSR.2019.00068},
abstract = {Data produced by version control systems are widely used in software research and development. Version control data users always use the name or email address field to identify the committer or author of a modification. However, developers may use multiple names and email addresses, which brings difficulties for identification of distinct developers. In this paper, we sample 450 Git repositories from GitHub to study the multiple names and email addresses of developers. We conduct a conservative estimation of its prevalence and impact on related measurements. We merge the multiple names and email addresses of a developer through a method of high precision. With the merged identities, we obtain a number of interesting findings, e.g., about 6% of the developers used multiple names or email addresses in more than 60% of the repositories, and they contributed about half of all the commits. Our impact analysis shows that the multiple names and email addresses issue cannot be ignored for the basic related measurements, e.g., the number of developers in a repository. Our results could help researchers and practitioners have a more clear understanding of multiple names and email addresses in practice to improve the accuracy of related measurements.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {409–420},
numpages = {12},
keywords = {pattern, data, multiple names, version control, multiple email addresses, impact},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3236024.3264594,
author = {Wang, Kaiyuan and Sullivan, Allison and Marinov, Darko and Khurshid, Sarfraz},
title = {ASketch: A Sketching Framework for Alloy},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264594},
doi = {10.1145/3236024.3264594},
abstract = {Alloy is a declarative modeling language that supports first-order logic with transitive closure. Alloy has been used in a variety of domains to model software systems and find design deficiencies. However, it is often challenging to make an Alloy model correct or to debug a faulty Alloy model. ASketch is a sketching/synthesis technique that can help users write correct Alloy models. ASketch allows users to provide a partial Alloy model with holes, a generator that specifies candidate fragments to be considered for each hole, and a set of tests that capture the desired model properties. Then, the tool completes the holes such that all tests for the completed model pass. ASketch uses tests written for the recently introduced AUnit framework, which provides a foundation of testing (unit tests, test execution, and model coverage) for Alloy models in the spirit of traditional unit testing. This paper describes our Java implementation of ASketch, which is a command-line tool, released as an open-source project on GitHub. Our experimental results show that ASketch can handle partial Alloy models with multiple holes and a large search space. The demo video for ASketch can be found at https://youtu.be/T5NIVsV329E.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {916–919},
numpages = {4},
keywords = {ASketch, first-order logic, Sketching},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3270112.3270127,
author = {Bucchiarone, Antonio and Cicchetti, Antonio},
title = {Towards an Adaptive City Journey Planner with MDE},
year = {2018},
isbn = {9781450359658},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3270112.3270127},
doi = {10.1145/3270112.3270127},
abstract = {Although there are many city journey planners already available in the market and involving various transportation services, there is none yet that allows city mobility operators and local government municipalities to be an active part of the city's mobility.In this demonstrator, we present our first attempt towards multi-view based modelling of adaptive and multimodal city journey planners. In particular, by exploiting Model-Driven Engineering (MDE) techniques, the different stakeholders involved in the city mobility are able to provide their own updated information or promote their own challenges at higher levels of abstraction. Such information is then automatically translated into code-based artefacts that implement/ensure the desired journey planning behaviour, notably to filter travel routes and to make the city mobility more sustainable.The journey planner prototype, implementing the proposed solution, is demonstrated in the context of Trento city mobility. A supporting video illustrating the main features and a demonstration of our solution can be found at: https://youtu.be/KM21WD2dQGs, while the related artefacts and the details on how to create your own prototype are available at the demo GitHub repository, reachable at https://github.com/modelsconf2018/artifact-evaluation/tree/master/bucchiarone.},
booktitle = {Proceedings of the 21st ACM/IEEE International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
pages = {7–11},
numpages = {5},
keywords = {journey planning, model-driven engineering, model transformation, smart mobility},
location = {Copenhagen, Denmark},
series = {MODELS '18}
}

@inproceedings{10.1145/3219104.3229261,
author = {Canas, Karen and Ubiera, Brandon and Liu, Xinlian and Liu, Yanling},
title = {Scalable Biomedical Image Synthesis with GAN},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229261},
doi = {10.1145/3219104.3229261},
abstract = {Despite the fast-paced progress in imaging techniques made possible by ubiquitous applications of convolutional neural networks, biomedical imaging has yet to benefit from the full potential of deep learning. An unresolved bottleneck is the lack of training set data. Some experimentally obtained data are kept and preserved by individual research groups where they were produced, out of the reach of the public; more often, high cost and rare occurrences simply mean not enough such images have been made. We propose to develop deep learning based workflow to overcome this barrier. Leveraging the largest radiology data (chest X-Ray) recently published by the NIH, we train a generative adversarial network (GAN) and use it to produce photorealistic images that retain pathological quality. We also explore porting our models to a range of supercomputing platforms and systems that we have access to, including XSEDE, NERSC, OLCF, Blue Waters, NIH Biowulf etc., to investigate and compare their performance. In addition to the obvious benefits of biomedical research, our work will help understand how current supercomputing infrastructure embraces machine learning demands. Our code and enhanced data set are available through GitHub/Binder.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {95},
numpages = {3},
keywords = {Image Synthesis, Deep Learning, Generative Adversarial Network, Biomedical Imaging, Convolutional Neural Network},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1145/3368089.3417940,
author = {Xie, Mulong and Feng, Sidong and Xing, Zhenchang and Chen, Jieshan and Chen, Chunyang},
title = {UIED: A Hybrid Tool for GUI Element Detection},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417940},
doi = {10.1145/3368089.3417940},
abstract = {Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be further edited in popular UI design tools such as Sketch and Photoshop. UIED is evaluated to be capable of accurate detection and useful for downstream works. Tool URL: <a>http://uied.online</a> Github Link: <a>https://github.com/MulongXie/UIED</a>},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1655–1659},
numpages = {5},
keywords = {Computer Vision, Deep Learning, User Interface, Object Detection},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3340531.3412881,
author = {Iglesias, Enrique and Jozashoori, Samaneh and Chaves-Fraga, David and Collarana, Diego and Vidal, Maria-Esther},
title = {SDM-RDFizer: An RML Interpreter for the Efficient Creation of RDF Knowledge Graphs},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412881},
doi = {10.1145/3340531.3412881},
abstract = {In recent years, the amount of data has increased exponentially, and knowledge graphs have gained attention as data structures to integrate data and knowledge harvested from myriad data sources. However, data complexity issues like large volume, high-duplicate rate, and heterogeneity usually characterize these data sources, being required data management tools able to address the negative impact of these issues on the knowledge graph creation process. In this paper, we propose the SDM-RDFizer, an interpreter of the RDF Mapping Language (RML), to transform raw data in various formats into an RDF knowledge graph. SDM-RDFizer implements novel algorithms to execute the logical operators between mappings in RML, allowing thus to scale up to complex scenarios where data is not only broad but has a high-duplication rate. We empirically evaluate the SDM-RDFizer performance against diverse testbeds with diverse configurations of data volume, duplicates, and heterogeneity. The observed results indicate that SDM-RDFizer is two orders of magnitude faster than state of the art, thus, meaning that SDM-RDFizer an interoperable and scalable solution for knowledge graph creation. SDM-RDFizer is publicly available as a resource through a Github repository and a DOI.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3039–3046},
numpages = {8},
keywords = {rml, rdf, knowledge graph},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1145/3357141.3357145,
author = {Santos, Jo\~{a}o Pedro and Rocha, Tha\'{\i}s and Borba, Paulo},
title = {Improving the Prediction of Files Changed by Programming Tasks},
year = {2019},
isbn = {9781450376372},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357141.3357145},
doi = {10.1145/3357141.3357145},
abstract = {Integration conflicts often damage software quality and developers' productivity in a collaborative development environment. For reducing merge conflicts, we could avoid asking developers to execute potentially conflicting tasks in parallel, as long as we can predict the files to be changed by each task. As manually predicting that is hard, the TAITI tool tries to do that in the context of BDD (Behaviour-Driven Development) projects, by statically analysing the automated acceptance tests that validate each task. TAITI computes the set of files that might be executed by the tests of a task (a so called test-based task interface), approximating the files that developers will change when performing the task. Although TAITI performs better than a random task interface, there is space for accuracy improvements. In this paper, we extend the interfaces computed by TAITI by including the dependences of the application files reached by the task tests. To understand the potential benefits of our extension, we evaluate precision and recall of 60 task interfaces from 8 Rails GitHub projects. The results bring evidence that the extended interface improves recall by slightly compromising precision.},
booktitle = {Proceedings of the XIII Brazilian Symposium on Software Components, Architectures, and Reuse},
pages = {53–62},
numpages = {10},
keywords = {Collaborative development, Behaviour-Driven Development, File change prediction},
location = {Salvador, Brazil},
series = {SBCARS '19}
}

@inproceedings{10.1109/ICPC.2019.00025,
author = {de Almeida Filho, Francisco Gon\c{c}alves and Martins, Ant\^{o}nio Diogo Forte and Vinuto, Tiago da Silva and Monteiro, Jos\'{e} Maria and de Sousa, \'{I}talo Pereira and de Castro Machado, Javam and Rocha, Lincoln Souza},
title = {Prevalence of Bad Smells in PL/SQL Projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00025},
doi = {10.1109/ICPC.2019.00025},
abstract = {Code Smell can be defined as any feature in the source code of a software that may indicate possible problems. In database languages, the term Bad Smell has been used as a generalization of Code Smell, once some features that are not directly related to code also can indicate problems, such as, for instance, the inappropriate type of an index structure or a SQL query written inefficiently. Bearing in mind the recurrence of different Bad Smell, they were catalogued. Along with these catalogs, tools were developed to automatically identify Bad Smell occurrences in a given code. With the help of these tools, it has become possible to perform quick and effective analysis. In this context, this paper proposes an exploratory study about Bad Smell in PL/SQL codes, from free software projects, published on GitHub. We analyzed 20 open-source PL/SQL projects and empirically study the prevalence of bad smells. Our results showed that some smells occur together. Besides, some smells are more frequent than others. Based on this principle, this paper has the potential to aid professionals from the databases area to avoid future problems during the development of a PL/SQL project.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {116–121},
numpages = {6},
keywords = {PL/SQL, code smell, bad smell, prevalence},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/3239235.3267440,
author = {Sahal, Emre and Tosun, Ayse},
title = {Identifying Bug-Inducing Changes for Code Additions},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3267440},
doi = {10.1145/3239235.3267440},
abstract = {Background. SZZ algorithm has been popularly used to identify bug-inducing changes in version history. It is still limited to link a fixing change to an inducing one, when the fix constitutes of code additions only. Goal. We improve the original SZZ by proposing a way to link the code additions in a fixing change to a list of candidate inducing changes. Method. The improved version, A-SZZ, finds the code block encapsulating the new code added in a fixing change, and traces back to the historical changes of the code block. We mined the GitHub repositories of two projects, Angular.js and Vue, and ran A-SZZ to identify bug-inducing changes of code additions. We evaluated the effectiveness of A-SZZ in terms of inducing and fixing ratios, and time span between the two changes. Results. The approach works well for linking code additions with previous changes, although it still produces many false positives. Conclusions. Nearly a quarter of the files in fixing changes contain code additions only, and hence, new heuristics should be implemented to link those with inducing changes in a more efficient way.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {57},
numpages = {2},
keywords = {repository mining, bug inducing changes, SZZ},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3238147.3240488,
author = {Janes, Andrea and Mairegger, Michael and Russo, Barbara},
title = {Code_call_lens: Raising the Developer Awareness of Critical Code},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240488},
doi = {10.1145/3238147.3240488},
abstract = {As a developer, it is often complex to foresee the impact of changes in source code on usage, e.g., it is time-consuming to find out all components that will be impacted by a change or estimate the impact on the usability of a failing piece of code. It is therefore hard to decide how much effort in quality assurance is justifiable to obtain the desired business goals. In this paper, to reduce the difficulty for developers to understand the importance of source code, we propose an automated way to provide this information to developers as they are working on a given piece of code. As a proof-of-concept, we developed a plug-in for Microsoft Visual Studio Code that informs about the importance of source code methods based on the frequency of usage by the end-users of the developed software. The plug-in aims to increase the awareness developers have about the importance of source code in an unobtrusive way, helping them to prioritize their effort to quality assurance, technical excellence, and usability. code_call_lens can be downloaded from GitHub at https://github.com/xxMUROxx/vscode.code_call_lens.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {876–879},
numpages = {4},
keywords = {User tracking, Empirical Software Engineering},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3196321.3197546,
author = {Porkol\'{a}b, Zolt\'{a}n and Brunner, Tibor and Krupp, D\'{a}niel and Csord\'{a}s, M\'{a}rton},
title = {Codecompass: An Open Software Comprehension Framework for Industrial Usage},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3197546},
doi = {10.1145/3196321.3197546},
abstract = {CodeCompass is an open source LLVM/Clang-based tool developed by Ericsson Ltd. and E\"{o}tv\"{o}s Lor\'{a}nd University, Budapest to help the understanding of large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithms are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass also utilizes build information to explore the system architecture as well as version control information.CodeCompass is regularly used by hundreds of designers and developers. Having a web-based, pluginable, extensible architecture, the CodeCompass framework can be an open platform to further code comprehension, static analysis and software metrics efforts. The source code and a tutorial is publicly available on GitHub, and a live demo is also available online.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {361–369},
numpages = {9},
keywords = {code comprehension, C/C++ programming language, software visualization},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/2950290.2950334,
author = {Gu, Xiaodong and Zhang, Hongyu and Zhang, Dongmei and Kim, Sunghun},
title = {Deep API Learning},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950334},
doi = {10.1145/2950290.2950334},
abstract = { Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bags-of-words and lack a deep understanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bag-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {631–642},
numpages = {12},
keywords = {API usage, deep learning, code search, API, RNN},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1109/ICGSE.2019.00033,
author = {Kanakis, Georgios and Fischer, Stefan and Khelladi, Djamel Eddine and Egyed, Alexander},
title = {Supporting a Flexible Grouping Mechanism for Collaborating Engineering Teams},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICGSE.2019.00033},
doi = {10.1109/ICGSE.2019.00033},
abstract = {Most engineering tools do not provide much support for collaborating teams and today's engineering knowledge repositories lack flexibility and are limited. Engineering teams have different needs and their team members have different preferences on how and when to collaborate. These needs may depend on the individual work style, the role an engineer has, and the tasks they have to perform within the collaborating group. However, individual collaboration is insufficient and engineers need to collaborate in groups. This work presents a collaboration framework for collaborating groups capable of providing synchronous and asynchronous mode of collaboration. Additionally, our approach enables engineers to mix these collaboration modes to meet the preferences of individual group members. We evaluate the scalability of this framework using four real life large collaboration projects. These projects were found from GitHub and they were under active development by the time of evaluation. We have tested our approach creating groups of different sizes for each project. The results showed that our approach scales to support every case for the groups created. Additionally, we scouted the literature and discovered studies that support the usefulness of different groups with collaboration styles.},
booktitle = {Proceedings of the 14th International Conference on Global Software Engineering},
pages = {119–128},
numpages = {10},
keywords = {software engineering, collaboration, change propagation, collaborating groups},
location = {Montreal, Quebec, Canada},
series = {ICGSE '19}
}

@inproceedings{10.1145/3107411.3108173,
author = {Eslami, Taban and Awan, Muaaz Gul and Saeed, Fahad},
title = {GPU-PCC: A GPU Based Technique to Compute Pairwise Pearson's Correlation Coefficients for Big FMRI Data},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3108173},
doi = {10.1145/3107411.3108173},
abstract = {Functional Magnetic Resonance Imaging (fMRI) is a non-invasive brain imaging technique for studying the brain's functional activities. Pearson's Correlation Coefficient is an important measure for capturing dynamic behaviors and functional connectivity between brain components. One bottleneck in computing Correlation Coefficients is the time it takes to process big fMRI data. In this paper, we propose GPU-PCC, a GPU based algorithm based on vector dot product, which is able to compute pairwise Pearson's Correlation Coefficients while performing computation once for each pair. Our method is able to compute Correlation Coefficients in an ordered fashion without the need to do post-processing reordering of coefficients. We evaluated GPU-PCC using synthetic and real fMRI data and compared it with sequential version of computing Correlation Coefficient on CPU and existing state-of-the-art GPU method. We show that our GPU-PCC runs 94.62x faster as compared to the CPU version and 4.28x faster than the existing GPU based technique on a real fMRI dataset of size 90k voxels. The implemented code is available as GPL license on GitHub portal of our lab at https://github.com/pcdslab/GPU-PCC.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {723–728},
numpages = {6},
keywords = {Pearson's correlation coefficient, FMRI, GPU, time series, CUDA},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1109/ICSE.2017.16,
author = {Khatchadourian, Raffi and Masuhara, Hidehiko},
title = {Automated Refactoring of Legacy Java Software to Default Methods},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.16},
doi = {10.1109/ICSE.2017.16},
abstract = {Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {82–93},
numpages = {12},
keywords = {default methods, Java, refactoring, interfaces},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1109/ICPC.2017.30,
author = {Zampetti, Fiorella and Ponzanelli, Luca and Bavota, Gabriele and Mocci, Andrea and Di Penta, Massimiliano and Lanza, Michele},
title = {How Developers Document Pull Requests with External References},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.30},
doi = {10.1109/ICPC.2017.30},
abstract = {Online resources of formal and informal documentation-such as reference manuals, forum discussions and tutorials-have become an asset to software developers, as they allow them to tackle problems and to learn about new tools, libraries, and technologies. This study investigates to what extent and for which purpose developers refer to external online resources when they contribute changes to a repository by raising a pull request. Our study involved (i) a quantitative analysis of over 150k URLs occurring in pull requests posted in GitHub; (ii) a manual coding of the kinds of software evolution activities performed in commits related to a statistically significant sample of 2,130 pull requests referencing external documentation resources; (iii) a survey with 69 participants, who provided feedback on how they use online resources and how they refer to them when filing a pull request. Results of the study indicate that, on the one hand, developers find external resources useful to learn something new or to solve specific problems, and they perceive useful referring such resources to better document changes. On the other hand, both interviews and repository mining suggest that external resources are still rarely referred in document changes.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {23–33},
numpages = {11},
keywords = {documenting changes, empirical study, online resources},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/2492517.2492634,
author = {Albano, Alice and Guillaume, Jean-Loup and Heymann, S\'{e}bastien and Grand, B\'{e}n\'{e}dicte Le},
title = {A Matter of Time - Intrinsic or Extrinsic - for Diffusion in Evolving Complex Networks},
year = {2013},
isbn = {9781450322409},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2492517.2492634},
doi = {10.1145/2492517.2492634},
abstract = {Diffusion phenomena occur in many kinds of real-world complex networks, e.g., biological, information or social networks. Because of this diversity, several types of diffusion models have been proposed in the literature: epidemiological models, threshold models, innovation adoption models, among others. Many studies aim at investigating diffusion as an evolving phenomenon but mostly occurring on static networks, and much remains to be done to understand diffusion on evolving networks. In order to study the impact of graph dynamics on diffusion, we propose in this paper an innovative approach based on a notion of intrinsic time, where the time unit corresponds to the appearance of a new link in the graph. This original notion of time allows us to isolate somehow the diffusion phenomenon from the evolution of the network. The objective is to compare the diffusion features observed with this intrinsic time concept from those obtained with traditional (extrinsic) time, based on seconds. The comparison of these time concepts is easily understandable yet completely new in the study of diffusion phenomena. We experiment our approach on synthetic graphs, as well as on a dataset extracted from the Github sofware sharing platform.},
booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {202–206},
numpages = {5},
location = {Niagara, Ontario, Canada},
series = {ASONAM '13}
}

@inproceedings{10.1145/2462932.2462950,
author = {West, Andrew G. and Lee, Insup},
title = {Towards Content-Driven Reputation for Collaborative Code Repositories},
year = {2012},
isbn = {9781450316057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462932.2462950},
doi = {10.1145/2462932.2462950},
abstract = {As evidenced by SourceForge and GitHub, code repositories now integrate Web 2.0 functionality that enables global participation with minimal barriers-to-entry. To prevent detrimental contributions enabled by crowdsourcing, reputation is one proposed solution. Fortunately this is an issue that has been addressed in analogous version control systems such as the wiki for natural language content. The WikiTrust algorithm ("content-driven reputation"), while developed and evaluated in wiki environments operates under a possibly shared collaborative assumption: actions that "survive" subsequent edits are reflective of good authorship.In this paper we examine WikiTrust's ability to measure author quality in collaborative code development. We first define a mapping from repositories to wiki environments and use it to evaluate a production SVN repository with 92,000 updates. Analysis is particularly attentive to reputation loss events and attempts to establish ground truth using commit comments and bug tracking. A proof-of-concept evaluation suggests the technique is promising (about two-thirds of reputation loss is justified) with false positives identifying areas for future refinement. Equally as important, these false positives exemplify differences in content evolution and the cooperative process between wikis and code repositories.},
booktitle = {Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration},
articleno = {13},
numpages = {4},
keywords = {reputation, wikis, SVN, code quality, content persistence, code repository, WikiTrust, trust management},
location = {Linz, Austria},
series = {WikiSym '12}
}

@inproceedings{10.1145/3379299.3379305,
author = {Zeng, Yirui and Ma, Zhengming},
title = {A Lightweight Channel-Spatial Attention Network for Real-Time Image De-Raining},
year = {2019},
isbn = {9781450376983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379299.3379305},
doi = {10.1145/3379299.3379305},
abstract = {Image de-raining aims to eliminate rain streaks captured by outdoor equipment such as video surveillance, remote sensor and automatic pilot. Recently, a de-raining method called non-locally enhanced encoder-decoder network (NLEDN) has achieved reliability performance. Nevertheless, it is very time consuming (2.2571s per image) and takes up memory so that it cannot be applied to mobile devices to process image in real-time. To solve this problem, we design a lightweight channel-spatial attention network that is 55 times faster (41ms per image) and memory saving. The most advanced performances are achieved in most de-raining data sets. More specifically, we design a channel-spatial attention dense block (CSADB). The channel attention operation will be carried out together with the spatial attention. Our experiments demonstrate that the network can learn more effective features by this way. In order to make our proposed method more lightweight, the depthwise convolutions are adapted in each block to reduce parameters. We conduct experiments on four public synthetic datasets to demonstrate the effectiveness of our proposed method, which achieve excellent performance. And the real-world de-raining results are also tacked into comparison. Moreover, an additional experiment demonstrates that our method also works well on face hallucination task. The relevant code and trained models will be available in GitHub soon.},
booktitle = {Proceedings of the 2019 2nd International Conference on Digital Medicine and Image Processing},
pages = {43–48},
numpages = {6},
keywords = {Image de-raining, Non-local mean calculation, Mobile net, Squeeze-and-excitation attention},
location = {Shanghai, China},
series = {DMIP '19}
}

@inproceedings{10.1109/ASE.2019.00107,
author = {Amreen, Sadika and Karnauch, Andrey and Mockus, Audris},
title = {Developer Reputation Estimator (DRE)},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00107},
doi = {10.1109/ASE.2019.00107},
abstract = {Evidence shows that developer reputation is extremely important when accepting pull requests or resolving reported issues. It is particularly salient in Free/Libre Open Source Software since the developers are distributed around the world, do not work for the same organization and, in most cases, never meet face to face. The existing solutions to expose developer reputation tend to be forge specific (GitHub), focus on activity instead of impact, do not leverage social or technical networks, and do not correct often misspelled developer identities. We aim to remedy this by amalgamating data from all public Git repositories, measuring the impact of developer work, expose developer's collaborators, and correct notoriously problematic developer identity data. We leverage World of Code (WoC), a collection of an almost complete (and continuously updated) set of Git repositories by first allowing developers to select which of the 34 million(M) Git commit author IDs belong to them and then generating their profiles by treating the selected collection of IDs as that single developer. As a side-effect, these selections serve as a training set for a supervised learning algorithm that merges multiple identity strings belonging to a single individual. As we evaluate the tool and the proposed impact measure, we expect to build on these findings to develop reputation badges that could be associated with pull requests and commits so developers could easier trust and prioritize them.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1082–1085},
numpages = {4},
keywords = {software ecosystem, identity disambiguation, developer reputation},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3307339.3343482,
author = {Eslami, Taban and Saeed, Fahad},
title = {Auto-ASD-Network: A Technique Based on Deep Learning and Support Vector Machines for Diagnosing Autism Spectrum Disorder Using FMRI Data},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343482},
doi = {10.1145/3307339.3343482},
abstract = {Quantitative analysis of brain disorders such as Autism Spectrum Disorder (ASD) is an ongoing field of research. Machine learning and deep learning techniques have been playing an important role in automating the diagnosis of brain disorders by extracting discriminative features from the brain data. In this study, we propose a model called Auto-ASD-Network in order to classify subjects with Autism disorder from healthy subjects using only fMRI data. Our model consists of a multilayer perceptron (MLP) with two hidden layers. We use an algorithm called SMOTE for performing data augmentation in order to generate artificial data and avoid overfitting, which helps increase the classification accuracy. We further investigate the discriminative power of features extracted using MLP by feeding them to an SVM classifier. In order to optimize the hyperparameters of SVM, we use a technique called Auto Tune Models (ATM) which searches over the hyperparameter space to find the best values of SVM hyperparameters. Our model achieves more than 70% classification accuracy for 4 fMRI datasets with the highest accuracy of 80%. It improves the performance of SVM by 26%, the stand-alone MLP by 16% and the state of the art method in ASD classification by 14%. The implemented code will be available as GPL license on GitHub portal of our lab (https://github.com/PCDS).},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {646–651},
numpages = {6},
keywords = {mlp, classification, pearson's correlation coefficient, fmri, time series, asd, deep learning},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1109/ESEM.2017.42,
author = {Griffith, Isaac and Izurieta, Clemente and Huvaere, Chris},
title = {An Industry Perspective to Comparing the SQALE and Quamoco Software Quality Models},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.42},
doi = {10.1109/ESEM.2017.42},
abstract = {Context: We investigate the different perceptions of quality provided by leading operational quality models when used to evaluate software systems from an industry perspective. Goal: To compare and evaluate the quality assessments of two competing quality models and to develop an extensible solution to meet the quality assurance measurement needs of an industry stakeholder -The Construction Engineering Research Laboratory (CERL). Method: In cooperation with our industry partner TechLink, we operationalize the Quamoco quality model and employ a multiple case study design comparing the results of Quamoco and SQALE, two implementations of well known quality models. The study is conducted across current versions of several open source software projects sampled from GitHub and commercial software for sustainment management systems implemented in the C# language from our industry partner. Each project represents a separate embedded unit of study in a given context -open source or commercial. We employ inter-rater agreement and correlation analysis to compare the results of both models, focusing on Maintainability, Reliability, and Security assessments. Results: Our observations suggest that there is a significant disconnect between the assessments of quality under both quality models. Conclusion: In order to support industry adoption, additional work is required to bring competing implementations of quality models into alignment. This exploratory case study helps us shed light into this problem.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {287–296},
numpages = {10},
keywords = {software quality, quality assurance, quality standards},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3416505.3423562,
author = {Steinhauer, Martin and Palomba, Fabio},
title = {Speeding up the Data Extraction of Machine Learning Approaches: A Distributed Framework},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423562},
doi = {10.1145/3416505.3423562},
abstract = {In the last decade, mining software repositories (MSR) has become one of the most important sources to feed machine learning models. Especially open-source projects on platforms like GitHub are providing a tremendous amount of data and make them easily accessible. Nevertheless, there is still a lack of standardized pipelines to extract data in an automated and fast way. Even though several frameworks and tools exist which can fulfill specific tasks or parts of the data extraction process, none of them allow neither building an automated mining pipeline nor the possibility for full parallelization. As a consequence, researchers interested in using mining software repositories to feed machine learning models are often forced to re-implement commonly used tasks leading to additional development time and libraries may not be integrated optimally.  This preliminary study aims to demonstrate current limitations of existing tools and Git itself which are threatening the prospects of standardization and parallelization. We also introduce the multi-dimensionality aspects of a Git repository and how they affect the computation time. Finally, as a proof of concept, we define an exemplary pipeline for predicting refactoring operations, assessing its performance. Finally, we discuss the limitations of the pipeline and further optimizations to be done.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Mining Software Repositories, Machine Learning Pipelines, Distributed Mining},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/3332186.3332217,
author = {Tward, Daniel and Kolasny, Anthony and Khan, Fatima and Troncoso, Juan and Miller, Michael},
title = {Expanding the Computational Anatomy Gateway from Clinical Imaging to Basic Neuroscience Research},
year = {2019},
isbn = {9781450372275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332186.3332217},
doi = {10.1145/3332186.3332217},
abstract = {The Computational Anatomy Gateway, powered largely by the Comet (San Diego Super-computer Center) and Stampede (Texas Advanced Computing Center) clusters through XSEDE, provides software as a service tools for atlas based analysis of human brain magnetic resonance images. This includes deformable registration, automatic labeling of tissue types, and morphometric analysis. Our goal is to extend these services to the broader neuroscience community, accommodating multiple model organisms and imaging modalities, as well as low quality or missing data. We developed a new approach to multimodality registration: by predicting one modality from another, we can replace ad hoc image similarity metrics (such as mutual information or normalized cross correlation) with a log likelihood under a noise model. This statistical approach enables us to account for missing data using the Expectation Maximization algorithm. For portability and scalability we have implemented this algorithm in tensorflow. For accessibility we have compiled and many working examples for multiple model organisms, imaging systems, and missing tissue or image anomaly situations. These examples are made easily usable in the form of Jupyter notebooks, and made publicly available through github. This framework will significantly reduce the barrier to entry for basic neuroscientists, enabling the community to benefit from atlas based computational image analysis techniques.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
articleno = {9},
numpages = {6},
keywords = {brain mapping, neuroimaging, image registration},
location = {Chicago, IL, USA},
series = {PEARC '19}
}

@inproceedings{10.1145/3314221.3314648,
author = {Chibotaru, Victor and Bichsel, Benjamin and Raychev, Veselin and Vechev, Martin},
title = {Scalable Taint Specification Inference with Big Code},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314648},
doi = {10.1145/3314221.3314648},
abstract = {We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library APIs (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on GitHub), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7,000 API roles from over 210,000 candidate APIs with very little supervision (less than 300 annotations) and with high estimated precision (67%). Further, using the learned specifications, our taint analyzer flagged more than 20,000 violations in open source projects, 97% of which were undetectable without the inferred specifications.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {760–774},
numpages = {15},
keywords = {Taint Analysis, Specification Inference, Big Code},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/3297280.3297453,
author = {Javed, Omar and Villaz\'{o}n, Alex and Binder, Walter},
title = {JUniVerse: Large-Scale JUnit-Test Analysis in the Wild},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297453},
doi = {10.1145/3297280.3297453},
abstract = {Current approaches for analyzing large number of open-source projects mainly focus on data mining or on static analysis techniques. On the contrary, research applying dynamic analyses such as Runtime Verification (RV) to open-source projects is scarce. This is due to lack of automatic means for executing arbitrary pieces of software that rely on complex dependencies and input parameters. In this paper, we present a fully automated infrastructure, JUniVerse, to conduct large-scale studies on unit tests in open-source projects in the wild. The proposed infrastructure runs on a cluster for parallel execution. We demonstrate the effectiveness of JUniVerse by conducting a large-scale study on Java projects hosted on GitHub. We apply a selection criteria based on static analysis to select 3 490 active projects. To show the feasibility of JUniVerse, we choose RV as a case study, and investigate the applicability of 182 publicly available JavaMOP specifications to the code exercised by unit tests. Our study reveals that 37 (out of 182) specifications (i.e., 20%) are not applicable to the code exercised by unit tests of real-world projects. Finally, with JUniVerse, we are able to identify a set of specs and projects for future RV studies.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {1768–1775},
numpages = {8},
keywords = {unit testing, software-repository mining, dynamic program analysis, runtime verification, open-source projects},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3236024.3236043,
author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Barr, Earl T.},
title = {Darwinian Data Structure Selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236043},
doi = {10.1145/3236024.3236043},
abstract = {Data structure selection and tuning is laborious but can vastly improve an application’s performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86% (37/43) of the projects. The median improvement across the best solutions is 4.8%, 10.1%, 5.1% for runtime, memory and CPU usage. These aggregate results understate ARTEMIS’s potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache’s XML transformation tool. ARTEMIS improves gson by 16.5%, 1% and 2.2% for memory, runtime, and CPU; ARTEMIS improves xalan’s memory consumption by 23.5%. Every client of these projects will benefit from these performance improvements.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {118–128},
numpages = {11},
keywords = {Search-based Software Engineering, Data Structure Optimisation, Genetic Improvement, Software Analysis and Optimisation},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2676723.2691870,
author = {Redmiles, Elissa and Abad, Mary Allison and Coronado, Isabella and Kross, Sean and Malone, Amelia},
title = {A Classroom Tested Accessible Multimedia Resource for Engaging Underrepresented Students in Computing: The University of Maryland Curriculum In A Box},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2691870},
doi = {10.1145/2676723.2691870},
abstract = {In 2012, women earned 18% of computer science degrees; African American and Hispanic students made up less than 20% of computing degree holders that year. Research shows that relatable role models and engaging curriculum are required to engage underrepresented students in computing. There is a need for engaging and relatable curriculum to be delivered to students at the middle school level, when these students first begin to lose interest in computing. Thus, based on the results of a survey of current and former middle school computing teachers and a comprehensive literature review, we developed the University of Maryland Curriculum In A Box (CIAB). The CIAB includes profiles of relatable computing role models, accessible video and text curriculum and challenge projects for HTML/CSS. To simulate a "real world" programming environment, the CIAB guides students through programming within open source social media frameworks and Github. The CIAB also includes teacher enablement resources such as assessments and a week-by-week implementation guide. The CIAB was successfully implemented with a group of 6th and 7th grade students in Prince Georges (PG) County, a majority minority county in Maryland. Our demo will provide a walk-through of the CIAB assets, accessibility features and design process, as well as implementation advice informed by our CIAB implementation in PG County.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {178},
numpages = {1},
keywords = {curriculum, accessible, middle school, active learning, web-based},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on Evaluation Experiments Using Different Machine Learning Techniques for Defect Prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1145/3387514.3405871,
author = {Kakarla, Siva Kesava Reddy and Beckett, Ryan and Arzani, Behnaz and Millstein, Todd and Varghese, George},
title = {GRooT: Proactive Verification of DNS Configurations},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405871},
doi = {10.1145/3387514.3405871},
abstract = {The Domain Name System (DNS) plays a vital role in today's Internet but relies on complex distributed management of records. DNS misconfiguration related outages have rendered popular services like GitHub, HBO, LinkedIn, and Azure inaccessible for extended periods. This paper introduces GRoot, the first verifier that performs static analysis of DNS configuration files, enabling proactive and exhaustive checking for common DNS bugs; by contrast, existing solutions are reactive and incomplete. GRoot uses a new, fast verification algorithm based on generating and enumerating DNS query equivalence classes. GRoot symbolically executes the set of queries in each equivalence class to efficiently find (or prove the absence of) any bugs such as rewrite loops. To prove the correctness of our approach, we develop a formal semantic model of DNS resolution. Applied to the configuration files from a campus network with over a hundred thousand records, GRoot revealed 109 bugs within seconds. When applied to internal zone files consisting of over 3.5 million records from a large infrastructure service provider, GRoot revealed around 160k issues of blackholing, initiating a cleanup. Finally, on a synthetic dataset with over 65 million real records, we find GRoot can scale to networks with tens of millions of records.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {310–328},
numpages = {19},
keywords = {DNS, Static Analysis, Formal Methods, Verification},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3350768.3350781,
author = {Fazzolino, Rafael and Rodrigues, Gena\'{\i}na Nunes},
title = {Feature-Trace: Generating Operational Profile and Supporting Testing Prioritization from BDD Features},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350781},
doi = {10.1145/3350768.3350781},
abstract = {Operational Profiles provide quantitative information about how the software will be used, which supports highlighting those software components more sensitive to reliability based on their profile usage. However, the generation of Operational Profiles usually requires a considerable team effort to liaise requirements specification until their reification into expected software artifacts. In this sense, it becomes paramount in the software life cycle the ability to seamlessly or efficiently perform traceability from requirement to code, embracing the testing process as a means to ensure that the requirements are satisfiably covered and addressed. In this work, we propose the Feature-Trace approach which merges the advantages of the Operational Profile and the benefits of the requirements-to-code traceability present in the BDD (Behavior-Driven Development) approach. The primary goal of our work is to use the BDD approach as an information source for the semi-automated generation of the Operational Profile. The proposed approach was evaluated on the Diaspora software, on a GitHub open source software. The case study revealed that the Feature-Trace approach is capable of extracting the operational profile seamlessly from the specified Diaspora's BDD features as well as obtaining and presenting vital information to guide the process of test cases prioritization.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {332–336},
numpages = {5},
keywords = {testing, operational profile, requirements traceability, behavior-driven development},
location = {Salvador, Brazil},
series = {SBES 2019}
}

@inproceedings{10.1109/ESEM.2017.46,
author = {Nayebi, Maleknaz and Farrahi, Homayoon and Ruhe, Guenther},
title = {Which Version Should Be Released to App Store?},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.46},
doi = {10.1109/ESEM.2017.46},
abstract = {Background: Several mobile app releases do not find their way to the end users. Our analysis of 11,514 releases across 917 open source mobile apps revealed that 44.3% of releases created in GitHub never shipped to the app store (market). Aims: We introduce "marketability" of open source mobile apps as a new release decision problem. Considering app stores as a complex system with unknown treatments, we evaluate performance of predictive models and analogical reasoning for marketability decisions. Method: We performed a survey with 22 release engineers to identify the importance of marketability release decision. We compared different classifiers to predict release marketability. For guiding the transition of not successfully marketable releases into successful ones, we used analogical reasoning. We evaluated our results both internally (over time) and externally (by developers). Results: Random forest classification performed best with F1 score of 78%. Analyzing 58 releases over time showed that, for 81% of them, analogical reasoning could correctly identify changes in the majority of release attributes. A survey with seven developers showed the usefulness of our method for supporting real world decisions. Conclusions: Marketability decisions of mobile apps can be supported by using predictive analytics and by considering and adopting similar experience from the past.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {324–333},
numpages = {10},
keywords = {release management, marketability, mobile apps, empirical study, analogical reasoning, survey},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.5555/3155562.3155577,
author = {Mirhosseini, Samim and Parnin, Chris},
title = {Can Automated Pull Requests Encourage Software Developers to Upgrade Out-of-Date Dependencies?},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Developers neglect to update legacy software dependencies, resulting in buggy and insecure software. One explanation for this neglect is the difficulty of constantly checking for the availability of new software updates, verifying their safety, and addressing any migration efforts needed when upgrading a dependency. Emerging tools attempt to address this problem by introducing automated pull requests and project badges to inform the developer of stale dependencies. To understand whether these tools actually help developers, we analyzed 7,470 GitHub projects that used these notification mechanisms to identify any change in upgrade behavior. Our results find that, on average, projects that use pull request notifications upgraded 1.6x as often as projects that did not use any tools. Badge notifications were slightly less effective: users upgraded 1.4x more frequently. Unfortunately, although pull request notifications are useful, developers are often overwhelmed by notifications: only a third of pull requests were actually merged. Through a survey, 62 developers indicated that their most significant concerns are breaking changes, understanding the implications of changes, and migration effort. The implications of our work suggests ways in which notifications can be improved to better align with developers' expectations and the need for new mechanisms to reduce notification fatigue and improve confidence in automated pull requests. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {84–94},
numpages = {11},
keywords = {software dependencies, automated pull requests, notification fatigue},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.5555/3154768.3154773,
author = {Unruh, Tommi and Shastry, Bhargava and Skoruppa, Malte and Maggi, Federico and Rieck, Konrad and Seifert, Jean-Pierre and Yamaguchi, Fabian},
title = {Leveraging Flawed Tutorials for Seeding Large-Scaleweb Vulnerability Discovery},
year = {2017},
publisher = {USENIX Association},
address = {USA},
abstract = {The Web is replete with tutorial-style content on how to accomplish programming tasks. Unfortunately, even topranked tutorials suffer from severe security vulnerabilities, such as cross-site scripting (XSS), and SQL injection (SQLi). Assuming that these tutorials influence real-world software development, we hypothesize that code snippets from popular tutorials can be used to bootstrap vulnerability discovery at scale. To validate our hypothesis, we propose a semi-automated approach to find recurring vulnerabilities starting from a handful of top-ranked tutorials that contain vulnerable code snippets. We evaluate our approach by performing an analysis of tens of thousands of open-source web applications to check if vulnerabilities originating in the selected tutorials recur. Our analysis framework has been running on a standard PC, analyzed 64,415 PHP codebases hosted on GitHub thus far, and found a total of 117 vulnerabilities that have a strong syntactic similarity to vulnerable code snippets present in popular tutorials. In addition to shedding light on the anecdotal belief that programmers reuse web tutorial code in an ad hoc manner, our study finds disconcerting evidence of insufficiently reviewed tutorials compromising the security of open-source projects. Moreover, our findings testify to the feasibility of large-scale vulnerability discovery using poorly written tutorials as a starting point.},
booktitle = {Proceedings of the 11th USENIX Conference on Offensive Technologies},
pages = {5},
numpages = {1},
location = {Vancouver, BC, Canada},
series = {WOOT'17}
}

@inproceedings{10.1109/MSR.2017.42,
author = {Omran, Fouad Nasser A Al and Treude, Christoph},
title = {Choosing an NLP Library for Analyzing Software Documentation: A Systematic Literature Review and a Series of Experiments},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.42},
doi = {10.1109/MSR.2017.42},
abstract = {To uncover interesting and actionable information from natural language documents authored by software developers, many researchers rely on "out-of-the-box" NLP libraries. However, software artifacts written in natural language are different from other textual documents due to the technical language used. In this paper, we first analyze the state of the art through a systematic literature review in which we find that only a small minority of papers justify their choice of an NLP library. We then report on a series of experiments in which we applied four state-of-the-art NLP libraries to publicly available software artifacts from three different sources. Our results show low agreement between different libraries (only between 60% and 71% of tokens were assigned the same part-of-speech tag by all four libraries) as well as differences in accuracy depending on source: For example, spaCy achieved the best accuracy on Stack Overflow data with nearly 90% of tokens tagged correctly, while it was clearly outperformed by Google's SyntaxNet when parsing GitHub ReadMe files. Our work implies that researchers should make an informed decision about the particular NLP library they choose and that customizations to libraries might be necessary to achieve good results when analyzing software artifacts written in natural language.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {187–197},
numpages = {11},
keywords = {part-of-speech tagging, NLP libraries, software documentation, natural language processing},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3030207.3030213,
author = {Leitner, Philipp and Bezemer, Cor-Paul},
title = {An Exploratory Study of the State of Practice of Performance Testing in Java-Based Open Source Projects},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030213},
doi = {10.1145/3030207.3030213},
abstract = {The usage of open source (OS) software is wide-spread across many industries. While the functional quality of OS projects is considered to be similar to closed-source software, much is unknown about the quality in terms of performance. One challenge for OS developers is that, unlike for functional testing, there is a lack of accepted best practices for performance testing. To reveal the state of practice of performance testing in OS projects, we conduct an exploratory study on 111 Java-based OS projects from GitHub. We study the performance tests of these projects from five perspectives: (1) developers, (2) size, (3) test organization, (4) types of performance tests and (5) used tooling. We show that writing performance tests is not a popular task in OS projects: performance tests form only a small portion of the test suite, are rarely updated, and are usually maintained by a small group of core project developers. Further, even though many projects are aware that they need performance tests, developers appear to struggle implementing them. We argue that future performance testing frameworks should provider better support for low-friction testing, for instance via non-parameterized methods or performance test generation, as well as focus on a tight integration with standard continuous integration tooling.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {373–384},
numpages = {12},
keywords = {mining software repositories, performance engineering, empirical software engineering, open source, performance testing},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1109/Agile.2015.12,
author = {Rahman, Akond Ashfaque Ur and Helms, Eric and Williams, Laurie and Parnin, Chris},
title = {Synthesizing Continuous Deployment Practices Used in Software Development},
year = {2015},
isbn = {9781467371537},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/Agile.2015.12},
doi = {10.1109/Agile.2015.12},
abstract = {Continuous deployment speeds up the process of existing agile methods, such as Scrum, and Extreme Programming (XP) through the automatic deployment of software changes to end-users upon passing of automated tests. Continuous deployment has become an emerging software engineering process amongst numerous software companies, such as Facebook, Github, Netflix, and Rally Software. A systematic analysis of software practices used in continuous deployment can facilitate a better understanding of continuous deployment as a software engineering process. Such analysis can also help software practitioners in having a shared vocabulary of practices and in choosing the software practices that they can use to implement continuous deployment. The goal of this paper is to aid software practitioners in implementing continuous deployment through a systematic analysis of software practices that are used by software companies. We studied the continuous deployment practices of 19 software companies by performing a qualitative analysis of Internet artifacts and by conducting follow-up inquiries. In total, we found 11 software practices that are used by 19 software companies. We also found that in terms of use, eight of the 11 software practices are common across 14 software companies. We observe that continuous deployment necessitates the consistent use of sound software engineering practices such as automated testing, automated deployment, and code review.},
booktitle = {Proceedings of the 2015 Agile Conference},
pages = {1–10},
numpages = {10},
keywords = {follow-up inquiries, industry practices, continuous deployment, continuous delivery, agile, internet artifacts},
series = {AGILE '15}
}

@inproceedings{10.1145/3196321.3196344,
author = {H\"{a}rtel, Johannes and Aksu, Hakan and L\"{a}mmel, Ralf},
title = {Classification of APIs by Hierarchical Clustering},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196344},
doi = {10.1145/3196321.3196344},
abstract = {APIs can be classified according to the programming domains (e.g., GUIs, databases, collections, or security) that they address. Such classification is vital in searching repositories (e.g., the Maven Central Repository for Java) and for understanding the technology stack used in software projects. We apply hierarchical clustering to a curated suite of Java APIs to compare the computed API clusters with preexisting API classifications. Clustering entails various parameters (e.g., the choice of IDF versus LSI versus LDA). We describe the corresponding variability in terms of a feature model. We exercise all possible configurations to determine the maximum correlation with respect to two baselines: i) a smaller suite of APIs manually classified in previous research; ii) a larger suite of APIs from the Maven Central Repository, thereby taking advantage of crowd-sourced classification while relying on a threshold-based approach for identifying important APIs and versions thereof, subject to an API dependency analysis on GitHub. We discuss the configurations found in this way and we examine the influence of particular features on the correlation between computed clusters and baselines. To this end, we also leverage interactive exploration of the parameter space and the resulting dendrograms. In this manner, we can also identify issues with the use of classifiers (e.g., missing classifiers) in the baselines and limitations of the clustering approach.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {233–243},
numpages = {11},
keywords = {clustering exploration, APIs, maven central repository, hierarchical clustering, github, feature modeling},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3183440.3190332,
author = {Beller, Moritz},
title = {Toward an Empirical Theory of Feedback-Driven Development},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190332},
doi = {10.1145/3183440.3190332},
abstract = {Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are pre- rather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial framework on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {503–505},
numpages = {3},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3379597.3387464,
author = {Wang, Peipei and Brown, Chris and Jennings, Jamie A. and Stolee, Kathryn T.},
title = {An Empirical Study on Regular Expression Bugs},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387464},
doi = {10.1145/3379597.3387464},
abstract = {Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice.This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3%). The remaining root causes are incorrect API usage (9.3%) and other code issues that require regular expression changes in the fix (29.5%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {103–113},
numpages = {11},
keywords = {bug fixes, Regular expression bug characteristics, pull requests},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.5555/3155562.3155689,
author = {Lin, Jinfeng and Liu, Yalin and Guo, Jin and Cleland-Huang, Jane and Goss, William and Liu, Wenchuang and Lohar, Sugandha and Monaikul, Natawut and Rasin, Alexander},
title = {TiQi: A Natural Language Interface for Querying Software Project Data},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Abstract—Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo. Keywords-Natural Language Interface, Project Data, Query },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {973–977},
numpages = {5},
keywords = {Project Data, Natural Language Interface, Query},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3106237.3117771,
author = {Zhou, Yaqin and Sharma, Asankhaya},
title = {Automated Identification of Security Issues from Commit Messages and Bug Reports},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117771},
doi = {10.1145/3106237.3117771},
abstract = { The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {914–919},
numpages = {6},
keywords = {commit, vulnerability identification, bug report, machine learning},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2931037.2931073,
author = {Chapman, Carl and Stolee, Kathryn T.},
title = {Exploring Regular Expression Usage and Context in Python},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931073},
doi = {10.1145/2931037.2931073},
abstract = { Due to the popularity and pervasive use of regular expressions, researchers have created tools to support their creation, validation, and use. However, little is known about the context in which regular expressions are used, the features that are most common, and how behaviorally similar regular expressions are to one another.  In this paper, we explore the context in which regular expressions are used through a combination of developer surveys and repository analysis. We survey 18 professional developers about their regular expression usage and pain points. Then, we analyze nearly 4,000 open source Python projects from GitHub and extract nearly 14,000 unique regular expression patterns. We map the most common features used in regular expressions to those features supported by four major regex research efforts from industry and academia: brics, Hampi, RE2, and Rex. Using similarity analysis of regular expressions across projects, we identify six common behavioral clusters that describe how regular expressions are often used in practice. This is the first rigorous examination of regex usage and it provides empirical evidence to support design decisions by regex tool builders. It also points to areas of needed future work, such as refactoring regular expressions to increase regex understandability and context-specific tool support for common regex usages. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {282–293},
numpages = {12},
keywords = {developer survey, repository analysis, regular expressions},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/2656434.2656440,
author = {Kim, William and Chung, Sam and Endicott-Popovsky, Barbara},
title = {Software Architecture Model Driven Reverse Engineering Approach to Open Source Software Development},
year = {2014},
isbn = {9781450327114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656434.2656440},
doi = {10.1145/2656434.2656440},
abstract = {Popular Open Source Software (OSS) development platforms like GitHub, Google Code, and Bitbucket take advantage of some best practices of traditional software development like version control and issue tracking. Current major open source software environments, including IDE tools and online code repositories, do not provide support for visual architecture modeling. Research has shown that visual modeling of complex software projects has benefits throughout the software lifecycle. Then why is it that software architecture modeling is so conspicuously missing from popular online open source code repositories? How can including visual documentation improve the overall quality of open source software projects? Our goal is to answer both of these questions and bridge the gap between traditional software engineering best practices and open source development by applying a software architecture documentation methodology using Unified Modeling Language, called 5W1H Re-Doc, on a real open source project for managing identity and access, MITREid Connect. We analyze the effect of a model-driven software engineering approach on collaboration of open source contributors, quality of specification conformance, and state-of-the-art of architecture modeling. Our informal experiment revealed that in some cases, having the visual documentation can significantly increase comprehension of an online OSS project over having only the textual information that currently exists for that project.},
booktitle = {Proceedings of the 3rd Annual Conference on Research in Information Technology},
pages = {9–14},
numpages = {6},
keywords = {model-driven software engineering, software architecture documentation, open source software development},
location = {Atlanta, Georgia, USA},
series = {RIIT '14}
}

@inproceedings{10.1145/2506583.2506595,
author = {Robbins, David E. and Gr\"{u}neberg, Alexander and Deus, Helena F. and Tanik, Murat M. and Almeida, Jonas},
title = {TCGA Toolbox: An Open Web App Framework for Distributing Big Data Analysis Pipelines for Cancer Genomics},
year = {2013},
isbn = {9781450324342},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2506583.2506595},
doi = {10.1145/2506583.2506595},
abstract = {The diversity and volume of data generated by the cancer genome atlas (TCGA) has been increasing exponentially, with the number of data files hosted by NHI, currently 3/4 million, doubling every 7 months since January 2010. The proponents have recently developed a browser-based self-updating mechanism to catalog this dynamic big data repository. In this report, that foundation is built upon to devise a web app framework to distribute TCGA analytical pipelines in a manner that can be fully reproducible without the usual requirement for a pre-installed specialized computational statistics environment. The solution found relies exclusively of sandboxed code injection (JavaScript) and on access permission configuration by the browser's app store. This framework was devised with an open architecture such that third party analyses, ideally hosted with web-facing version control in a repository such as GitHub, SourceForge, Bitbucket, or Google Code, can be distributed to the toolbox. The openness of the framework developed is specifically reflected by enabling the user to invoke the third party analysis simply by inputing the corresponding URL. Similarly, the toolbox also mediates the ability of the user to then distribute the result of the analysis as a reproducible procedure, also fully invoked as a Universal Resource Locator (URL).},
booktitle = {Proceedings of the International Conference on Bioinformatics, Computational Biology and Biomedical Informatics},
pages = {62–67},
numpages = {6},
keywords = {The Cancer Genome Atlas, Big Data, Genomics},
location = {Wshington DC, USA},
series = {BCB'13}
}

@inproceedings{10.1109/ICSM.2013.18,
author = {McDonnell, Tyler and Ray, Baishakhi and Kim, Miryung},
title = {An Empirical Study of API Stability and Adoption in the Android Ecosystem},
year = {2013},
isbn = {9780769549811},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2013.18},
doi = {10.1109/ICSM.2013.18},
abstract = {When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28% of API references in client applications are outdated with a median lagging time of 16 months. 22% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Software Maintenance},
pages = {70–79},
numpages = {10},
series = {ICSM '13}
}

@inproceedings{10.1145/3383652.3423901,
author = {Aneja, Deepali and McDuff, Daniel and Czerwinski, Mary},
title = {Conversational Error Analysis in Human-Agent Interaction},
year = {2020},
isbn = {9781450375863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383652.3423901},
doi = {10.1145/3383652.3423901},
abstract = {Conversational Agents (CAs) present many opportunities for changing how we interact with information and computer systems in a more natural, accessible way. Building on research in machine learning and HCI, it is now possible to design and test multi-turn CA that is capable of extended interactions. However, there are many ways in which these CAs can "fail" and fall short of human expectations. We systematically analyzed how five different types of conversational errors impacted perceptions of an embodied CA. Not all errors negatively impacted perceptions of the agent. Repetitions by the agent and clarifications by the human significantly decreased the perceived intelligence and anthropomorphism of the agent. Turn-taking errors significantly decreased the likability of the agent. However, coherence errors significantly positively increased likability, and these errors were also associated with positive valence via facial expressions, suggesting that the users found them amusing. We believe this work is the first to identify that turn-taking, repetition, clarification, and coherence errors directly affect users' acceptance of an embodied CA, and are worth taking note by designers of such systems during dialog configuration. We release the Agent Conversational Error (ACE) dataset, a set of transcripts and error annotations of human-agent conversations. The dataset can be found at the GITHUB link: https://github.com/deepalianeja/ACE-dataset},
booktitle = {Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
articleno = {3},
numpages = {8},
keywords = {conversational error, coherence, clarification, repetition, human-agent dialogue, conversational agent, turn-taking},
location = {Virtual Event, Scotland, UK},
series = {IVA '20}
}

@inproceedings{10.1145/3321707.3321738,
author = {Sobania, Dominik and Rothlauf, Franz},
title = {Teaching GP to Program like a Human Software Developer: Using Perplexity Pressure to Guide Program Synthesis Approaches},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321738},
doi = {10.1145/3321707.3321738},
abstract = {Program synthesis is one of the relevant applications of GP with a strong impact on new fields such as genetic improvement. In order for synthesized code to be used in real-world software, the structure of the programs created by GP must be maintainable. We can teach GP how real-world software is built by learning the relevant properties of mined human-coded software - which can be easily accessed through repository hosting services such as GitHub. So combining program synthesis and repository mining is a logical step. In this paper, we analyze if GP can write programs with properties similar to code produced by human software developers. First, we compare the structure of functions generated by different GP initialization methods to a mined corpus containing real-world software. The results show that the studied GP initialization methods produce a totally different combination of programming language elements in comparison to real-world software. Second, we propose perplexity pressure and analyze how its use changes the properties of code produced by GP. The results are very promising and show that we can guide the search to the desired program structure. Thus, we recommend using perplexity pressure as it can be easily integrated in various search-based algorithms.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1065–1074},
numpages = {10},
keywords = {genetic programming, grammatical evolution, language models, mining software repositories, software synthesis, genetic improvement},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/3233547.3233577,
author = {Amin, Mohammad Ruhul and Yurovsky, Alisa and Tian, Yingtao and Skiena, Steven},
title = {DeepAnnotator: Genome Annotation with Deep Learning},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233577},
doi = {10.1145/3233547.3233577},
abstract = {Genome annotation is the process of labeling DNA sequences of an organism with its biological features, and is one of the fundamental problems in Bioinformatics. Public annotation pipelines such as NCBI integrate a variety of algorithms and homology searches on public and private databases. However, they build on the information of varying consistency and quality, produced over the last two decades. We identified 12,415 errors in NCBI RNA gene annotations, demonstrating the need for improved annotation programs. We use Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) to demonstrate the potential of deep learning networks to annotate genome sequences, and evaluate different approaches on prokaryotic sequences from NCBI database. Particularly, we evaluate DNA $K-$mer embeddings and the application of RNNs for genome annotation. We show how to improve the performance of our deep networks by incorporating intermediate objectives and downstream algorithms to achieve better accuracy. Our method, called DeepAnnotator, achieves an F-score of ~94%, and establishes a generalized computational approach for genome annotation using deep learning. Our results are very encouraging as our method eliminates the requirement of hand crafted features and motivates further research in application of deep learning to full genome annotation. DeepAnnotator algorithms and models can be accessed in Github: urlhttps://github.com/ruhulsbu/DeepAnnotator.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {254–259},
numpages = {6},
keywords = {genome annotation, dna embeddings, rnns, deep learning},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3203217.3203239,
author = {Eslami, Taban and Saeed, Fahad},
title = {Similarity Based Classification of ADHD Using Singular Value Decomposition},
year = {2018},
isbn = {9781450357616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3203217.3203239},
doi = {10.1145/3203217.3203239},
abstract = {Attention deficit hyperactivity disorder (ADHD) is one of the most common brain disorders among children. This disorder is considered as a big threat for public health and causes attention, focus and organizing difficulties for children and even adults. Since the cause of ADHD is not known yet, data mining algorithms are being used to help discover patterns which discriminate healthy from ADHD subjects. Numerous efforts are underway with the goal of developing classification tools for ADHD diagnosis based on functional and structural magnetic resonance imaging data of the brain. In this paper, we used Eros, which is a technique for computing similarity between two multivariate time series along with k-Nearest-Neighbor classifier, to classify healthy vs ADHD children. We designed a model selection scheme called J-Eros which is able to pick the optimum value of k for k-Nearest-Neighbor from the training data. We applied this technique to the public data provided by ADHD-200 Consortium competition and our results show that J-Eros is capable of discriminating healthy from ADHD children such that we outperformed the best results reported by ADHD-200 competition about 20 percent for two datasets.The implemented code is available as GPL license on GitHub portal of our lab at https://github.com/pcdslab/J-Eros.},
booktitle = {Proceedings of the 15th ACM International Conference on Computing Frontiers},
pages = {19–25},
numpages = {7},
keywords = {ADHD disorder, multivariate time series, cross validation, eros, fMRI},
location = {Ischia, Italy},
series = {CF '18}
}

@inproceedings{10.1145/2983323.2983794,
author = {Zhang, Jianwei and Zeng, Jia and Yuan, Mingxuan and Rao, Weixiong and Yan, Jianfeng},
title = {LDA Revisited: Entropy, Prior and Convergence},
year = {2016},
isbn = {9781450340731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983323.2983794},
doi = {10.1145/2983323.2983794},
abstract = {Inference algorithms of latent Dirichlet allocation (LDA), either for small or big data, can be broadly categorized into expectation-maximization (EM), variational Bayes (VB) and collapsed Gibbs sampling (GS). Looking for a unified understanding of these different inference algorithms is currently an important open problem. In this paper, we revisit these three algorithms from the entropy perspective, and show that EM can achieve the best predictive perplexity (a standard performance metric for LDA accuracy) by minimizing directly the cross entropy between the observed word distribution and LDA's predictive distribution. Moreover, EM can change the entropy of LDA's predictive distribution through tuning priors of LDA, such as the Dirichlet hyperparameters and the number of topics, to minimize the cross entropy with the observed word distribution. Finally, we propose the adaptive EM (AEM) algorithm that converges faster and more accurate than the current state-of-the-art SparseLDA [20] and AliasLDA [12] from small to big data and LDA models. The core idea is that the number of active topics, measured by the residuals between E-steps at successive iterations, decreases significantly, leading to the amortized σ(1) time complexity in terms of the number of topics. The open source code of AEM is available at GitHub.},
booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
pages = {1763–1772},
numpages = {10},
keywords = {convergence, prior, big data, latent dirichlet allocation, entropy, adaptive em algorithms},
location = {Indianapolis, Indiana, USA},
series = {CIKM '16}
}

@inproceedings{10.1145/2970276.2970358,
author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
title = {Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970358},
doi = {10.1145/2970276.2970358},
abstract = { Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {426–437},
numpages = {12},
keywords = {mining software repositories, continuous integration},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/2908131.2908145,
author = {Chen, Guanliang and Davis, Dan and Lin, Jun and Hauff, Claudia and Houben, Geert-Jan},
title = {Beyond the MOOC Platform: Gaining Insights about Learners from the Social Web},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908145},
doi = {10.1145/2908131.2908145},
abstract = {Massive Open Online Courses (MOOCs) have enabled millions of learners across the globe to increase their levels of expertise in a wide variety of subjects. Research efforts surrounding MOOCs are typically focused on improving the learning experience, as the current retention rates (less than 7% of registered learners complete a MOOC) show a large gap between vision and reality in MOOC learning.Current data-driven approaches to MOOC adaptations rely on data traces learners generate within a MOOC platform such as edX or Coursera. As a MOOC typically lasts between five and eight weeks and with many MOOC learners being rather passive consumers of the learning material, this exclusive use of MOOC platform data traces limits the insights that can be gained from them.The Social Web potentially offers a rich source of data to supplement the MOOC platform data traces, as many learners are also likely to be active on one or more Social Web platforms. In this work, we present a first exploratory analysis of the Social Web platforms MOOC learners are active on --- we consider more than 320,000 learners that registered for 18 MOOCs on the edX platform and explore their user profiles and activities on StackExchange, GitHub, Twitter and LinkedIn.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {15–24},
numpages = {10},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1145/3311790.3399619,
author = {Nandigam, Viswanath and Lin, Kai and Shantharam, Manu and Sakai, Scott and Sivagnanam, Subhashini},
title = {Research Workflows - Towards Reproducible Science via Detailed Provenance Tracking in Open Science Chain},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3399619},
doi = {10.1145/3311790.3399619},
abstract = {Scientific research has always struggled with problems related to reproducibility caused in part by low data sharing rates and lack of provenance. Credibility of the research hypothesis comes into question when results cannot be replicated. While the growing amount of data and widespread use of computational code in research has been pushing scientific breakthroughs, their references in scientific publications is insufficient from a reproducibility perspective. The NSF funded Open Science Chain (OSC) is a cyberinfrastructure platform built using blockchain technologies that enables researchers to efficiently validate the authenticity of published data, track their provenance and view lineage. It does this by leveraging blockchain technology to securely store metadata and verification information about research data and track changes to that data in an auditable manner. In this poster we introduce the concept of ”research workflows”, a tool that allows researchers to create a detailed workflow of their scientific experiment, linking specific data and computational code used in their published results in order to enable independent verification of the analysis. OSC research workflows will allow for detailed provenance tracking both within the OSC platform as well as external repositories like Github, thereby enabling transparency and fostering trust in the scientific process. },
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {484–486},
numpages = {3},
keywords = {Data Provenance, Data Integrity, Data Reproducibility, Blockchain},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@inproceedings{10.1109/MSR.2019.00039,
author = {Campos, Uriel and Smethurst, Guilherme and Moraes, Jo\~{a}o Pedro and Bonif\'{a}cio, Rodrigo and Pinto, Gustavo},
title = {Mining Rule Violations in JavaScript Code Snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00039},
doi = {10.1109/MSR.2019.00039},
abstract = {Programming code snippets readily available on platforms such as StackOverflow are undoubtedly useful for software engineers. Unfortunately, these code snippets might contain issues such as deprecated, misused, or even buggy code. These issues could pass unattended, if developers do not have adequate knowledge, time, or tool support to catch them. In this work we expand the understanding of such issues (or the so called "violations") hidden in code snippets written in JavaScript, the programming language with the highest number of questions on StackOverflow. To characterize the violations, we extracted 336k code snippets from answers to JavaScript questions on StackOverflow and statically analyzed them using ESLinter, a JavaScript linter. We discovered that there is no single JavaScript code snippet without a rule violation. On average, our studied code snippets have 11 violations, but we found instances of more than 200 violations. In particular, rules related to stylistic issues are by far the most violated ones (82.9% of the violations pertain to this category). Possible errors, which developers might be more interested in, represent only 0.1% of the violations. Finally, we found a small fraction of code snippets flagged with possible errors being reused on actual GitHub software projects. Indeed, one single code snippet with possible errors was reused 1,261 times.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {195–199},
numpages = {5},
keywords = {ES-linter, rule violations, JavaScript code snippets},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3183713.3196888,
author = {Yan, Cong and He, Yeye},
title = {Synthesizing Type-Detection Logic for Rich Semantic Data Types Using Open-Source Code},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196888},
doi = {10.1145/3183713.3196888},
abstract = {Given a table of data, existing systems can often detect basic atomic types (e.g., strings vs. numbers) for each column. A new generation of data-analytics and data-preparation systems are starting to automatically recognize rich semantic types such as date-time, email address, etc., for such metadata can bring an array of benefits including better table understanding, improved search relevance, precise data validation, and semantic data transformation. However, existing approaches only detect a limited number of types using regular-expression-like patterns, which are often inaccurate, and cannot handle rich semantic types such as credit card and ISBN numbers that encode semantic validations (e.g., checksum).We developed AUTOTYPE from open-source repositories like GitHub. Users only need to provide a set of positive examples for a target data type and a search keyword, our system will automatically identify relevant code, and synthesize type-detection functions using execution traces. We compiled a benchmark with 112 semantic types, out of which the proposed system can synthesize code to detect 84 such types at a high precision. Applying the synthesized type-detection logic on web table columns have also resulted in a significant increase in data types discovered compared to alternative approaches.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {35–50},
numpages = {16},
keywords = {metadata management, code search, type detection, semantic data types, open-source code, data preparation},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.5555/2820518.2820599,
author = {Sawant, Anand Ashok and Bacchelli, Alberto},
title = {A Dataset for API Usage},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {An Application Programming Interface (API) provides a specific set of functionalities to a developer. The main aim of an API is to encourage the reuse of already existing functionality. There has been some work done into API popularity trends, API evolution and API usage. For all the aforementioned research avenues there has been a need to mine the usage of an API in order to perform any kind of analysis. Each one of the approaches that has been employed in the past involved a certain degree of inaccuracy as there was no type check that takes place. We introduce an approach that takes type information into account while mining API method invocations and annotation usages. This approach accurately makes a connection between a method invocation and the class of the API to which the method belongs to. We try collecting as many usages of an API as possible, this is achieved by targeting projects hosted on GitHub. Additionally, we look at the history of every project to collect the usage of an API from earliest version onwards. By making such a large and rich dataset public, we hope to stimulate some more research in the field of APIs with the aid of accurate API usage samples.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {506–509},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3387904.3389269,
author = {Shuai, Jianhang and Xu, Ling and Liu, Chao and Yan, Meng and Xia, Xin and Lei, Yan},
title = {Improving Code Search with Co-Attentive Representation Learning},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389269},
doi = {10.1145/3387904.3389269},
abstract = {Searching and reusing existing code from a large-scale codebase, e.g, GitHub, can help developers complete a programming task efficiently. Recently, Gu et al. proposed a deep learning-based model (i.e., DeepCS), which significantly outperformed prior models. The DeepCS embedded codebase and natural language queries into vectors by two LSTM (long and short-term memory) models separately, and returned developers the code with higher similarity to a code search query. However, such embedding method learned two isolated representations for code and query but ignored their internal semantic correlations. As a result, the learned isolated representations of code and query may limit the effectiveness of code search.To address the aforementioned issue, we propose a co-attentive representation learning model, i.e., Co-Attentive Representation Learning Code Search-CNN (CARLCS-CNN). CARLCS-CNN learns interdependent representations for the embedded code and query with a co-attention mechanism. Generally, such mechanism learns a correlation matrix between embedded code and query, and co-attends their semantic relationship via row/column-wise max-pooling. In this way, the semantic correlation between code and query can directly affect their individual representations. We evaluate the effectiveness of CARLCS-CNN on Gu et al.'s dataset with 10k queries. Experimental results show that the proposed CARLCS-CNN model significantly outperforms DeepCS by 26.72% in terms of MRR (mean reciprocal rank). Additionally, CARLCS-CNN is five times faster than DeepCS in model training and four times in testing.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {196–207},
numpages = {12},
keywords = {code search, representation learning, co-attention mechanism},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining Energy-Aware Commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3382494.3422171,
author = {Scoccia, Gian Luca and Autili, Marco},
title = {Web Frameworks for Desktop Apps: An Exploratory Study},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422171},
doi = {10.1145/3382494.3422171},
abstract = {Background: Novel frameworks for the development of desktop applications with web technologies have become popular. These desktop web app frameworks allow developers to reuse existing code and knowledge of web applications for the creation of cross-platform apps integrated with native APIs. Aims: To date, desktop web app frameworks have not been studied empirically. In this paper, we aim to fill this gap by characterizing the usage of web frameworks and providing evidence on how beneficial are their pros and how impactful are their cons. Method: We conducted an empirical study, collecting and analyzing 453 desktop web apps publicly available on GitHub. We performed qualitative and quantitative analyses to uncover the traits and issues of desktop web apps. Results: We found that desktop web app frameworks enable the development of cross-platform applications even for teams of limited dimensions, taking advantage of the abundant number of available web libraries. However, at the same time, bugs deriving from platform compatibility issues are common. Conclusions: Our study provides concrete evidence on some disadvantages associated with desktop web app frameworks. Future work is required to assess their impact on the required development and maintenance effort, and to investigate other aspects not considered in this first research.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {35},
numpages = {6},
keywords = {desktop apps, Web technologies, cross-platform},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1109/NOMS47738.2020.9110380,
author = {Hauser, Frederik and Schmidt, Mark and Menth, Michael},
title = {XRAC: Execution and Access Control for Restricted Application Containers on Managed Hosts},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NOMS47738.2020.9110380},
doi = {10.1109/NOMS47738.2020.9110380},
abstract = {We propose xRAC to permit users to run special applications on managed hosts and to grant them access to protected network resources. We use restricted application containers (RACs) for that purpose. A RAC is a virtualization container with only a selected set of applications. Authentication verifies the RAC user’s identity and the integrity of the RAC image. If the user is permitted to use the RAC on a managed host, launching the RAC is authorized and access to protected network resources may be given, e.g., to internal networks, servers, or the Internet. xRAC simplifies traffic control as the traffic of a RAC has a unique IPv6 address so that it can be easily identified in the network. The architecture of xRAC reuses standard technologies, protocols, and infrastructure. Those are the Docker virtualization platform and 802.1X including EAP-over-UDP and RADIUS. Thus, xRAC improves network security without modifying core parts of applications, hosts, and infrastructure. In this paper, we review the technological background of xRAC, explain its architecture, discuss selected use cases, and investigate on the performance. To demonstrate the feasibility of xRAC, we implement it based on standard components with only a few modifications. Finally, we validate xRAC through experiments. We publish the testbed setup guide and prototypical implementation on GitHub [1].},
booktitle = {NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium},
pages = {1–9},
numpages = {9},
location = {Budapest, Hungary}
}

@inproceedings{10.1145/3364641.3364648,
author = {Oliveira, Johnatan and Viggiato, Markos and Figueiredo, Eduardo},
title = {How Well Do You Know This Library? Mining Experts from Source Code Analysis},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364648},
doi = {10.1145/3364641.3364648},
abstract = {Third-party libraries have been widely adopted in modern software projects due to several benefits, such as code reuse and software quality. Software development is increasingly complex and requires specialists with knowledge in several technologies, such as the nowadays libraries. Such complexity turns it extremely challenging to deliver quality software given the time pressure. For this purpose, it is necessary to identify and hire qualified developers, to obtain a good team, both in open source and proprietary systems. For these reasons, enterprise and open source projects try to build teams composed of highly skilled developers in specific libraries. Developers with expertise in specific libraries may reduce the time spent on software development tasks and improve the quality of the final product. However, their identification may not be trivial. In this paper, we first argue that source code activities can be used to identify library experts. We then evaluate a mining-based strategy to identify library experts. To achieve our goal, we selected the 9 most popular Java libraries and identified the top-10 experts in each library by analyzing commits in 16,703 Java projects on GitHub. We validated the results by applying a survey with 137 library expert candidates and observed, on average, 88% of precision for the applied strategy.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {49–58},
numpages = {10},
keywords = {Mining Software Repositories, Library Experts, Software Skills, Expert Identification},
location = {Fortaleza, Brazil},
series = {SBQS'19}
}

@inproceedings{10.1145/3338906.3338907,
author = {Rigger, Manuel and Marr, Stefan and Adams, Bram and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Understanding GCC Builtins to Develop Better Tools},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338907},
doi = {10.1145/3338906.3338907},
abstract = {C programs can use compiler builtins to provide functionality that the C language lacks. On Linux, GCC provides several thousands of builtins that are also supported by other mature compilers, such as Clang and ICC. Maintainers of other tools lack guidance on whether and which builtins should be implemented to support popular projects. To assist tool developers who want to support GCC builtins, we analyzed builtin use in 4,913 C projects from GitHub. We found that 37% of these projects relied on at least one builtin. Supporting an increasing proportion of projects requires support of an exponentially increasing number of builtins; however, implementing only 10 builtins already covers over 30% of the projects. Since we found that many builtins in our corpus remained unused, the effort needed to support 90% of the projects is moderate, requiring about 110 builtins to be implemented. For each project, we analyzed the evolution of builtin use over time and found that the majority of projects mostly added builtins. This suggests that builtins are not a legacy feature and must be supported in future tools. Systematic testing of builtin support in existing tools revealed that many lacked support for builtins either partially or completely; we also discovered incorrect implementations in various tools, including the formally verified CompCert compiler.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {74–85},
numpages = {12},
keywords = {C GitHub projects, compiler intrinsics, GCC builtins},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3236024.3236033,
author = {Zhang, Yang and Vasilescu, Bogdan and Wang, Huaimin and Filkov, Vladimir},
title = {One Size Does Not Fit All: An Empirical Study of Containerized Continuous Deployment Workflows},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236033},
doi = {10.1145/3236024.3236033},
abstract = {Continuous deployment (CD) is a software development practice aimed at automating delivery and deployment of a software product, following any changes to its code. If properly implemented, CD together with other automation in the development process can bring numerous benefits, including higher control and flexibility over release schedules, lower risks, fewer defects, and easier on-boarding of new developers. Here we focus on the (r)evolution in CD workflows caused by containerization, the virtualization technology that enables packaging an application together with all its dependencies and execution environment in a light-weight, self-contained unit, of which Docker has become the de-facto industry standard. There are many available choices for containerized CD workflows, some more appropriate than others for a given project. Owing to cross-listing of GitHub projects on Docker Hub, in this paper we report on a mixed-methods study to shed light on developers' experiences and expectations with containerized CD workflows. Starting from a survey, we explore the motivations, specific workflows, needs, and barriers with containerized CD. We find two prominent workflows, based on the automated builds feature on Docker Hub or continuous integration services, with different trade-offs. We then propose hypotheses and test them in a large-scale quantitative study.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {295–306},
numpages = {12},
keywords = {GitHub, Containerization, Continuous Deployment, Docker},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/2971808.2972009,
author = {Fursin, Grigori and Lokhmotov, Anton and Plowman, Ed},
title = {Collective Knowledge: Towards R&amp;D Sustainability},
year = {2016},
isbn = {9783981537062},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Research funding bodies strongly encourage research projects to disseminate discovered knowledge and transfer developed technology to industry. Unfortunately, capturing, sharing, reproducing and building upon experimental results has become close to impossible in computer systems' R&amp;D. The main challenges include the ever changing hardware and software technologies, lack of standard experimental methodology and lack of robust knowledge exchange mechanisms apart from publications where reproducibility is still rarely considered.Supported by the EU FP7 TETRACOM Coordination Action, we have developed Collective Knowledge (CK), an open-source framework and methodology that involves the R&amp;D community to solve the above problems collaboratively. CK helps researchers gradually convert their code and data into reusable components and share them via repositories such as GitHub, design and evolve over time experimental scenarios, replay experiments under the same or similar conditions, apply state-of-the-art statistical techniques, crowdsource experiments across different platforms, and enable interactive publications. Importantly, CK encourages the continuity and sustainability of R&amp;D efforts: researchers and engineers can build upon the work of others and make their own work available for others to build upon. We believe that R&amp;D sustainability will lead to better research and faster commercialization, thus increasing return-on-investment.},
booktitle = {Proceedings of the 2016 Conference on Design, Automation &amp; Test in Europe},
pages = {864–869},
numpages = {6},
location = {Dresden, Germany},
series = {DATE '16}
}

@inbook{10.1145/3196398.3196421,
author = {Bernardo, Jo\~{a}o Helis and da Costa, Daniel Alencar and Kulesza, Uir\'{a}},
title = {Studying the Impact of Adopting Continuous Integration on the Delivery Time of Pull Requests},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196421},
abstract = {Continuous Integration (CI) is a software development practice that leads developers to integrate their work more frequently. Software projects have broadly adopted CI to ship new releases more frequently and to improve code integration. The adoption of CI is motivated by the allure of delivering new functionalities more quickly. However, there is little empirical evidence to support such a claim. Through the analysis of 162,653 pull requests (PRs) of 87 GitHub projects that are implemented in 5 different programming languages, we empirically investigate the impact of adopting CI on the time to deliver merged PRs. Surprisingly, only 51.3% of the projects deliver merged PRs more quickly after adopting CI. We also observe that the large increase of PR submissions after CI is a key reason as to why projects deliver PRs more slowly after adopting CI. To investigate the factors that are related to the time-to-delivery of merged PRs, we train regression models that obtain sound median R-squares of 0.64-0.67. Finally, a deeper analysis of our models indicates that, before the adoption of CI, the integration-load of the development team, i.e., the number of submitted PRs competing for being merged, is the most impactful metric on the time to deliver merged PRs before CI. Our models also reveal that PRs that are merged more recently in a release cycle experience a slower delivery time.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {131–141},
numpages = {11}
}

@inproceedings{10.1109/ASE.2015.42,
author = {Lv, Fei and Zhang, Hongyu and Lou, Jian-guang and Wang, Shaowei and Zhang, Dongmei and Zhao, Jianjun},
title = {CodeHow: Effective Code Search Based on API Understanding and Extended Boolean Model},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.42},
doi = {10.1109/ASE.2015.42},
abstract = {Over the years of software development, a vast amount of source code has been accumulated. Many code search tools were proposed to help programmers reuse previously-written code by performing free-text queries over a large-scale codebase. Our experience shows that the accuracy of these code search tools are often unsatisfactory. One major reason is that existing tools lack of query understanding ability. In this paper, we propose CodeHow, a code search technique that can recognize potential APIs a user query refers to. Having understood the potentially relevant APIs, CodeHow expands the query with the APIs and performs code retrieval by applying the Extended Boolean model, which considers the impact of both text similarity and potential APIs on code search. We deploy the backend of CodeHow as a Microsoft Azure service and implement the frontend as a Visual Studio extension. We evaluate CodeHow on a large-scale codebase consisting of 26K C# projects downloaded from GitHub. The experimental results show that when the top 1 results are inspected, CodeHow achieves a precision score of 0.794 (i.e., 79.4% of the first returned results are relevant code snippets). The results also show that CodeHow outperforms conventional code search tools. Furthermore, we perform a controlled experiment and a survey of Microsoft developers. The results confirm the usefulness and effectiveness of CodeHow in programming practices.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {260–270},
numpages = {11},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.5555/2663360.2663368,
author = {Rigby, Peter C. and Barr, Earl T. and Bird, Christian and Devanbu, Prem and German, Daniel M.},
title = {What Effect Does Distributed Version Control Have on OSS Project Organization?},
year = {2013},
isbn = {9781467364416},
publisher = {IEEE Press},
abstract = {Many Open Source Software (OSS) projects are moving form Centralized Version Control (CVC) to Distributed Version Control (DVC). The effect of this shift on project organization and developer collaboration is not well understood. In this paper, we use a theoretical argument to evaluate the appropriateness of using DVC in the context of two very common organization forms in OSS: a dictatorship and a peer group. We find that DVC facilitates large hierarchical communities as well as smaller groups of developers, while CVC allows for consensus-building by a peer group. We also find that the flexibility of DVC systems allows for diverse styles of developer collaboration. With CVC, changes flow up and down (and publicly) via a central repository. In contrast, DVC facilitates collaboration in which work output can flow sideways (and privately) between collaborators, with no repository being inherently more important or central. These sideways flows are a relatively new concept. Developers on the Linux project, who tend to be experienced DVC users, cluster around "sandboxes:" repositories where developers can work together on a particular topic, isolating their changes from other developers. In this work, we focus on two large, mature OSS projects to illustrate these findings. However, we suggest that social media sites like GitHub may engender other original styles of collaboration that deserve further study.},
booktitle = {Proceedings of the 1st International Workshop on Release Engineering},
pages = {29–32},
numpages = {4},
location = {San Francisco, California},
series = {RELENG '13}
}

@inproceedings{10.1145/3368089.3409699,
author = {Pan, Linjie and Cui, Baoquan and Liu, Hao and Yan, Jiwei and Wang, Siqi and Yan, Jun and Zhang, Jian},
title = {Static Asynchronous Component Misuse Detection for Android Applications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409699},
doi = {10.1145/3368089.3409699},
abstract = {Facing the limited resource of smartphones, asynchronous programming significantly improves the performance of Android applications. Android provides several packaged components to ease the development of asynchronous programming. Among them, the AsyncTask component is widely used by developers since it is easy to implement. However, the abuse of AsyncTask component can decrease responsiveness and even lead to crashes. By investigating the Android Developer Documentation and technical forums, we summarize five misuse patterns about AsyncTask. To detect them, we propose a flow, context, object and field-sensitive inter-procedural static analysis approach. Specifically, the static analysis includes typestate analysis, reference analysis and loop analysis. Based on the AsyncTask-related information obtained during static analysis, we check the misuse according to predefined detection rules. The proposed approach is implemented into a tool called AsyncChecker. We evaluate AsyncChecker on a self-designed benchmark suite called AsyncBench and 1,759 real-world apps. AsyncChecker finds 17,946 misused AsyncTask instances in 1,417 real-world apps (80.6%). The precision, recall and F-measure of AsyncChecker on real-world applications are 97.2%, 89.8% and 0.93, respectively. Compared with existing tools, AsyncChecker can detect more asynchronous problems. We report the misuse problems to developers via GitHub. Several developers have confirmed and fixed the problems found by AsyncChecker. The result implies that our approach is effective and developers do take the misuse of AsyncTask as a serious problem.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {952–963},
numpages = {12},
keywords = {Android, Static Analysis, Asynchronous Programming, AsyncTask, Misuse Detection},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3319535.3363283,
author = {Alam, Aftab and Krombholz, Katharina and Bugiel, Sven},
title = {Poster: Let History Not Repeat Itself (This Time) -- Tackling WebAuthn Developer Issues Early On},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363283},
doi = {10.1145/3319535.3363283},
abstract = {The FIDO2 open authentication standard, developed jointly by the FIDO Alliance and the W3C, provides end-users with the means to use public-key cryptography in addition to or even instead of text-based passwords for authentication on the web. Its WebAuthn protocol has been adopted by all major browser vendors and recently also by major service providers (e.g., Google, GitHub, Dropbox, Microsoft, and others). Thus, FIDO2 is a very strong contender for finally tackling the problem of insecure user authentication on the web. However, there remain a number of open questions to be answered for FIDO2 to succeed as expected. In this poster, we focus specifically on the critical question of how well web-service developers can securely roll out WebAuthn in their own services and which issues have to be tackled to help developers in this task. The past has unfortunately shown that software developers struggle with correctly implementing or using security-critical APIs, such as TLS/SSL, password storage, or cryptographic APIs. We report here on ongoing work that investigates potential problem areas and concrete pitfalls for adopters of WebAuthn and tries to lay out a plan of how our community can help developers. We believe that raising awareness for foreseeable developer problems and calling for action to support developers early on is critical on the path for establishing FIDO2 as a de-facto authentication solution.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2669–2671},
numpages = {3},
keywords = {usable security for developers, fido2, webauthn},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1145/3338906.3338955,
author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
title = {A Comprehensive Study on Deep Learning Bug Characteristics},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338955},
doi = {10.1145/3338906.3338955},
abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {510–520},
numpages = {11},
keywords = {Q&amp;A forums, Deep learning bugs, Empirical Study of Bugs, Bugs, Deep learning software},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3196321.3196330,
author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Goues, Claire Le and Vasilescu, Bogdan},
title = {Meaningful Variable Names for Decompiled Code: A Machine Translation Approach},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196330},
doi = {10.1145/3196321.3196330},
abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2% of the time, which represents a lower bound for performance.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {20–30},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3180155.3180208,
author = {Steinmacher, Igor and Pinto, Gustavo and Wiese, Igor Scaliante and Gerosa, Marco A.},
title = {Almost There: A Study on Quasi-Contributors in Open Source Software Projects},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180208},
doi = {10.1145/3180155.3180208},
abstract = {Recent studies suggest that well-known OSS projects struggle to find the needed workforce to continue evolving---in part because external developers fail to overcome their first contribution barriers. In this paper, we investigate how and why quasi-contributors (external developers who did not succeed in getting their contributions accepted to an OSS project) fail. To achieve our goal, we collected data from 21 popular, non-trivial GitHub projects, identified quasi-contributors, and analyzed their pull-requests. In addition, we conducted surveys with quasi-contributors, and projects' integrators, to understand their perceptions about nonacceptance. We found 10,099 quasi-contributors --- about 70% of the total actual contributors --- that submitted 12,367 nonaccepted pull-requests. In five projects, we found more quasi-contributors than actual contributors. About one-third of the developers who took our survey disagreed with the nonacceptance, and around 30% declared the nonacceptance demotivated or prevented them from placing another pull-request. The main reasons for pull-request nonacceptance from the quasi-contributors' perspective were "superseded/duplicated pull-request" and "mismatch between developer's and team's vision/opinion." A manual analysis of a representative sample of 263 pull-requests corroborated with this finding. We also found reasons related to the relationship with the community and lack of experience or commitment from the quasi-contributors. This empirical study is particularly relevant to those interested in fostering developers' participation and retention in OSS communities.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {256–266},
numpages = {11},
keywords = {pull-requests, newcomers, quasi-contributors, open source software},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3106237.3106261,
author = {Garbervetsky, Diego and Zoppi, Edgardo and Livshits, Benjamin},
title = {Toward Full Elasticity in Distributed Static Analysis: The Case of Callgraph Analysis},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106261},
doi = {10.1145/3106237.3106261},
abstract = { In this paper we present the design and implementation of a distributed, whole-program static analysis framework that is designed to scale with the size of the input. Our approach is based on the actor programming model and is deployed in the cloud. Our reliance on a cloud cluster provides a degree of elasticity for CPU, memory, and storage resources. To demonstrate the potential of our technique, we show how a typical call graph analysis can be implemented in a distributed setting. The vision that motivates this work is that every large-scale software repository such as GitHub, BitBucket, or Visual Studio Online will be able to perform static analysis on a large scale. We experimentally validate our implementation of the distributed call graph analysis using a combination of both synthetic and real benchmarks. To show scalability, we demonstrate how the analysis presented in this paper is able to handle inputs that are almost 10 million lines of code (LOC) in size, without running out of memory. Our results show that the analysis scales well in terms of memory pressure independently of the input size, as we add more virtual machines (VMs). As the number of worker VMs increases, we observe that the analysis time generally improves as well. Lastly, we demonstrate that querying the results can be performed with a median latency of 15 ms. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {442–453},
numpages = {12},
keywords = {Program analysis, Performance and scalability, Development environments and tools, Parallel, and concurrent systems, Program comprehension and visualization, distributed},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/3035918.3058739,
author = {Singh, Rohit and Meduri, Vamsi and Elmagarmid, Ahmed and Madden, Samuel and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Solar-Lezama, Armando and Tang, Nan},
title = {Generating Concise Entity Matching Rules},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058739},
doi = {10.1145/3035918.3058739},
abstract = {Entity matching (EM) is a critical part of data integration and cleaning. In many applications, the users need to understand why two entities are considered a match, which reveals the need for interpretable and concise EM rules. We model EM rules in the form of General Boolean Formulas (GBFs) that allows arbitrary attribute matching combined by conjunctions (∨), disjunctions (∧), and negations. (¬) GBFs can generate more concise rules than traditional EM rules represented in disjunctive normal forms (DNFs). We use program synthesis, a powerful tool to automatically generate rules (or programs) that provably satisfy a high-level specification, to automatically synthesize EM rules in GBF format, given only positive and negative matching examples.In this demo, attendees will experience the following features: (1) Interpretability -- they can see and measure the conciseness of EM rules defined using GBFs; (2) Easy customization -- they can provide custom experiment parameters for various datasets, and, easily modify a rich predefined (default) synthesis grammar, using a Web interface; and (3) High performance -- they will be able to compare the generated concise rules, in terms of accuracy, with probabilistic models (e.g., machine learning methods), and hand-written EM rules provided by experts. Moreover, this system will serve as a general platform for evaluating different methods that discover EM rules, which will be released as an open-source tool on GitHub.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1635–1638},
numpages = {4},
keywords = {general boolean formulas, program synthesis, disjunctive normal forms, entity matching},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3368089.3417926,
author = {Li, Boao and Yan, Meng and Xia, Xin and Hu, Xing and Li, Ge and Lo, David},
title = {DeepCommenter: A Deep Code Comment Generation Tool with Hybrid Lexical and Syntactical Information},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417926},
doi = {10.1145/3368089.3417926},
abstract = {As the scale of software projects increases, the code comments are more and more important for program comprehension. Unfortunately, many code comments are missing, mismatched or outdated due to tight development schedule or other reasons. Automatic code comment generation is of great help for developers to comprehend source code and reduce their workload. Thus, we propose a code comment generation tool (DeepCommenter) to generate descriptive comments for Java methods. DeepCommenter formulates the comment generation task as a machine translation problem and exploits a deep neural network that combines the lexical and structural information of Java methods. We implement DeepCommenter in the form of an Integrated Development Environment (i.e., Intellij IDEA) plug-in. Such plug-in is built upon a Client/Server architecture. The client formats the code selected by the user, sends request to the server and inserts the comment generated by the server above the selected code. The server listens for client’s request, analyzes the requested code using the pre-trained model and sends back the generated comment to the client. The pre-trained model learns both the lexical and syntactical information from source code tokens and Abstract Syntax Trees (AST) respectively and combines these two types of information together to generate comments. To evaluate DeepCommenter, we conduct experiments on a large corpus built from a large number of open source Java projects on GitHub. The experimental results on different metrics show that DeepCommenter outperforms the state-of-the-art approaches by a substantial margin.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1571–1575},
numpages = {5},
keywords = {Comment Generation, Program Comprehension, Deep Learning},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3340531.3412777,
author = {Sakor, Ahmad and Singh, Kuldeep and Patel, Anery and Vidal, Maria-Esther},
title = {Falcon 2.0: An Entity and Relation Linking Tool over Wikidata},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3412777},
doi = {10.1145/3340531.3412777},
abstract = {The Natural Language Processing (NLP) community has significantly contributed to the solutions for entity and relation recognition from a natural language text, and possibly linking them to proper matches in Knowledge Graphs (KGs). Considering Wikidata as the background KG, there are still limited tools to link knowledge within the text to Wikidata. In this paper, we present Falcon 2.0, the first joint entity and relation linking tool over Wikidata. It receives a short natural language text in the English language and outputs a ranked list of entities and relations annotated with the proper candidates in Wikidata. The candidates are represented by their Internationalized Resource Identifier (IRI) in Wikidata. Falcon 2.0 resorts to the English language model for the recognition task (e.g., N-Gram tiling and N-Gram splitting), and then an optimization approach for the linking task. We have empirically studied the performance of Falcon 2.0 on Wikidata and concluded that it outperforms all the existing baselines. Falcon 2.0 is open source and can be reused by the community; all the required instructions of Falcon 2.0 are well-documented at our GitHub repository (https://github.com/SDM-TIB/falcon2.0). We also demonstrate an online API, which can be run without any technical expertise. Falcon 2.0 and its background knowledge bases are available as resources at https://labs.tib.eu/falcon/falcon2/.},
booktitle = {Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management},
pages = {3141–3148},
numpages = {8},
keywords = {english morphology, relation linking, nlp, dbpedia, background knowledge, entity linking, wikidata},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}

@inproceedings{10.1109/AIM43001.2020.9159011,
author = {Wan, Fang and Wang, Haokun and Liu, Xiaobo and Yang, Linhan and Song, Chaoyang},
title = {DeepClaw: A Robotic Hardware Benchmarking Platform for Learning Object Manipulation<sup>*</sup>},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AIM43001.2020.9159011},
doi = {10.1109/AIM43001.2020.9159011},
abstract = {We present DeepClaw as a reconfigurable benchmark of robotic hardware and task hierarchy for robot learning. The DeepClaw benchmark aims at a mechatronics perspective of the robot learning problem, which features a minimum design of robot cell that can be easily reconfigured to host robot hardware from various vendors, including manipulators, grippers, cameras, desks, and objects, aiming at a streamlined collection of physical manipulation data and evaluation of the learned skills for hardware benchmarking. We provide a detailed design of the robot cell with readily available parts to build the experiment environment that can host a wide range of robotic hardware commonly adopted for robot learning. We propose a hierarchical pipeline of software integration, including localization, recognition, grasp planning, and motion planning, to streamline learning-based robot control, data collection, and experiment validation towards shareability and reproducibility. We present benchmarking results of the DeepClaw system for a baseline Tic-Tac-Toe task, a bin-clearing task, and a jigsaw puzzle task using three sets of standard robotic hardware. Our results show that tasks defined in DeepClaw can be easily reproduced on three robot cells. Under the same task setup, the differences in robotic hardware used will present a non-negligible impact on the performance metrics of robot learning. All design layouts and codes are hosted on Github for open access (https://github.com/bionicdl-sustech/DeepClaw).},
booktitle = {2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
pages = {2011–2018},
numpages = {8},
location = {Boston, MA, USA}
}

@inproceedings{10.1145/3180155.3180167,
author = {Gu, Xiaodong and Zhang, Hongyu and Kim, Sunghun},
title = {Deep Code Search},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180167},
doi = {10.1145/3180155.3180167},
abstract = {To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {933–944},
numpages = {12},
keywords = {joint embedding, deep learning, code search},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3379597.3387477,
author = {Sousa, Leonardo and Cedrim, Diego and Garcia, Alessandro and Oizumi, Willian and Bibiano, Ana C. and Oliveira, Daniel and Kim, Miryung and Oliveira, Anderson},
title = {Characterizing and Identifying Composite Refactorings: Concepts, Heuristics and Patterns},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387477},
doi = {10.1145/3379597.3387477},
abstract = {Refactoring consists of a transformation applied to improve the program internal structure, for instance, by contributing to remove code smells. Developers often apply multiple interrelated refactorings called composite refactoring. Even though composite refactoring is a common practice, an investigation from different points of view on how composite refactoring manifests in practice is missing. Previous empirical studies also neglect how different kinds of composite refactorings affect the removal, prevalence or introduction of smells. To address these matters, we provide a conceptual framework and two heuristics to respectively characterize and identify composite refactorings within and across commits. Then, we mined the commit history of 48 GitHub software projects. We identified and analyzed 24,911 composite refactorings involving 104,505 single refactorings. Amongst several findings, we observed that most composite refactorings occur in the same commit and have the same refactoring type. We found that several refactorings are semantically related to each other, which occur in different parts of the system but are still related to the same task. Our study is the first to reveal that many smells are introduced in a program due to "incomplete" composite refactorings. Our study is also the first to reveal 111 patterns of composite refactorings that frequently introduce or remove certain smell types. These patterns can be used as guidelines for developers to improve their refactoring practices as well as for designers of recommender systems.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {186–197},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3318464.3389738,
author = {Yan, Cong and He, Yeye},
title = {Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks},
year = {2020},
isbn = {9781450367356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318464.3389738},
doi = {10.1145/3318464.3389738},
abstract = {Data preparation is widely recognized as the most time-consuming process in modern business intelligence (BI) and machine learning (ML) projects. Automating complex data preparation steps (e.g., Pivot, Unpivot, Normalize-JSON, etc.)holds the potential to greatly improve user productivity, and has therefore become a central focus of research. We propose a novel approach to "auto-suggest" contextualized data preparation steps, by "learning" from how data scientists would manipulate data, which are documented by data science notebooks widely available today. Specifically, we crawled over 4M Jupyter notebooks on GitHub, and replayed them step-by-step, to observe not only full input/output tables (data-frames) at each step, but also the exact data-preparation choices data scientists make that they believe are best suited to the input data (e.g., how input tables are Joined/Pivoted/Unpivoted, etc.). By essentially "logging" how data scientists interact with diverse tables, and using the resulting logs as a proxy of "ground truth", we can learn-to-recommend data preparation steps best suited to given user data, just like how search engines (Google or Bing) leverage their click-through logs to learn-to-rank documents. This data-driven and log-driven approach leverages the "collective wisdom" of data scientists embodied in the notebooks, and is shown to significantly outperform strong baselines including commercial systems in terms of accuracy.},
booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
pages = {1539–1554},
numpages = {16},
keywords = {data wrangling, data preparation, pivot and unpivot, learning-to-recommend},
location = {Portland, OR, USA},
series = {SIGMOD '20}
}

@inproceedings{10.1145/3328778.3366905,
author = {Barowy, Daniel W. and Jannen, William K.},
title = {Infrastructor: Flexible, No-Infrastructure Tools for Scaling CS},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366905},
doi = {10.1145/3328778.3366905},
abstract = {Demand for computer science education has skyrocketed in the last decade. Although challenging everywhere, scaling up CS course capacities is especially painful at small, liberal arts colleges (SLACs). SLACs tend to have few instructors, few large-capacity classrooms, and little or no dedicated IT support staff. As CS enrollment growth continues to outpace the ability to hire instructional staff, maintaining the quality of the close, nurturing learning environment that SLACs advertise-and students expect-is a major challenge.We present Infrastructor, a workflow and collection of course scaling tools that address the needs of resource-strapped CS departments. Infrastructor removes unnecessary administrative burdens so that instructors can focus on teaching and mentoring students. Unlike a traditional learning management system (LMS), which is complex, monolithic, and usually administered by a campus-wide IT staff, instructors deploy Infrastructor themselves and can trivially tailor the software to suit their own needs. Notably, Infrastructor does not require local hardware resources or platform-specific tools. Instead, Infrastructor is built on top of version control systems. This design choice lets instructors host courses on commodity, cloud-based repositories like GitHub. Since developing Infrastructor two years ago, we have successfully deployed it in ten sections of CS courses (323 students), and over the next year, we plan to more than double its use in our CS program.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1005–1011},
numpages = {7},
keywords = {course workflow, github, enrollment crunch, capacity scaling},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1109/ICSE.2019.00058,
author = {Arya, Deeksha and Wang, Wenting and Guo, Jin L. C. and Cheng, Jinghui},
title = {Analysis and Detection of Information Types of Open Source Software Issue Discussions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00058},
doi = {10.1109/ICSE.2019.00058},
abstract = {Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {issue tracking system, issue discussion analysis, collaborative software engineering},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3127005.3127014,
author = {Thompson, Christopher and Wagner, David},
title = {A Large-Scale Study of Modern Code Review and Security in Open Source Projects},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127014},
doi = {10.1145/3127005.3127014},
abstract = {Background: Evidence for the relationship between code review process and software security (and software quality) has the potential to help improve code review automation and tools, as well as provide a better understanding of the economics for improving software security and quality. Prior work in this area has primarily been limited to case studies of a small handful of software projects. Aims: We investigate the effect of modern code review on software security. We extend and generalize prior work that has looked at code review and software quality. Method: We gather a very large dataset from GitHub (3,126 projects in 143 languages, with 489,038 issues and 382,771 pull requests), and use a combination of quantification techniques and multiple regression modeling to study the relationship between code review coverage and participation and software quality and security. Results: We find that code review coverage has a significant effect on software security. We confirm prior results that found a relationship between code review coverage and software defects. Most notably, we find evidence of a negative relationship between code review of pull requests and the number of security bugs reported in a project. Conclusions: Our results suggest that implementing code review policies within the pull request model of development may have a positive effect on the quality and security of software.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {83–92},
numpages = {10},
keywords = {software quality, quantification models, mining software repositories, multiple regression models, code review, software security},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1145/2964284.2973796,
author = {Viitanen, Marko and Koivula, Ari and Lemmetti, Ari and Yl\"{a}-Outinen, Arttu and Vanne, Jarno and H\"{a}m\"{a}l\"{a}inen, Timo D.},
title = {Kvazaar: Open-Source HEVC/H.265 Encoder},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973796},
doi = {10.1145/2964284.2973796},
abstract = {Kvazaar is an academic software video encoder for the emerging High Efficiency Video Coding (HEVC/H.265) standard. It provides students, academic professionals, and industry experts a free, cross-platform HEVC encoder for x86, x64, PowerPC, and ARM processors on Windows, Linux, and Mac. Kvazaar is being developed from scratch in C and optimized in Assembly under the LGPLv2.1 license. The development is being coordinated by Ultra Video Group at Tampere University of Technology (TUT) and the implementation work is carried out by an active community on GitHub. Developer friendly source code of Kvazaar makes joining easy for new developers. Currently, Kvazaar includes all essential coding tools of HEVC and its modular source code facilitates parallelization on multi and manycore processors as well as algorithm acceleration on hardware. Kvazaar is able to attain real-time HEVC coding speed up to 4K video on an Intel 14-core Xeon processor. Kvazaar is also supported by FFmpeg and Libav. These de-facto standard multimedia frameworks boost Kvazaar popularity and enable its joint usage with other well-known multimedia processing tools. Nowadays, Kvazaar is an integral part of teaching at TUT and it has got a key role in three Eureka Celtic-Plus projects in the fields of 4K TV broadcasting, virtual advertising, Video on Demand, and video surveillance.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1179–1182},
numpages = {4},
keywords = {open source, kvazaar hevc encoder, video encoder, high efficiency video coding (HEVC), video coding},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.1145/2652524.2652549,
author = {Al Alam, S. M. Didar and Shahnewaz, S. M. and Pfahl, Dietmar and Ruhe, Guenther},
title = {Monitoring Bottlenecks in Achieving Release Readiness: A Retrospective Case Study across Ten OSS Projects},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652549},
doi = {10.1145/2652524.2652549},
abstract = {Context: Not releasing software on time can cause substantial loss in revenue. Continuous awareness of the product release status is required. Release readiness is a time-dependent attribute of the status of the product release, which aggregates the degree of satisfaction of a portfolio of release process and product measures.Goal: At different stages of a release cycle, the goal is to understand frequencies and pattern of occurrence of factors affecting project success by restricting the status of release readiness (called bottlenecks).Method: As a form of explorative case study research, we analyzed ten open source software (OSS) projects taken from the GitHub repository. As a retrospective study covering a period of 28 weeks, we monitored eight release readiness attributes and identified their impact on release readiness over time across the ten projects.Results: Feature completion rate, Bug fixing rate, and Features implemented were observed as the most frequent bottlenecks. The most frequent transition between bottlenecks is from Pull-request completion rate to Bug fixing rate. With the exception of Pull-request completion rate, no significant differences were found in occurrence of bottleneck factors between early and late stage of the release cycle.Conclusions: We received an initial understanding of the most frequent bottleneck factors for release readiness and their likelihood of subsequent occurrence. This is intended to guide the effort spent on improving release engineering.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {60},
numpages = {4},
keywords = {bottleneck identification, retrospective analysis, exploratory case study, release readiness},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1145/3373376.3378503,
author = {Angstadt, Kevin and Jeannin, Jean-Baptiste and Weimer, Westley},
title = {Accelerating Legacy String Kernels via Bounded Automata Learning},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378503},
doi = {10.1145/3373376.3378503},
abstract = {The adoption of hardware accelerators, such as FPGAs, into general-purpose computation pipelines continues to rise, but programming models for these devices lag far behind their CPU counterparts. Legacy programs must often be rewritten at very low levels of abstraction, requiring intimate knowledge of the target accelerator architecture. While techniques such as high-level synthesis can help port some legacy software, many programs perform poorly without manual, architecture-specific optimization.We propose an approach that combines dynamic and static analyses to learn a model of functional behavior for off-the-shelf legacy code and synthesize a hardware description from this model. We develop a framework that transforms Boolean string kernels into hardware descriptions using techniques from both learning theory and software verification. These include Angluin-style state machine learning algorithms, bounded software model checking with incremental loop unrolling, and string decision procedures. Our prototype implementation can correctly learn functionality for kernels that recognize regular languages and provides a near approximation otherwise. We evaluate our prototype tool on a benchmark suite of real-world, legacy string functions mined from GitHub repositories and demonstrate that we are able to learn fully-equivalent hardware designs in 72% of cases and close approximations in another 11%. Finally, we identify and discuss challenges and opportunities for more general adoption of our proposed framework to a wider class of function types.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {235–249},
numpages = {15},
keywords = {automata learning, legacy programs, automata processing},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3308558.3313700,
author = {Kanakia, Anshul and Shen, Zhihong and Eide, Darrin and Wang, Kuansan},
title = {A Scalable Hybrid Research Paper Recommender System for Microsoft Academic},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313700},
doi = {10.1145/3308558.3313700},
abstract = {We present the design and methodology for the large scale hybrid paper recommender system used by Microsoft Academic. The system provides recommendations for approximately 160 million English research papers and patents. Our approach handles incomplete citation information while also alleviating the cold-start problem that often affects other recommender systems. We use the Microsoft Academic Graph (MAG), titles, and available abstracts of research papers to build a recommendation list for all documents, thereby combining co-citation and content based approaches. Tuning system parameters also allows for blending and prioritization of each approach which, in turn, allows us to balance paper novelty versus authority in recommendation results. We evaluate the generated recommendations via a user study of 40 participants, with over 2400 recommendation pairs graded and discuss the quality of the results using P@10 and nDCG scores. We see that there is a strong correlation between participant scores and the similarity rankings produced by our system but that additional focus needs to be put towards improving recommender precision, particularly for content based recommendations. The results of the user survey and associated analysis scripts are made available via GitHub and the recommendations produced by our system are available as part of the MAG on Azure to facilitate further research and light up novel research paper recommendation applications.},
booktitle = {The World Wide Web Conference},
pages = {2893–2899},
numpages = {7},
keywords = {recommender system, document collection, big data, word embedding, k-means, clustering},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1109/ESEM.2017.12,
author = {Ahmed, Iftekhar and Brindescu, Caius and Mannan, Umme Ayda and Jensen, Carlos and Sarma, Anita},
title = {An Empirical Examination of the Relationship between Code Smells and Merge Conflicts},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.12},
doi = {10.1109/ESEM.2017.12},
abstract = {Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both "smelly" and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {58–67},
numpages = {10},
keywords = {code smell, empirical analysis, machine learning, merge conflict},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3377811.3380378,
author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
title = {Repairing Deep Neural Networks: Fix Patterns and Challenges},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380378},
doi = {10.1145/3377811.3380378},
abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1135–1146},
numpages = {12},
keywords = {bugs, bug fix, deep neural networks, bug fix patterns},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3379597.3387479,
author = {Jebnoun, Hadhemi and Ben Braiek, Houssem and Rahman, Mohammad Masudur and Khomh, Foutse},
title = {The Scent of Deep Learning Code: An Empirical Study},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387479},
doi = {10.1145/3379597.3387479},
abstract = {Deep learning practitioners are often interested in improving their model accuracy rather than the interpretability of their models. As a result, deep learning applications are inherently complex in their structures. They also need to continuously evolve in terms of code changes and model updates. Given these confounding factors, there is a great chance of violating the recommended programming practices by the developers in their deep learning applications. In particular, the code quality might be negatively affected due to their drive for the higher model performance. Unfortunately, the code quality of deep learning applications has rarely been studied to date. In this paper, we conduct an empirical study to investigate the distribution of code smells in deep learning applications. To this end, we perform a comparative analysis between deep learning and traditional open-source applications collected from GitHub. We have several major findings. First, long lambda expression, long ternary conditional expression, and complex container comprehension smells are frequently found in deep learning projects. That is, deep learning code involves more complex or longer expressions than the traditional code does. Second, the number of code smells increases across the releases of deep learning applications. Third, we found that there is a co-existence between code smells and software bugs in the studied deep learning code, which confirms our conjecture on the degraded code quality of deep learning applications.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {420–430},
numpages = {11},
keywords = {code quality, Deep learning, code smells},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9013941,
author = {Yan, Haonan and Li, Hui and Xiao, Mingchi and Dai, Rui and Zheng, Xianchun and Zhao, Xingwen and Li, Fenghua},
title = {PGSM-DPI: Precisely Guided Signature Matching of Deep Packet Inspection for Traffic Analysis},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9013941},
doi = {10.1109/GLOBECOM38437.2019.9013941},
abstract = {In the field of network traffic analysis, Deep Packet Inspection (DPI) technology is widely used at present. However, the increase in network traffic has brought tremendous processing pressure on the DPI. Consequently, detection speed has become the bottleneck of the entire application. In order to speed up the traffic detection of DPI, a lot of research works have been applied to improve signature matching algorithms, which is the most influential factor in DPI performance. In this paper, we present a novel method from a different angle called Precisely Guided Signature Matching (PGSM). Instead of matching packets with signature directly, we use supervised learning to automate the rules of specific protocol in PGSM. By testing the performance of a packet in the rules, the target packet could be decided when and which signatures should be matched with. Thus, the PGSM method reduces the number of aimless matches which are useless and numerous. After proposing PGSM, we build a framework called PGSM-DPI to verify the effectiveness of guidance rules. The PGSM-DPI framework consists of PGSM method and open source DPI library. The framework is running on a distributed platform with better throughput and computational performance. Finally, the experimental results demonstrate that our PGSM-DPI can reduce 59.23% original DPI time and increase 21.31% throughput. Besides, all source codes and experimental results can be accessed on our GitHub.},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@inproceedings{10.1145/3213846.3213875,
author = {Shi, August and Gyori, Alex and Mahmood, Suleman and Zhao, Peiyuan and Marinov, Darko},
title = {Evaluating Test-Suite Reduction in Real Software Evolution},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213875},
doi = {10.1145/3213846.3213875},
abstract = {Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.  We perform the first extensive study of TSR using real test failures in (failed) builds that occurred for real code changes. We analyze 1478 failed builds from 32 GitHub projects that run their tests on Travis. Each failed build can have multiple faults, so we propose a family of mappings from test failures to faults. We use these mappings to compute Failed-Build Detection Loss (FBDL), the percentage of failed builds where the reduced test suite misses to detect all the faults detected by the original test suite. We find that FBDL can be up to 52.2%, which is higher than suggested by traditional TSR metrics. Moreover, traditional TSR metrics are not good predictors of FBDL, making it difficult for developers to decide whether to use reduced test suites.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {84–94},
numpages = {11},
keywords = {regression testing, Test-suite reduction, continuous integration},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3183713.3183746,
author = {Gao, Yihan and Huang, Silu and Parameswaran, Aditya},
title = {Navigating the Data Lake with DATAMARAN: Automatically Extracting Structure from Log Datasets},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3183746},
doi = {10.1145/3183713.3183746},
abstract = {Organizations routinely accumulate semi-structured log datasets generated as the output of code; these datasets remain unused and uninterpreted, and occupy wasted space---this phenomenon has been colloquially referred to as "data lake'' problem. One approach to leverage these semi-structured datasets is to convert them into a structured relational format, following which they can be analyzed in conjunction with other datasets. We present DATAMARAN, an tool that extracts structure from semi-structured log datasets with no human supervision. DATAMARAN automatically identifies field and record endpoints, separates the structured parts from the unstructured noise or formatting, and can tease apart multiple structures from within a dataset, in order to efficiently extract structured relational datasets from semi-structured log datasets, at scale with high accuracy. Compared to other unsupervised log dataset extraction tools developed in prior work, DATAMARAN does not require the record boundaries to be known beforehand, making it much more applicable to the noisy log files that are ubiquitous in data lakes. DATAMARAN can successfully extract structured information from all datasets used in prior work, and can achieve 95% extraction accuracy on automatically collected log datasets from GitHub---a substantial 66% increase of accuracy compared to unsupervised schemes from prior work. Our user study further demonstrates that the extraction results of DATAMARAN are closer to the desired structure than competing algorithms.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {943–958},
numpages = {16},
keywords = {unsupervised structure extraction, log datasets},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3041021.3053052,
author = {Wang, Jingbo and Aryani, Amir and Wyborn, Lesley and Evans, Ben},
title = {Providing Research Graph Data in JSON-LD Using Schema.Org},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053052},
doi = {10.1145/3041021.3053052},
abstract = {In this position paper, we describe a pilot project that provides Research Graph records to external web services using JSON-LD. The Research Graph database contains a large-scale graph that links research datasets (i.e., data used to support research) to funding records (i.e. grants), publications and researcher records such as ORCID profiles. This database was derived from the work of the Research Data Alliance Working Group on Data Description Registry Interoperability (DDRI), and curated using the Research Data Switchboard open source software. By being available in Linked Data format, the Research Graph database is more accessible to third-party web services over the Internet, which thus opens the opportunity to connect to the rest of the world in the semantic format.The primary purpose of this pilot project is to evaluate the feasibility of converting registry objects in Research Graph to JSON-LD by accessing widely used vocabularies published at Schema.org. In this paper, we provide examples of publications, datasets and grants from international research institutions such as CERN INSPIREHEP, National Computational Infrastructure (NCI) in Australia, and Australian Research Council (ARC). Furthermore, we show how these Research Graph records are made semantically available as Linked Data through using Schema.org. The mapping between Research Graph schema and Schema.org is available on GitHub repository. We also discuss the potential need for an extension to Schema.org vocabulary for scholarly communication.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1213–1218},
numpages = {6},
keywords = {semantic web, linked data, schema.org, json-ld},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/2983990.2984041,
author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
title = {Probabilistic Model for Code with Decision Trees},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984041},
doi = {10.1145/2983990.2984041},
abstract = { In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).  The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy.  Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy. },
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {731–747},
numpages = {17},
keywords = {Code Completion, Probabilistic Models of Code, Decision Trees},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984041,
author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
title = {Probabilistic Model for Code with Decision Trees},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984041},
doi = {10.1145/3022671.2984041},
abstract = { In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., GitHub) to make predictions about new programs (e.g., code completion, repair, etc).  The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called TGen). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as ID3, but also to obtain new variants we refer to as ID3+ and E13, not previously explored and ones that outperform ID3 in prediction accuracy.  Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of JavaScript and Python. Our experimental results indicate that Deep3 predicts elements of JavaScript and Python code with precision above 82% and 69%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy. },
journal = {SIGPLAN Not.},
month = oct,
pages = {731–747},
numpages = {17},
keywords = {Probabilistic Models of Code, Code Completion, Decision Trees}
}

@inproceedings{10.1145/2642937.2648627,
author = {Thung, Ferdian and Kochhar, Pavneet Singh and Lo, David},
title = {DupFinder: Integrated Tool Support for Duplicate Bug Report Detection},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2648627},
doi = {10.1145/2642937.2648627},
abstract = {To track bugs that appear in a software, developers often make use of a bug tracking system. Users can report bugs that they encounter in such a system. Bug reporting is inherently an uncoordinated distributed process though and thus when a user submits a new bug report, there might be cases when another bug report describing exactly the same problem is already present in the system. Such bug reports are duplicate of each other and these duplicate bug reports need to be identified. A number of past studies have proposed a number of automated approaches to detect duplicate bug reports. However, these approaches are not integrated to existing bug tracking systems. In this paper, we propose a tool named DupFinder, which implements the state-of-the-art unsupervised duplicate bug report approach by Runeson et al., as a Bugzilla extension. DupFinder does not require any training data and thus can easily be deployed to any project. DupFinder extracts texts from summary and description fields of a new bug report and recent bug reports present in a bug tracking system, uses vector space model to measure similarity of bug reports, and provides developers with a list of potential duplicate bug reports based on the similarity of these reports with the new bug report. We have released DupFinder as an open source tool in GitHub, which is available at: https://github.com/smagsmu/dupfinder.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {871–874},
numpages = {4},
keywords = {bugzilla, duplicate bug reports, integrated tool support},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3308558.3313729,
author = {Waller, Isaac and Anderson, Ashton},
title = {Generalists and Specialists: Using Community Embeddings to Quantify Activity Diversity in Online Platforms},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313729},
doi = {10.1145/3308558.3313729},
abstract = {In many online platforms, people must choose how broadly to allocate their energy. Should one concentrate on a narrow area of focus, and become a specialist, or apply oneself more broadly, and become a generalist? In this work, we propose a principled measure of how generalist or specialist a user is, and study behavior in online platforms through this lens. To do this, we construct highly accurate community embeddings that represent communities in a high-dimensional space. We develop sets of community analogies and use them to optimize our embeddings so that they encode community relationships extremely well. Based on these embeddings, we introduce a natural measure of activity diversity, the GS-score. Applying our embedding-based measure to online platforms, we observe a broad spectrum of user activity styles, from extreme specialists to extreme generalists, in both community membership on Reddit and programming contributions on GitHub. We find that activity diversity is related to many important phenomena of user behavior. For example, specialists are much more likely to stay in communities they contribute to, but generalists are much more likely to remain on platforms as a whole. We also find that generalists engage with significantly more diverse sets of users than specialists do. Furthermore, our methodology leads to a simple algorithm for community recommendation, matching state-of-the-art methods like collaborative filtering. Our methods and results introduce an important new dimension of online user behavior and shed light on many aspects of online platform use.},
booktitle = {The World Wide Web Conference},
pages = {1954–1964},
numpages = {11},
keywords = {community embeddings, generalist and specialists, activity diversity, community recommendation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3236024.3275527,
author = {Celik, Ahmet and Lee, Young Chul and Gligoric, Milos},
title = {Regression Test Selection for TizenRT},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275527},
doi = {10.1145/3236024.3275527},
abstract = {Regression testing - running tests after code modifications - is widely practiced in industry, including at Samsung. Regression Test Selection (RTS) optimizes regression testing by skipping tests that are not affected by recent code changes. Recent work has developed robust RTS tools, which mostly target managed languages, e.g., Java and C#, and thus are not applicable to large C projects, e.g., TizenRT, a lightweight RTOS-based platform.  We present Selfection, an RTS tool for projects written in C; we discuss the key challenges to develop Selfection and our design decisions. Selfection uses the objdump and readelf tools to statically build a dependency graph of functions from binaries and detect modified code elements. We integrated Selfection in TizenRT and evaluated its benefits if tests are run in an emulator and on a supported hardware platform (ARTIK 053). We used the latest 150 revisions of TizenRT available on GitHub. We measured the benefits of Selfection as the reduction in the number of tests and reduction in test execution time over running all tests at each revision (i.e., RetestAll). Our results show that Selfection can reduce, on average, the number of tests to 4.95% and end-to-end execution time to 7.04% when tests are executed in the emulator, and to 5.74% and 26.82% when tests are executed on the actual hardware. Our results also show that the time taken to maintain the dependency graph and detect modified functions is negligible.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {845–850},
numpages = {6},
keywords = {Regression test selection, static dependency analysis, TizenRT},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3236024.3275525,
author = {Liang, Jie and Jiang, Yu and Chen, Yuanliang and Wang, Mingzhe and Zhou, Chijin and Sun, Jiaguang},
title = {PAFL: Extend Fuzzing Optimizations of Single Mode to Industrial Parallel Mode},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275525},
doi = {10.1145/3236024.3275525},
abstract = {Researchers have proposed many optimizations to improve the efficiency of fuzzing, and most optimized strategies work very well on their targets when running in single mode with instantiating one fuzzer instance. However, in real industrial practice, most fuzzers run in parallel mode with instantiating multiple fuzzer instances, and those optimizations unfortunately fail to maintain the efficiency improvements.  In this paper, we present PAFL, a framework that utilizes efficient guiding information synchronization and task division to extend those existing fuzzing optimizations of single mode to industrial parallel mode. With an additional data structure to store the guiding information, the synchronization ensures the information is shared and updated among different fuzzer instances timely. Then, the task division promotes the diversity of fuzzer instances by splitting the fuzzing task into several sub-tasks based on branch bitmap. We first evaluate PAFL using 12 different real-world programs from Google fuzzer-test-suite. Results show that in parallel mode, two AFL improvers–AFLFast and FairFuzz do not outperform AFL, which is different from the case in single mode. However, when augmented with PAFL, the performance of AFLFast and FairFuzz in parallel mode improves. They cover 8% and 17% more branches, trigger 79% and 52% more unique crashes. For further evaluation on more widely-used software systems from GitHub, optimized fuzzers augmented with PAFL find more real bugs, and 25 of which are security-critical vulnerabilities registered as CVEs in the US National Vulnerability Database.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {809–814},
numpages = {6},
keywords = {Parallel, Fuzzing, Software testing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3097983.3106683,
author = {Pafka, Szil\'{a}rd},
title = {Machine Learning Software in Practice: Quo Vadis?},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3106683},
doi = {10.1145/3097983.3106683},
abstract = {Due to the hype in our industry in the last couple of years, there is a growing mismatch between software tools machine learning practitioners wish for, what they would truly need for their work, what's available (either commercially or open source) and what tool developers and researchers focus on. In this talk we will give a couple of examples of this mismatch. Several surveys and anecdotal evidence show that most practitioners work most of the time (at least in the modeling phase) with datasets that t in the RAM of a single server, therefore distributed computing tools are very of- ten overkill. Our benchmarks (available on github [1]) of the most widely used open source tools for binary classification (various implementations of algorithms such as linear methods, random forests, gradient boosted trees and neural networks) on such data show over 10x speed and over 10x RAM usage difference between various tools, with "big data" tools being the most inefficient. Significant performance gains have been obtained by those tools that incorporate various low-level (close to CPU and memory architecture) optimizations. Nevertheless, we will show that even the best tools show degrading performance on the multi-socket servers featuring a high number of cores, systems that have become widely accessible more recently. Finally, while most of this talk is about performance, we will also argue that machine learning tools that feature high-level easy-to-use APIs provide increasing productivity for practitioners and therefore are preferable.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {25},
numpages = {1},
keywords = {software implementations, training speed, accuracy, memory footprint, binary classification},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1109/ICSE.2017.30,
author = {Wittern, Erik and Ying, Annie T. T. and Zheng, Yunhui and Dolby, Julian and Laredo, Jim A.},
title = {Statically Checking Web API Requests in JavaScript},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.30},
doi = {10.1109/ICSE.2017.30},
abstract = {Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {244–254},
numpages = {11},
keywords = {static analysis, JavaScript, web APIs},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/2896825.2896833,
author = {Wilder, Nathan and Smith, Jared M. and Mockus, Audris},
title = {Exploring a Framework for Identity and Attribute Linking across Heterogeneous Data Systems},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896833},
doi = {10.1145/2896825.2896833},
abstract = {Online-activity-generated digital traces provide opportunities for novel services and unique insights as demonstrated in, for example, research on mining software repositories. The inability to link these traces within and among systems, such as Twitter, GitHub, or Reddit, inhibit the advances in this area. Furthermore, no single approach to integrate data from these disparate sources is likely to work. We aim to design Foreseer, an extensible framework, to design and evaluate identity matching techniques for public, large, and low-accuracy operational data. Foreseer consists of three functionally independent components designed to address the issues of discovery and preparation, storage and representation, and analysis and linking of traces from disparate online sources. The framework includes a domain specific language for manipulating traces, generating insights, and building novel services. We have applied it in a pilot study of roughly 10TB of data from Twitter, Reddit, and StackExchange including roughly 6M distinct entities and, using basic matching techniques, found roughly 83,000 matches among these sources. We plan to add additional entity extraction and identification algorithms, data from other sources, and design tools for facilitating dynamic ingestion and tagging of incoming data on a more robust infrastructure using Apache Spark or another distributed processing framework. We will then evaluate the utility and effectiveness of the framework in applications ranging from identifying malicious contributors in software repositories to the evaluation of the utility of privacy preservation schemes.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {19–25},
numpages = {7},
keywords = {entity identification, big data architecture, entity extraction, identity linking, domain specific language},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1109/ICSM.2015.7332449,
author = {Vendome, Christopher and Linares-Vasquez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel M. and Poshyvanyk, Denys},
title = {When and Why Developers Adopt and Change Software Licenses},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332449},
doi = {10.1109/ICSM.2015.7332449},
abstract = {Software licenses legally govern the way in which developers can use, modify, and redistribute a particular system. While previous studies either investigated licensing through mining software repositories or studied licensing through FOSS reuse, we aim at understanding the rationale behind developers' decisions for choosing or changing software licensing by surveying open source developers. In this paper, we analyze when developers consider licensing, the reasons why developers pick a license for their project, and the factors that influence licensing changes. Additionally, we explore the licensing-related problems that developers experienced and expectations they have for licensing support from forges (e.g., GitHub). Our investigation involves, on one hand, the analysis of the commit history of 16,221 Java open source projects to identify the commits where licenses were added or changed. On the other hand, it consisted of a survey—in which 138 developers informed their involvement in licensing-related decisions and 52 provided deeper insights about the rationale behind the actions that they had undertaken. The results indicate that developers adopt licenses early in the project's development and change licensing after some period of development (if at all). We also found that developers have inherent biases with respect to software licensing. Additionally, reuse—whether by a non-contributor or for commercial purposes—is a dominant reason why developers change licenses of their systems. Finally, we discuss potential areas of research that could ameliorate the difficulties that software developers are facing with regard to licensing issues of their software systems.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {31–40},
numpages = {10},
series = {ICSME '15}
}

@inproceedings{10.1145/3416506.3423579,
author = {Liu, Binbin and Dong, Wei and Zhang, Yating and Wang, Daiyan and Liu, Jiaxin},
title = {Boosting Component-Based Synthesis with Control Structure Recommendation},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423579},
doi = {10.1145/3416506.3423579},
abstract = {Component-based synthesis is an important research field in program synthesis. API-based synthesis is a subfield of component-based synthesis, the component library of which are Java APIs. Unlike existing work in API-based synthesis that can only generate loop-free programs constituted by APIs, state-of-the-art work FrAngel can generate programs with control structures. However, for the generation of control structures, it samples different types of control structures all at random. Given the information about the desired method (such as method name and input/output types), experienced programmers can have an initial thought about the possible control structures that could be used in implementing the desired method. The knowledge about control structures in the method can be learned from high-quality projects. In this paper, we propose a novel approach of recommending control structures for API-based synthesis based on deep learning. A neural network that can jointly embed the natural language description, method name, and input/output types into high-dimensional vectors to predict the possible control structures of the desired method is proposed. We integrate the prediction model into the synthesizer to improve the efficiency of synthesis. We train our model on a codebase of high-quality Java projects from GitHub. The prediction results of the neural network are fed to the API-based synthesizer to guide the sampling process of control structures. The experimental results on 40 programming tasks show that our approach can effectively improve the efficiency of synthesis.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {19–28},
numpages = {10},
keywords = {deep learning, program synthesis, control structure recommendation},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.1109/ASE.2019.00032,
author = {Saifullah, C M Khaled and Asaduzzaman, Muhammad and Roy, Chanchal K.},
title = {Learning from Examples to Find Fully Qualified Names of API Elements in Code Snippets},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00032},
doi = {10.1109/ASE.2019.00032},
abstract = {Developers often reuse code snippets from online forums, such as Stack Overflow, to learn API usages of software frameworks or libraries. These code snippets often contain ambiguous undeclared external references. Such external references make it difficult to learn and use those APIs correctly. In particular, reusing code snippets containing such ambiguous undeclared external references requires significant manual efforts and expertise to resolve them. Manually resolving fully qualified names (FQN) of API elements is a non-trivial task. In this paper, we propose a novel context-sensitive technique, called COSTER, to resolve FQNs of API elements in such code snippets. The proposed technique collects locally specific source code elements as well as globally related tokens as the context of FQNs, calculates likelihood scores, and builds an occurrence likelihood dictionary (OLD). Given an API element as a query, COSTER captures the context of the query API element, matches that with the FQNs of API elements stored in the OLD, and rank those matched FQNs leveraging three different scores: likelihood, context similarity, and name similarity scores. Evaluation with more than 600K code examples collected from GitHub and two different Stack Overflow datasets shows that our proposed technique improves precision by 4-6% and recall by 3-22% compared to state-of-the-art techniques. The proposed technique significantly reduces the training time compared to the StatType, a state-of-the-art technique, without sacrificing accuracy. Extensive analyses on results demonstrate the robustness of the proposed technique.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {243–254},
numpages = {12},
keywords = {recommendation system, context sensitive technique, fully qualified name, API usages},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3357766.3359541,
author = {Seifer, Philipp and H\"{a}rtel, Johannes and Leinberger, Martin and L\"{a}mmel, Ralf and Staab, Steffen},
title = {Empirical Study on the Usage of Graph Query Languages in Open Source Java Projects},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359541},
doi = {10.1145/3357766.3359541},
abstract = {Graph data models are interesting in various domains, in part because of the intuitiveness and flexibility they offer compared to relational models. Specialized query languages, such as Cypher for property graphs or SPARQL for RDF, facilitate their use. In this paper, we present an empirical study on the usage of graph-based query languages in open-source Java projects on GitHub. We investigate the usage of SPARQL, Cypher, Gremlin and GraphQL in terms of popularity and their development over time. We select repositories based on dependencies related to these technologies and employ various popularity and source-code based filters and ranking features for a targeted selection of projects. For the concrete languages SPARQL and Cypher, we analyze the activity of repositories over time. For SPARQL, we investigate common application domains, query use and existence of ontological data modeling in applications that query for concrete instance data. Our results show, that the usage of graph query languages in open-source projects increased over the last years, with SPARQL and Cypher being by far the most popular. SPARQL projects are more active in terms of query related artifact changes and unique developers involved, but Cypher is catching up. Relatively few applications use SPARQL to query for concrete instance data: A majority of those applications employ multiple different ontologies, including project and domain specific ones. Common application domains are management systems and data visualization tools.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {152–166},
numpages = {15},
keywords = {Query Languages, Graphs, Gremlin, Empirical Study, GitHub, GraphQL, SPARQL, Cypher},
location = {Athens, Greece},
series = {SLE 2019}
}

@inproceedings{10.1145/3340672.3341113,
author = {R\"{u}mmer, Philipp},
title = {JayHorn: A Java Model Checker},
year = {2019},
isbn = {9781450368643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340672.3341113},
doi = {10.1145/3340672.3341113},
abstract = {This talk will give an overview of the JayHorn verification tool, a model checker for sequential Java programs annotated with assertions expressing safety conditions. JayHorn is fully automatic and based to a large degree on standard infrastructure for compilation and verification: it uses the Soot library as front-end to read Java bytecode and translate it to the Jimple three-address format, and the state-of-the-art Horn solvers SPACER and Eldarica as back-ends that infer loop invariants, object and class invariants, and method contracts. Since JayHorn uses an invariant-based representation of heap data-structures, it is particularly useful for analysing programs with unbounded data-structures and unbounded run-time, while at the same time avoiding the use of logical theories, like the theory of arrays, often considered hard for Horn solvers. The development of JayHorn is ongoing, and the talk will also cover some of the future features of JayHorn, in particular the handling of strings.The talk presents joint work with Daniel Dietsch, Temesghen Kahsai, Rody Kersten, Huascar Sanchez, Martin Sch\"{a}f, and Valentin W\"{u}stholz.JayHorn is open source and distributed under MIT license, and its source code is available on Github (https://github.com/jayhorn/jayhorn). The development of JayHorn is funded in parts by AFRL contract No. FA8750-15-C-0010, NSF Award No. 1422705, by the Swedish Research Council (VR) under grants 2014-5484 and 2018-4727, and by the Swedish Foundation for Strategic Research (SSF) under the project WebSec (Ref. RIT17-0011).},
booktitle = {Proceedings of the 21st Workshop on Formal Techniques for Java-like Programs},
articleno = {1},
numpages = {1},
location = {London, United Kingdom},
series = {FTfJP '19}
}

@inproceedings{10.1145/3287324.3287554,
author = {Malan, David J. and Lloyd, Doug and Zidane, Kareem},
title = {Interactive Programming Environments for Teachers and Students},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287554},
doi = {10.1145/3287324.3287554},
abstract = {We present in this hands-on workshop a suite of interactive programming environments for teachers and students, each of them cloud-based and free. The first is CS50 Sandbox, a web app at sandbox.cs50.io that enables teachers and students to create temporary programming environments quickly and share copies of those sandboxes with others. With this app can a teacher start programs in class that students can then finish, distribute starter code for problems, and post interactive solutions. The second tool is CS50 Lab, a web app at lab.cs50.io that enables teachers to create step-by-step programming lessons, providing incremental feedback at each step, and enables students to progress from an empty file (or starter code) to working code, with hints and feedback along the way. Via this app can teachers author their own Codecademy-style lessons using just a GitHub repository of their own. And third in the suite is CS50 IDE, a web app at ide.cs50.io built atop Cloud9 that provides students with their own cloud-based Linux environment. Each of these environments offers a built-in file browser and code editor and, most importantly, an interactive terminal window with shell access to their very own container. And each enables students to write programs in any language. Throughout this workshop will we discuss lessons learned from having deployed these tools in CS50 at Harvard to hundreds of students on campus and thousands of students online. And we'll discuss challenges encountered and best practices adopted.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1242},
numpages = {1},
keywords = {ide, programming, sandbox},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1109/MOBILESoft.2017.29,
author = {Kessentini, Marouane and Ouni, Ali},
title = {Detecting Android Smells Using Multi-Objective Genetic Programming},
year = {2017},
isbn = {9781538626696},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MOBILESoft.2017.29},
doi = {10.1109/MOBILESoft.2017.29},
abstract = {The evolution rate of mobile applications is much higher than regular software applications having shorter release deadlines and smaller code base. Mobile applications tend to be evolved quickly by developers to meet several new customer requirements and fix discovered bugs. However, evolving the existing features and design may introduce bad design practices, also called code smells, which can highly decrease the maintainability and performance of these mobile applications. However, unlike the area of object-oriented software systems, the detection of code smells in mobile applications received a very little of attention. Recent, few studies defined a set of quality metrics for Android applications and proposed a support to manually write a set of rules to detect code smells by combining these quality metrics. However, finding the best combination of metrics and their thresholds to identify code smells is left to the developer as a manual process. In this paper, we propose to automatically generate rules for the detection of code smells in Android applications using a multi-objective genetic programming algorithm (MOGP). The MOGP algorithm aims at finding the best set of rules that cover a set of code smell examples of Android applications based on two conflicting objective functions of precision and recall. We evaluate our approach on 184 Android projects with source code hosted in GitHub. The statistical test of our results show that the generated detection rules identified 10 Android smell types on these mobile applications with an average correctness higher than 82% and an average relevance of 77% based on the feedback of active developers of mobile apps.},
booktitle = {Proceedings of the 4th International Conference on Mobile Software Engineering and Systems},
pages = {122–132},
numpages = {11},
keywords = {search-based software engineering, quality, Android apps},
location = {Buenos Aires, Argentina},
series = {MOBILESoft '17}
}

@inproceedings{10.1145/2884781.2884874,
author = {Lu, Yafeng and Lou, Yiling and Cheng, Shiyang and Zhang, Lingming and Hao, Dan and Zhou, Yangfan and Zhang, Lu},
title = {How Does Regression Test Prioritization Perform in Real-World Software Evolution?},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884874},
doi = {10.1145/2884781.2884874},
abstract = {In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {535–546},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2810103.2813604,
author = {Perl, Henning and Dechand, Sergej and Smith, Matthew and Arp, Daniel and Yamaguchi, Fabian and Rieck, Konrad and Fahl, Sascha and Acar, Yasemin},
title = {VCCFinder: Finding Potential Vulnerabilities in Open-Source Projects to Assist Code Audits},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2810103.2813604},
doi = {10.1145/2810103.2813604},
abstract = {Despite the security community's best effort, the number of serious vulnerabilities discovered in software is increasing rapidly. In theory, security audits should find and remove the vulnerabilities before the code ever gets deployed. However, due to the enormous amount of code being produced, as well as a the lack of manpower and expertise, not all code is sufficiently audited. Thus, many vulnerabilities slip into production systems. A best-practice approach is to use a code metric analysis tool, such as Flawfinder, to flag potentially dangerous code so that it can receive special attention. However, because these tools have a very high false-positive rate, the manual effort needed to find vulnerabilities remains overwhelming. In this paper, we present a new method of finding potentially dangerous code in code repositories with a significantly lower false-positive rate than comparable systems. We combine code-metric analysis with metadata gathered from code repositories to help code review teams prioritize their work. The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database. Second, based on this database, we trained a SVM classifier to flag suspicious commits. Compared to Flawfinder, our approach reduces the amount of false alarms by over 99 % at the same level of recall. Finally, we present a thorough quantitative and qualitative analysis of our approach and discuss lessons learned from the results. We will share the database as a benchmark for future research and will also provide our analysis tool as a web service.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {426–437},
numpages = {12},
keywords = {machine learning, static analysis, vulnerabilities},
location = {Denver, Colorado, USA},
series = {CCS '15}
}

@inproceedings{10.1109/MSR.2019.00021,
author = {Rahman, Musfiqur and Rigby, Peter C and Palani, Dharani and Nguyen, Tien},
title = {Cleaning StackOverflow for Machine Translation},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00021},
doi = {10.1109/MSR.2019.00021},
abstract = {Generating source code API sequences from an English query using Machine Translation (MT) has gained much interest in recent years. For any kind of MT, the model needs to be trained on a parallel corpus. In this paper we clean StackOverflow, one of the most popular online discussion forums for programmers, to generate a parallel English-Code corpus from Android posts. We contrast three data cleaning approaches: standard NLP, title only, and software task extraction. We evaluate the quality of the each corpus for MT. To provide indicators of how useful each corpus will be for machine translation, we provide researchers with measurements of the corpus size, percentage of unique tokens, and per-word maximum likelihood alignment entropy. We have used these corpus cleaning approaches to translate between English and Code [22, 23], to compare existing SMT approaches from word mapping to neural networks [24], and to re-examine the "natural software" hypothesis [29]. After cleaning and aligning the data, we create a simple maximum likelihood MT model to show that English words in the corpus map to a small number of specific code elements. This model provides a basis for the success of using StackOverflow for search and other tasks in the software engineering literature and paves the way for MT. Our scripts and corpora are publicly available on GitHub [1] as well as at https://search.datacite.org/works/10.5281/zenodo.2558551.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {79–83},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3107411.3107466,
author = {Awan, Muaaz Gul and Saeed, Fahad},
title = {An Out-of-Core GPU Based Dimensionality Reduction Algorithm for Big Mass Spectrometry Data and Its Application in Bottom-up Proteomics},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107466},
doi = {10.1145/3107411.3107466},
abstract = {Modern high resolution Mass Spectrometry instruments can generate millions of spectra in a single systems biology experiment. Each spectrum consists of thousands of peaks but only a small number of peaks actively contribute to deduction of peptides. Therefore, pre-processing of MS data to detect noisy and non-useful peaks are an active area of research. Most of the sequential noise reducing algorithms are impractical to use as a pre-processing step due to high time-complexity. In this paper, we present a GPU based dimensionality-reduction algorithm, called G-MSR, for MS2 spectra. Our proposed algorithm uses novel data structures which optimize the memory and computational operations inside GPU. These novel data structures include Binary Spectra and Quantized Indexed Spectra (QIS). The former helps in communicating essential information between CPU and GPU using minimum amount of data while latter enables us to store and process complex 3-D data structure into a 1-D array structure while maintaining the integrity of MS data. Our proposed algorithm also takes into account the limited memory of GPUs and switches between in-core and out-of-core modes based upon the size of input data. G-MSR achieves a peak speed-up of 386x over its sequential counterpart and is shown to process over a million spectra in just 32 seconds. The code for this algorithm is available as a GPL open-source at GitHub at the following link: https://github.com/pcdslab/G-MSR.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {550–555},
numpages = {6},
keywords = {out-of-core, cuda, mass spectrometry, bigdata, gpu, proteomics, parallel computing, data reduction, algorithms},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/2635868.2661678,
author = {Thung, Ferdian and Le, Tien-Duy B. and Kochhar, Pavneet Singh and Lo, David},
title = {BugLocalizer: Integrated Tool Support for Bug Localization},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2661678},
doi = {10.1145/2635868.2661678},
abstract = { To manage bugs that appear in a software, developers often make use of a bug tracking system such as Bugzilla. Users can report bugs that they encounter in such a system. Whenever a user reports a new bug report, developers need to read the summary and description of the bug report and manually locate the buggy files based on this information. This manual process is often time consuming and tedious. Thus, a number of past studies have proposed bug localization techniques to automatically recover potentially buggy files from bug reports. Unfortunately, none of these techniques are integrated to bug tracking systems and thus it hinders their adoption by practitioners. To help disseminate research in bug localization to practitioners, we develop a tool named BugLocalizer, which is implemented as a Bugzilla extension and builds upon a recently proposed bug localization technique. Our tool extracts texts from summary and description fields of a bug report and source code files. It then computes similarities of the bug report with source code files to find the buggy files. Developers can use our tool online from a Bugzilla web interface by providing a link to a git source code repository and specifying the version of the repository to be analyzed. We have released our tool publicly in GitHub, which is available at: https://github.com/smagsmu/buglocalizer. We have also provided a demo video, which can be accessed at: http://youtu.be/iWHaLNCUjBY. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {767–770},
numpages = {4},
keywords = {Bug localization, Bugzilla, git},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/3307339.3343175,
author = {Pearson, Antony},
title = {Extracting Structure from Contaminated Symbolic Data},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343175},
doi = {10.1145/3307339.3343175},
abstract = {Symbolic data is the epitome of modern biological datasets. Modern sequencing technologies produce millions of reads giving insights on genome sequence, transcription levels, epigenetic modifications, and much more. To analyze those sequences one usually makes assumptions on their underlying structure, e.g., that the number of reads has Poisson distribution, or that transcription factor binding events occur independently at nonoverlapping promoters. These types of assumptions are often not exactly correct in reality. In fact, even when they are valid, a small amount of data "contamination" may make them appear untrue. The traditional approach to questioning assumptions on data has been hypothesis testing. This approach has various shortcomings, however. Particularly, its Boolean nature does not give room for a null hypothesis to be "approximately true.'' This tutorial introduces a methodology to assess statistical assumptions on symbolic data that may be contaminated. It will give a general overview on how to approach these problems numerically, and present analytical results for some special classes of structured probability distributions. It will demonstrate the applicability of this rather new methodology with DNA methylation data to question the common but unconscious assumption that methylation of CpGs is exchangeable, and transcription factor binding site k-mers to question the use of logoplots in summarizing TFBS behaviour. Data and code for this tutorial, in the form an iPython Notebook, will be made available via GitHub. This work is in collaboration with M. E. Lladser.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {556},
numpages = {1},
keywords = {independence, exchangeability, epigenetics, dna methylation, contamination, transcription factor binding sites, symbolic data},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1109/RAISE.2019.00010,
author = {Ferenc, Rudolf and Hegedundefineds, P\'{e}ter and Gyimesi, P\'{e}ter and Antal, G\'{a}bor and B\'{a}n, D\'{e}nes and Gyim\'{o}thy, Tibor},
title = {Challenging Machine Learning Algorithms in Predicting Vulnerable JavaScript Functions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAISE.2019.00010},
doi = {10.1109/RAISE.2019.00010},
abstract = {The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs.We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find the best performing models. We also examined the effect of various re-sampling strategies to handle the imbalanced nature of the dataset.The best performing algorithm was KNN, which created a model for the prediction of vulnerable functions with an F-measure of 0.76 (0.91 precision and 0.66 recall). Moreover, deep learning, tree and forest based classifiers, and SVM were competitive with F-measures over 0.70. Although the F-measures did not vary significantly with the re-sampling strategies, the distribution of precision and recall did change. No re-sampling seemed to produce models preferring high precision, while resampling strategies balanced the IR measures.},
booktitle = {Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {8–14},
numpages = {7},
keywords = {code metrics, dataset, machine learning, vulnerability, deep learning, JavaScript},
location = {Montreal, Quebec, Canada},
series = {RAISE '19}
}

@inproceedings{10.1145/3127005.3127009,
author = {Businge, John and Kawuma, Simon and Bainomugisha, Engineer and Khomh, Foutse and Nabaasa, Evarist},
title = {Code Authorship and Fault-Proneness of Open-Source Android Applications: An Empirical Study},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127009},
doi = {10.1145/3127005.3127009},
abstract = {Context: In recent years, many research studies have shown how human factors play a significant role in the quality of software components. Code authorship metrics have been introduced to establish a chain of responsibility and simplify management when assigning tasks in large and distributed software development teams. Researchers have investigated the relationship between code authorship metrics and fault occurrences in software systems. However, we have observed that these studies have only been carried on large software systems having hundreds to thousands of contributors. In our preliminary investigations on Android applications that are considered to be relatively small, we observed that applications systems are not totally owned by a single developer (as one could expect) and that cases of no clear authorship also exist like in large systems. To this end, we do believe that the Android applications could face the same challenges faced by large software systems and could also benefit from such studies.Goal: We investigate the extent to which the findings obtained on large software systems applies to Android applications. Approach: Building on the designs of previous studies, we analyze 278 Android applications carefully selected from GitHub. We extract code authorship metrics from the applications and examine the relationship between code authorship metrics and faults using statistical modeling.Results: Our analyses confirm most of the previous findings, i.e., Android applications with higher levels of code authorship among contributors experience fewer faults.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {33–42},
numpages = {10},
keywords = {Software faults, Minor Contributors, Major Contributors, Most Values Contributors, Total Contributors},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1145/3388440.3412418,
author = {Li, Yue and Nair, Pratheeksha and Wen, Zhi and Chafi, Imane and Okhmatovskaia, Anya and Powell, Guido and Shen, Yannan and Buckeridge, David},
title = {Global Surveillance of COVID-19 by Mining News Media Using a Multi-Source Dynamic Embedded Topic Model},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412418},
doi = {10.1145/3388440.3412418},
abstract = {As the COVID-19 pandemic continues to unfold, understanding the global impact of non-pharmacological interventions (NPI) is important for formulating effective intervention strategies, particularly as many countries prepare for future waves. We used a machine learning approach to distill latent topics related to NPI from large-scale international news media. We hypothesize that these topics are informative about the timing and nature of implemented NPI, dependent on the source of the information (e.g., local news versus official government announcements) and the target countries. Given a set of latent topics associated with NPI (e.g., self-quarantine, social distancing, online education, etc), we assume that countries and media sources have different prior distributions over these topics, which are sampled to generate the news articles. To model the source-specific topic priors, we developed a semi-supervised, multi-source, dynamic, embedded topic model. Our model is able to simultaneously infer latent topics and learn a linear classifier to predict NPI labels using the topic mixtures as input for each news article. To learn these models, we developed an efficient end-to-end amortized variational inference algorithm. We applied our models to news data collected and labelled by the World Health Organization (WHO) and the Global Public Health Intelligence Network (GPHIN). Through comprehensive experiments, we observed superior topic quality and intervention prediction accuracy, compared to the baseline embedded topic models, which ignore information on media source and intervention labels. The inferred latent topics reveal distinct policies and media framing in different countries and media sources, and also characterize reaction to COVID-19 and NPI in a semantically meaningful manner. Our PyTorch code is available on Github (htps://github.com/li-lab-mcgill/covid19_media).},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {34},
numpages = {14},
keywords = {media news, text mining, Topic models, Bayesian inference, coronavirus},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3293882.3338996,
author = {Wang, Cong and Gao, Jian and Jiang, Yu and Xing, Zhenchang and Zhang, Huafeng and Yin, Weiliang and Gu, Ming and Sun, Jiaguang},
title = {Go-Clone: Graph-Embedding Based Clone Detector for Golang},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338996},
doi = {10.1145/3293882.3338996},
abstract = {Golang (short for Go programming language) is a fast and compiled language, which has been increasingly used in industry due to its excellent performance on concurrent programming. Golang redefines concurrent programming grammar, making it a challenge for traditional clone detection tools and techniques. However, there exist few tools for detecting duplicates or copy-paste related bugs in Golang. Therefore, an effective and efficient code clone detector on Golang is especially needed.  In this paper, we present Go-Clone, a learning-based clone detector for Golang. Go-Clone contains two modules -- the training module and the user interaction module. In the training module, firstly we parse Golang source code into llvm IR (Intermediate Representation). Secondly, we calculate LSFG (labeled semantic flow graph) for each program function automatically. Go-Clone trains a deep neural network model to encode LSFGs for similarity classification. In the user interaction module, users can choose one or more Golang projects. Go-Clone identifies and presents a list of function pairs, which are most likely clone code for user inspection. To evaluate Go-Clone's performance, we collect 6,110 commit versions from 48 Github projects to construct a Golang clone detection data set. Go-Clone can reach the value of AUC (Area Under Curve) and ACC (Accuracy) for 89.61% and 83.80% in clone detection. By testing several groups of unfamiliar data, we also demonstrates the generility of Go-Clone. The address of the abstract demo video: https://youtu.be/o5DogtYGbeo},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {374–377},
numpages = {4},
keywords = {deep neural network, code clone detection, go programming language, code similarity},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1109/MSR.2019.00072,
author = {De Bleser, Jonas and Di Nucci, Dario and De Roover, Coen},
title = {Assessing Diffusion and Perception of Test Smells in Scala Projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00072},
doi = {10.1109/MSR.2019.00072},
abstract = {Test smells are, analogously to code smells, defined as the characteristics exhibited by poorly designed unit tests. Their negative impact on test effectiveness, understanding, and maintenance has been demonstrated by several empirical studies.However, the scope of these studies has been limited mostly to JAVA in combination with the JUNIT testing framework. Results for other language and framework combinations are ---despite their prevalence in practice--- few and far between, which might skew our understanding of test smells. The combination of Scala and ScalaTest, for instance, offers more comprehensive means for defining and reusing test fixtures, thereby possibly reducing the diffusion and perception of fixture-related test smells.This paper therefore reports on two empirical studies conducted for this combination. In the first study, we analyse the tests of 164 open-source Scala projects hosted on GitHub for the diffusion of test smells. This required the transposition of their original definition to this new context, and the implementation of a tool (SoCRATES) for their automated detection. In the second study, we assess the perception and the ability of 14 Scala developers to identify test smells. For this context, our results show (i) that test smells have a low diffusion across test classes, (ii) that the most frequently occurring test smells are Lazy Test, Eager Test, and Assertion Roulette, and (iii) that many developers were able to perceive but not to identify the smells.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {457–467},
numpages = {11},
keywords = {scala language, test quality, test smells},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1109/QSIC.2014.19,
author = {Mostafa, Shaikh and Wang, Xiaoyin},
title = {An Empirical Study on the Usage of Mocking Frameworks in Software Testing},
year = {2014},
isbn = {9781479971985},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QSIC.2014.19},
doi = {10.1109/QSIC.2014.19},
abstract = {In software testing, especially unit testing, it is very common that software testers need to test a class or a component without integration with some of its dependencies. Typical reasons for excluding dependencies in testing include the unavailability of some dependency due to concurrent software development and callbacks in frameworks, high cost of invoking some dependencies (e.g., slow network or database operations, commercial third-party web services), and the potential interference of bugs in the dependencies. In practice, mock objects have been used in software testing to simulate such missing dependencies, and a number of popular mocking frameworks (e.g., Mockito, EasyMock) have been developed for software testers to generate mock objects more conveniently. However, despite the wide usage of mocking frameworks in software practice, there have been very few academic studies to observe and understand the usage status of mocking frameworks, and the major issues software testers are facing when using such mocking frameworks. In this paper, we report on an empirical study on the usage of four most popular mock frameworks (Mockito, EasyMock, JMock, and JMockit) in 5,000 open source software projects from GitHub. The results of our study show that the above mentioned mocking frameworks are used in a large portion (about 23%) of software projects that have test code. We also find that software testers typically create mocks for only part of the software dependencies, and there are more mocking of source code classes than library classes.},
booktitle = {Proceedings of the 2014 14th International Conference on Quality Software},
pages = {127–132},
numpages = {6},
keywords = {Testing, Mocking Frameworks},
series = {QSIC '14}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised Methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the Tenth International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Community Structure Detection, Software Engineering, Software Defect Prediction, Unsupervised Learning, Machine Learning},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT 2019}
}

@inproceedings{10.1145/3275219.3275226,
author = {Yan, Jiafei and Sun, Hailong and Wang, Xu and Liu, Xudong and Song, Xiaotao},
title = {Profiling Developer Expertise across Software Communities with Heterogeneous Information Network Analysis},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275226},
doi = {10.1145/3275219.3275226},
abstract = {Knowing developer expertise is critical for achieving effective task allocation. However, it is of great challenge to accurately profile the expertise of developers over the Internet as their activities often disperse across different online communities. In this regard, the existing works either merely concern a single community, or simply sum up the expertise in individual communities. The former suffers from low accuracy due to incomplete data, while the latter impractically assumes that developer expertise is completely independent and irrelavant across communities. To overcome those limitations, we propose a new approach to profile developer expertise across software communities through heterogeneous information network (HIN) analysis. A HIN is first built by analyzing the developer activities in various communities, where nodes represent objects like developers and skills, and edges represent the relations among objects. Second, as random walk with restart (RWR) is known for its ability to capture the global structure of the whole network, we adopt RWR over the HIN to estimate the proximity of developer nodes and skill nodes, which essentially reflects developer expertise. Based on the data of 72,645 common users of GitHub and Stack Overflow, we conducted an empirical study and evaluated developer expertise using proposed approach. To evaluate the effect of our approach, we use the obtained expertise to estimate the competency of developers in answering the questions posted in Stack Overflow. The experimental results demonstrate the superiority of our approach over existing methods.},
booktitle = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
articleno = {2},
numpages = {9},
keywords = {random walk with restart, Developer expertise, heterogeneous information network},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3377811.3380429,
author = {Watson, Cody and Tufano, Michele and Moran, Kevin and Bavota, Gabriele and Poshyvanyk, Denys},
title = {On Learning Meaningful Assert Statements for Unit Test Cases},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380429},
doi = {10.1145/3377811.3380429},
abstract = {Software testing is an essential part of the software lifecycle and requires a substantial amount of time and effort. It has been estimated that software developers spend close to 50% of their time on testing the code they write. For these reasons, a long standing goal within the research community is to (partially) automate software testing. While several techniques and tools have been proposed to automatically generate test methods, recent work has criticized the quality and usefulness of the assert statements they generate. Therefore, we employ a Neural Machine Translation (NMT) based approach called Atlas (<u>A</u>u<u>T</u>omatic <u>L</u>earning of <u>A</u>ssert <u>S</u>tatements) to automatically generate meaningful assert statements for test methods. Given a test method and a focal method (i.e., the main method under test), Atlas can predict a meaningful assert statement to assess the correctness of the focal method. We applied Atlas to thousands of test methods from GitHub projects and it was able to predict the exact assert statement manually written by developers in 31% of the cases when only considering the top-1 predicted assert. When considering the top-5 predicted assert statements, Atlas is able to predict exact matches in 50% of the cases. These promising results hint to the potential usefulness of our approach as (i) a complement to automatic test case generation techniques, and (ii) a code completion support for developers, who can benefit from the recommended assert statements while writing test code.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1398–1409},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3236024.3236053,
author = {Chen, Junjie and Lou, Yiling and Zhang, Lingming and Zhou, Jianyi and Wang, Xiaoleng and Hao, Dan and Zhang, Lu},
title = {Optimizing Test Prioritization via Test Distribution Analysis},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236053},
doi = {10.1145/3236024.3236053},
abstract = {Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice.  To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming state-of-the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks from the testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {656–667},
numpages = {12},
keywords = {Test Prioritization, Regression Testing, Machine Learning},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3184558.3191543,
author = {Zaveri, Amrapali and Serrano, Pedro Hernandez and Desai, Manisha and Dumontier, Michel},
title = {CrowdED: Guideline for Optimal Crowdsourcing Experimental Design},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191543},
doi = {10.1145/3184558.3191543},
abstract = {Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1109–1116},
numpages = {8},
keywords = {data science, data quality, metadata, crowdsourcing, biomedical, fair, reproducibility},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1145/3092703.3092731,
author = {Zhang, Mengshi and Li, Xia and Zhang, Lingming and Khurshid, Sarfraz},
title = {Boosting Spectrum-Based Fault Localization Using PageRank},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092731},
doi = {10.1145/3092703.3092731},
abstract = { Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.  We present PRFL, a lightweight technique that boosts spectrum-based fault localization by differentiating tests using PageRank algorithm. Given the original program spectrum information, PRFL uses PageRank to recompute the spectrum information by considering the contributions of different tests. Then, traditional SBFL techniques can be applied on the recomputed spectrum information to achieve more effective fault localization. Although simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 42% more real faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 2 minute average extra overhead on real faults) on 357 real faults from 5 Defects4J projects and 30692 artificial (i.e., mutation) faults from 87 GitHub projects, demonstrating a promising future for considering the contributions of different tests during fault localization. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–272},
numpages = {12},
keywords = {Spectrum-based fault localization, Software testing, PageRank},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1109/ICPC.2017.41,
author = {Malaquias, Romero and Ribeiro, M\'{a}rcio and Bonif\'{a}cio, Rodrigo and Monteiro, Eduardo and Medeiros, Fl\'{a}vio and Garcia, Alessandro and Gheyi, Rohit},
title = {The Discipline of Preprocessor-Based Annotations Does #ifdef TAG n't #endif Matter},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.41},
doi = {10.1109/ICPC.2017.41},
abstract = {The C preprocessor is a simple, effective, and language-independent tool. Developers use the preprocessor in practice to deal with portability and variability issues. Despite the widespread usage, the C preprocessor suffers from severe criticism, such as negative effects on code understandability and maintainability. In particular, these problems may get worse when using undisciplined annotations, i.e., when a preprocessor directive encompasses only parts of C syntactical units. Nevertheless, despite the criticism and guidelines found in systems like Linux to avoid undisciplined annotations, the results of a previous controlled experiment indicated that the discipline of annotations has no influence on program comprehension and maintenance. To better understand whether developers care about the discipline of preprocessor-based annotations and whether they can really influence on maintenance tasks, in this paper we conduct a mixed-method research involving two studies. In the first one, we identify undisciplined annotations in 110 open-source C/C++ systems of different domains, sizes, and popularity GitHub metrics. We then refactor the identified undisciplined annotations to make them disciplined. Right away, we submit pull requests with our code changes. Our results show that almost two thirds of our pull requests have been accepted and are now merged. In the second study, we conduct a controlled experiment. We have several differences with respect to the aforementioned one, such as blocking of cofounding effects and more replicas. We have evidences that maintaining undisciplined annotations is more time consuming and error prone, representing a different result when compared to the previous experiment. Overall, we conclude that undisciplined annotations should not be neglected.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {297–307},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3052973.3052982,
author = {Felsch, Dennis and Mainka, Christian and Mladenov, Vladislav and Schwenk, J\"{o}rg},
title = {SECRET: On the Feasibility of a Secure, Efficient, and Collaborative Real-Time Web Editor},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3052982},
doi = {10.1145/3052973.3052982},
abstract = {Real-time editing tools like Google Docs, Microsoft Office Online, or Etherpad have changed the way of collaboration. Many of these tools are based on Operational Transforms (OT), which guarantee that the views of different clients onto a document remain consistent over time. Usually, documents and operations are exposed to the server in plaintext -- and thus to administrators, governments, and potentially cyber criminals. Therefore, it is highly desirable to work collaboratively on encrypted documents. Previous implementations do not unleash the full potential of this idea: They either require large storage, network, and computation overhead, are not real-time collaborative, or do not take the structure of the document into account. The latter simplifies the approach since only OT algorithms for byte sequences are required, but the resulting ciphertexts are almost four times the size of the corresponding plaintexts.We present SECRET, the first secure, efficient, and collaborative real-time editor. In contrast to all previous works, SECRET is the first tool that (1.) allows the encryption of whole documents or arbitrary sub-parts thereof, (2.) uses a novel combination of tree-based OT with a structure preserving encryption, and (3.) requires only a modern browser without any extra software installation or browser extension.We evaluate our implementation and show that its encryption overhead is three times smaller in comparison to all previous approaches. SECRET can even be used by multiple users in a low-bandwidth scenario. The source code of SECRET is published on GitHub as an open-source project:https://github.com/RUB-NDS/SECRET/},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {835–848},
numpages = {14},
keywords = {structure preserving encryption, XML encryption, JSON, collaborative editing, operational transforms},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.5555/2818754.2818807,
author = {Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei},
title = {Learning to Log: Helping Developers Make Informed Logging Decisions},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {415–425},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3134600.3134612,
author = {Nemec, Matus and Klinec, Dusan and Svenda, Petr and Sekan, Peter and Matyas, Vashek},
title = {Measuring Popularity of Cryptographic Libraries in Internet-Wide Scans},
year = {2017},
isbn = {9781450353458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134600.3134612},
doi = {10.1145/3134600.3134612},
abstract = {We measure the popularity of cryptographic libraries in large datasets of RSA public keys. We do so by improving a recently proposed method based on biases introduced by alternative implementations of prime selection in different cryptographic libraries. We extend the previous work by applying statistical inference to approximate a share of libraries matching an observed distribution of RSA keys in an inspected dataset (e.g., Internet-wide scan of TLS handshakes). The sensitivity of our method is sufficient to detect transient events such as a periodic insertion of keys from a specific library into Certificate Transparency logs and inconsistencies in archived datasets.We apply the method on keys from multiple Internet-wide scans collected in years 2010 through 2017, on Certificate Transparency logs and on separate datasets for PGP keys and SSH keys. The results quantify a strong dominance of OpenSSL with more than 84% TLS keys for Alexa 1M domains, steadily increasing since the first measurement. OpenSSL is even more popular for GitHub client-side SSH keys, with a share larger than 96%. Surprisingly, new certificates inserted in Certificate Transparency logs on certain days contain more than 20% keys most likely originating from Java libraries, while TLS scans contain less than 5% of such keys.Since the ground truth is not known, we compared our measurements with other estimates and simulated different scenarios to evaluate the accuracy of our method. To our best knowledge, this is the first accurate measurement of the popularity of cryptographic libraries not based on proxy information like web server fingerprinting, but directly on the number of observed unique keys.},
booktitle = {Proceedings of the 33rd Annual Computer Security Applications Conference},
pages = {162–175},
numpages = {14},
keywords = {RSA algorithm, cryptographic library, prime generation},
location = {Orlando, FL, USA},
series = {ACSAC 2017}
}

@inproceedings{10.1145/3377811.3380335,
author = {Mirsaeedi, Ehsan and Rigby, Peter C.},
title = {Mitigating Turnover with Code Review Recommendation: Balancing Expertise, Workload, and Knowledge Distribution},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380335},
doi = {10.1145/3377811.3380335},
abstract = {Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover with minimal impacton the development process. We evaluate review recommenders in the context of ensuring expertise during review, Expertise, reducing the review workload of the core team, CoreWorkload, and reducing the Files at Risk to turnover, FaR. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing risk of knowledge loss from turnover by up to 65%. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover by -29% but they unacceptably reduce the overall expertise during reviews by -26%. We develop the Sofia recommender that suggests experts when none of the files under review are hoarded by developers, but distributes knowledge when files are at risk. In this way, we are able to simultaneously increase expertise during review with a ΔExpertise of 6%, with a negligible impact on workload of ΔCoreWorkload of 0.09%, and reduce the files at risk by ΔFaR -28%. Sofia is integrated into GitHub pull requests allowing developers to select an appropriate expert or "learner" based on the context of the review. We release the Sofia bot as well as the code and data for replication purposes.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1183–1195},
numpages = {13},
keywords = {knowledge distribution, code review, tool support, turnover, recommenders},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/3398761.3398866,
author = {Muri\'{c}, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\'{e}s and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio},
title = {Massive Cross-Platform Simulations of Online Social Networks},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {895–903},
numpages = {9},
keywords = {massive scale simulations, online social networks, agent based simulation, ai agents, collaborative platforms},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.1109/FIE43999.2019.9028657,
author = {Rahman, Md Mahmudur and Paudel, Roshan and Sharker, Monir H},
title = {Effects of Infusing Interactive and Collaborative Learning to Teach an Introductory Programming Course},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE43999.2019.9028657},
doi = {10.1109/FIE43999.2019.9028657},
abstract = {This Innovate Practice Full Paper presents positive effects in teaching an introductory programming course in Python by infusing both interactive and collaborative learning. For a dynamic classroom, we used an interactive computer programming environment, Repl.it, as a top-level shell and created several in-class exercises, assignments, small lab-based projects. In addition, we used an eBook, which offers an animation and software visualization tool where students can step through code line-by-line and a program editing and execution area where students can execute examples, change them, and execute the updated code. We also introduced collaborative learning at the beginning of this introductory programming course in the form of doing team projects submitted at the end of the semester. The students were instructed to commit code to GitHub which ensures that their work will not be lost as well as, provide them basic task management tools to collaborate. The proposed pedagogical approaches were applied in the Fall’2017 semester to teach an introductory CS course in Python. The traditional course instruction that has historically been used in the department are used as the control group. For evaluation and result analysis, thirteen sections of COSC 111 were included in this study over three semesters: Fall 2014, Fall 2016 and Fall 2017. The initial evaluation of summative assessment and analysis of the survey results enable us to conclude that the proposed instructional approach increased student motivation and engagement, facilitated learning, and contributed to the progress of students in this course as well as reduced the failure rates.},
booktitle = {2019 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Covington, KY, USA}
}

@inproceedings{10.1145/3357384.3358043,
author = {Shrestha, Prasha and Maharjan, Suraj and Arendt, Dustin and Volkova, Svitlana},
title = {Learning from Dynamic User Interaction Graphs to Forecast Diverse Social Behavior},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3358043},
doi = {10.1145/3357384.3358043},
abstract = {Most of the existing graph analytics for understanding social behavior focuses on learning from static rather than dynamic graphs using hand-crafted network features or recently emerged graph embeddings learned independently from a downstream predictive task, and solving predictive (e.g., link prediction) rather than forecasting tasks directly. To address these limitations, we propose (1) a novel task -- forecasting user interactions over dynamic social graphs, and (2) a novel deep learning, multi-task, node-aware attention model that focuses on forecasting social interactions, going beyond recently emerged approaches for learning dynamic graph embeddings. Our model relies on graph convolutions and recurrent layers to forecast future social behavior and interaction patterns in dynamic social graphs. We evaluate our model on the ability to forecast the number of retweets and mentions of a specific news source on Twitter (focusing on deceptive and credible news sources) with R^2 of 0.79 for retweets and 0.81 for mentions. An additional evaluation includes model forecasts of user-repository interactions on GitHub and comments to a specific video on YouTube with a mean absolute error close to 2% and R^2 exceeding 0.69. Our results demonstrate that learning from connectivity information over time in combination with node embeddings yields better forecasting results than when we incorporate the state-of-the-art graph embeddings e.g., Node2Vec and DeepWalk into our model. Finally, we perform in-depth analyses to examine factors that influence model performance across tasks and different graph types e.g., the influence of training and forecasting windows as well as graph topological properties.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {2033–2042},
numpages = {10},
keywords = {node-aware attention, social activity forecasting, attention, dynamic graphs},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3357766.3359546,
author = {Khelladi, Djamel Eddine and Kretschmer, Roland and Egyed, Alexander},
title = {Detecting and Exploring Side Effects When Repairing Model Inconsistencies},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359546},
doi = {10.1145/3357766.3359546},
abstract = {When software models change, developers often fail in keeping them consistent. Automated support in repairing inconsistencies is widely addressed. Yet, merely enumerating repairs for developers is not enough. A repair can as a side effect cause new unexpected inconsistencies (negative) or even fix other inconsistencies as well (positive). To make matters worse, repairing negative side effects can in turn cause further side effects. Current approaches do not detect and track such side effects in depth, which can increase developers' effort and time spent in repairing inconsistencies. This paper presents an automated approach for detecting and tracking the consequences of repairs, i.e. side effects. It recursively explores in depth positive and negative side effects and identifies paths and cycles of repairs. This paper further ranks repairs based on side effect knowledge so that developers may quickly find the relevant ones. Our approach and its tool implementation have been empirically assessed on 14 case studies from industry, academia, and GitHub. Results show that both positive and negative side effects occur frequently. A comparison with three versioned models showed the usefulness of our ranking strategy based on side effects. It showed that our approach's top prioritized repairs are those that developers would indeed choose. A controlled experiment with 24 participants further highlights the significant influence of side effects and of our ranking of repairs on developers. Developers who received side effect knowledge chose far more repairs with positive side effects and far less with negative side effects, while being 12.3% faster, in contrast to developers who did not receive side effect knowledge.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {113–126},
numpages = {14},
keywords = {Side effects, Repairs, Model Inconsistencies, Consequences},
location = {Athens, Greece},
series = {SLE 2019}
}

@inproceedings{10.1145/3338906.3338977,
author = {Chen, Zhenpeng and Cao, Yanbin and Lu, Xuan and Mei, Qiaozhu and Liu, Xuanzhe},
title = {SEntiMoji: An Emoji-Powered Learning Approach for Sentiment Analysis in Software Engineering},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338977},
doi = {10.1145/3338906.3338977},
abstract = {Sentiment analysis has various application scenarios in software engineering (SE), such as detecting developers' emotions in commit messages and identifying their opinions on Q&amp;A forums. However, commonly used out-of-the-box sentiment analysis tools cannot obtain reliable results on SE tasks and the misunderstanding of technical jargon is demonstrated to be the main reason. Then, researchers have to utilize labeled SE-related texts to customize sentiment analysis for SE tasks via a variety of algorithms. However, the scarce labeled data can cover only very limited expressions and thus cannot guarantee the analysis quality. To address such a problem, we turn to the easily available emoji usage data for help. More specifically, we employ emotional emojis as noisy labels of sentiments and propose a representation learning approach that uses both Tweets and GitHub posts containing emojis to learn sentiment-aware representations for SE-related texts. These emoji-labeled posts can not only supply the technical jargon, but also incorporate more general sentiment patterns shared across domains. They as well as labeled data are used to learn the final sentiment classifier. Compared to the existing sentiment analysis methods used in SE, the proposed approach can achieve significant improvement on representative benchmark datasets. By further contrast experiments, we find that the Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource, but try to transform knowledge from the open domain through ubiquitous signals such as emojis.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {841–852},
numpages = {12},
keywords = {Emoji, Sentiment analysis, Software engineering},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1109/MSR.2019.00073,
author = {Markovtsev, Vadim and Long, Waren and Mougard, Hugo and Slavnov, Konstantin and Bulychev, Egor},
title = {STYLE-ANALYZER: Fixing Code Style Inconsistencies with Interpretable Unsupervised Algorithms},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00073},
doi = {10.1109/MSR.2019.00073},
abstract = {Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes.This paper introduces style-analyzer, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. style-analyzer is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. style-analyzer can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of style-analyzer by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. style-analyzer includes a web application to visualize how the rules are triggered. We release style-analyzer as a reusable and extendable open source software package on GitHub for the benefit of the community.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {468–478},
numpages = {11},
keywords = {interpretable machine learning, assisted code review, decision tree forest, code style},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3180155.3180254,
author = {Wang, Kaiyuan and Zhu, Chenguang and Celik, Ahmet and Kim, Jongwook and Batory, Don and Gligoric, Milos},
title = {Towards Refactoring-Aware Regression Test Selection},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180254},
doi = {10.1145/3180155.3180254},
abstract = {Regression testing checks that recent project changes do not break previously working functionality. Although important, regression testing is costly when changes are frequent. Regression test selection (RTS) optimizes regression testing by running only tests whose results might be affected by a change. Traditionally, RTS collects dependencies (e.g., on files) for each test and skips the tests, at a new project revision, whose dependencies did not change. Existing RTS techniques do not differentiate behavior-preserving transformations (i.e., refactorings) from other code changes. As a result, tests are run more frequently than necessary.We present the first step towards a refactoring-aware RTS technique, dubbed Reks, which skips tests affected only by behavior-preserving changes. Reks defines rules to update the test dependencies without running the tests. To ensure that Reks does not hide any bug introduced by the refactoring engines, we integrate Reks only in the pre-submit testing phase, which happens on the developers' machines. We evaluate Reks by measuring the savings in the testing effort. Specifically, we reproduce 100 refactoring tasks performed by developers of 37 projects on GitHub. Our results show that Reks would not run, on average, 33% of available tests (that would be run by a refactoring-unaware RTS technique). Additionally, we systematically run 27 refactoring types on ten projects. The results, based on 74,160 refactoring tasks, show that Reks would not run, on average, 16% of tests (max: 97% and SD: 24%). Finally, our results show that the Reks update rules are efficient.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {233–244},
numpages = {12},
keywords = {Reks, behavior-preserving changes, regression test selection},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2339530.2339690,
author = {Majumder, Anirban and Datta, Samik and Naidu, K.V.M.},
title = {Capacitated Team Formation Problem on Social Networks},
year = {2012},
isbn = {9781450314626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2339530.2339690},
doi = {10.1145/2339530.2339690},
abstract = {In a team formation problem, one is required to find a group of users that can match the requirements of a collaborative task. Example of such collaborative tasks abound, ranging from software product development to various participatory sensing tasks in knowledge creation. Due to the nature of the task, team members are often required to work on a co-operative basis. Previous studies [1, 2] have indicated that co-operation becomes effective in presence of social connections. Therefore, effective team selection requires the team members to be socially close as well as a division of the task among team members so that no user is overloaded by the assignment. In this work, we investigate how such teams can be formed on a social network.Since our team formation problems are proven to be NP-hard, we design efficient approximate algorithms for finding near optimum teams with provable guarantees. As traditional data-sets from on-line social networks (e.g. Twitter, Facebook etc) typically do not contain instances of large scale collaboration, we have crawled millions of software repositories spanning a period of four years and hundreds of thousands of developers from GitHub, a popular open-source social coding network. We perform large scale experiments on this data-set to evaluate the accuracy and efficiency of our algorithms. Experimental results suggest that our algorithms achieve significant improvement in finding effective teams, as compared to naive strategies and scale well with the size of the data. Finally, we provide a validation of our techniques by comparing with existing software teams.},
booktitle = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1005–1013},
numpages = {9},
keywords = {capacity, team formation, social networks},
location = {Beijing, China},
series = {KDD '12}
}

@inproceedings{10.1145/3180155.3182513,
author = {Sirres, Raphael and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Lo, David and Klein, Jacques and Kim, Kisub and Traon, Yves Le},
title = {Augmenting and Structuring User Queries to Support Efficient Free-Form Code Search},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182513},
doi = {10.1145/3180155.3182513},
abstract = {Motivation: Code search is an important activity in software development since developers are regularly searching [6] for code examples dealing with diverse programming concepts, APIs, and specific platform peculiarities. To help developers search for source code, several Internet-scale code search engines, such as OpenHub [5] and Codota [1] have been proposed. Unfortunately, these Internet-scale code search engines have limited performance since they treat source code as natural language documents. To improve the performance of search engines, the construction of the search space index as well as the mapping process of querying must address the challenge that "no single word can be chosen to describe a programming concept in the best way" [2]. This is known in the literature as the vocabulary mismatch problem [3].Approach: We propose a novel approach to augmenting user queries in a free-form code search scenario. This approach aims at improving the quality of code examples returned by Internet-scale code search engines by building a Code voCaBulary (CoCaBu) [7]. The originality of CoCaBu is that it addresses the vocabulary mismatch problem, by expanding/enriching/re-targeting a user's free-form query, building on similar questions in Q&amp;A sites so that a code search engine can find highly relevant code in source code repositories. Figure 1 provides an overview of our approach.The search process begins with a free-form query from a user,i.e., a sentence written in a natural language:(a) For a given query, CoCaBu first searches for relevant posts in Q&amp;A forums. The role of the Search Proxy is then to forward developer free-form queries to web search engines that can collect and rank entries in Q&amp;A with the most relevant documents for the query.(b) CoCaBu then generates an augmented query based on the information in the relevant posts. It mainly leverages code snippets in the previously identified posts. The Code Query Generator then creates another query which includes not only the initial user query terms but also program elements. To accelerate this step in the search process, CoCaBu builds upfront a snippet index for Q&amp;A posts.(c) Once the augmented query is constructed, CoCaBu searches source files for code locations that match the query terms. For this step, we crawl a large number of repositories and build upfront a code index of program elements in the source code.Contributions:• CoCaBu approach to the vocabulary mismatch problem: We propose a technique for finding relevant code with freeform query terms that describe programming tasks, with no a-priori knowledge on the API keywords to search for.• GitSearch free-form search engine for GitHub: We instantiate the CoCaBu approach based on indices of Java files built from GitHub and Q&amp;A posts from Stack Overflow to find the most relevant code examples for developer queries.• Empirical user evaluation: Comparison with popular code search engines further shows that GitSearch is more effective in returning acceptable code search results. In addition, Comparison against web search engines indicates that GitSearch is a competitive alternative. Finally, via a live study, we show that users on Q&amp;A sites may find GitSearch's real code examples acceptable as answers to developer questions.Concluding remarks: As a follow-up work, we have also leveraged Stack Overflow data to build a practical, novel, and efficient code-to-code search engine [4].},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {945},
numpages = {1},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.5555/3172795.3172813,
author = {Dantas, Carlos E. C. and de A. Maia, Marcelo},
title = {On the Actual Use of Inheritance and Interface in Java Projects: Evolution and Implications},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Background: Inheritance is one of the main features in the object-oriented paradigm (OOP). Nonetheless, previous work recommend carefully using it, suggesting alternatives such as the adoption of composition with implementation of interfaces. Despite of being a well-studied theme, there is still little knowledge if such recommendations have been widely adopted by developers in general. Aims: This work aims at evaluating how the inheritance and composition with interfaces have been used in Java, comparing new projects with older ones (transversal), and also the different releases of the same projects (longitudinal). Method: A total of 1, 656 open-source projects built between 1997 and 2013, hosted in the repositories GitHub and SourceForge, were analyzed. The likelihood of more recent projects using inheritance and interfaces differently from older ones was analyzed considering indicators, such as, the prevalence of corrective changes, instanceof operations, and code smells. Regression analysis, chi-squared test of proportions and descriptive statistics were used to analyze the data. In addition, a thematic analysis based method was used to verify how often and why inheritance and interface are added or removed from classes. Results: We observed that developers still use inheritance primarily for code reuse, motivated by the need to avoid duplicity of source code. In newer projects, classes in inheritance had fewer corrective changes and subclasses had fewer use of the instance of operator. However, as they evolve, classes in inheritance tend to become complex as changes occur. Classes implementing interfaces have shown little relation to the interfaces, and there is indication that interfaces are still underutilized. Conclusion: These results show there is still some lack of knowledge about the use of recommended object-oriented practices, suggesting the need of training developers on how to design better classes.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {151–160},
numpages = {10},
keywords = {sourceforge, interfaces, inheritance, cohesion, GitHub, code smells},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3236024.3275535,
author = {Foo, Darius and Chua, Hendy and Yeo, Jason and Ang, Ming Yi and Sharma, Asankhaya},
title = {Efficient Static Checking of Library Updates},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275535},
doi = {10.1145/3236024.3275535},
abstract = {Software engineering practices have evolved to the point where a developer writing a new application today doesn’t start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {791–796},
numpages = {6},
keywords = {call graphs, semantic versioning, automated remediation, api diffs, library upgrades},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3159450.3162206,
author = {Cutler, Barbara and Peveler, Matthew and Breese, Samuel and Maicus, Evan and Milanova, Ana and Holzbauer, Buster and Aikens, Andrew and Anderson, James and Barthelmess, Josh and Cyrus, Timothy and Lee, Marisa and Montealegre, Leon and Wang, Jessica},
title = {Supporting Team Submissions and Peer Grading within Submitty: (Abstract Only)},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3162206},
doi = {10.1145/3159450.3162206},
abstract = {Submitty is an open source programming assignment submission system from the Rensselaer Center for Open Source Software (RCOS) at Rensselaer Polytechnic Institute (RPI) accessed via an online interface. Submitty allows students to submit their code through file upload or version control, such as an internal Git/SVN server or Github, where it is then tested with a highly configurable and customizable automated grader. For each assignment, instructors can specify whether or not students can work in teams. For team assignments, the instructor can either assign teammates or allow the students to choose. In addition to the auto-grading for submissions, Submitty supports human grading. The human graded rubric is developed by the graders as they work, allowing reuse of common feedback messages and partial credit points. The rubric can be searched and modified during and after grading is complete for consistency. By default, grading is handled by instructors and TAs who are assigned to sections of students, which can be rotated through the semester. However, an instructor can choose to incorporate peer grading, which will allow students to anonymously view and submit grades for each other, receiving multiple peer grades per assignment. Submitty has been used at RPI for several years for a variety of courses, serving over 1500 students and 50 instructors and TAs each semester, and has recently been used by several other universities. We will present "case studies" of assignment configurations for autograding and manual grading and demonstrate the grading interface in support of team submissions and peer grading.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {1111},
numpages = {1},
keywords = {peer grading, team assignments, version control, autograding, education},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1109/ICPC.2017.3,
author = {Mostafa, Shaikh and Rodriguez, Rodney and Wang, Xiaoyin},
title = {NetDroid: Summarizing Network Behavior of Android Apps for Network Code Maintenance},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.3},
doi = {10.1109/ICPC.2017.3},
abstract = {Network access is one of the most common features of Android applications. Statistics show that almost 80% of Android apps ask for network permission and thus may have some network-related features. Android apps may access multiple servers to retrieve or post various types of data, and the code to handle such network features often needs to change as a result of server API evolution or the content change of data transferred. Since various network code is used by multiple features, maintenance of network-related code is often difficult because the code may scatter in different places in the code base, and it may not be easy to predict the impact of a code change to the network behavior of an Android app. In this paper, we present an approach to statically summarize network behavior from the byte code of Android apps. Our approach is based on string taint analysis, and generates a summary of network requests by statically estimating the possible values of network API arguments. To evaluate our technique, we applied our technique to top 500 android apps from the official Google Play market, and the result shows that our approach is able to summarize network behavior for most apps efficiently (averagely less than 50 second for an app). Furthermore, we performed an empirical evaluation on 8 real-world maintenance tasks extracted from bug reports of open-source Android projects on Github. The empirical evaluation shows that our technique is effective in locating relevant network code.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {165–175},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1109/ESEM.2017.22,
author = {Falessi, Davide and Smith, Wyatt and Serebrenik, Alexander},
title = {STRESS: A Semi-Automated, Fully Replicabile Approach for Project Selection},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.22},
doi = {10.1109/ESEM.2017.22},
abstract = {The mining of software repositories has provided significant advances in a multitude of software engineering fields, including defect prediction. Several studies show that the performance of a software engineering technology (e.g., prediction model) differs across different project repositories. Thus, it is important that the project selection is replicable. The aim of this paper is to present STRESS, a semi-automated and fully replicable approach that allows researchers to select projects by configuring the desired level of diversity, fit, and quality. STRESS records the rationale behind the researcher decisions and allows different users to re-run or modify such decisions. STRESS is open-source and it can be used used locally or even online (www.falessi.com/STRESS/). We perform a systematic mapping study that considers studies that analyzed projects managed with JIRA and Git to asses the project selection replicability of past studies. We validate the feasible application of STRESS in realistic research scenarios by applying STRESS to select projects among the 211 Apache Software Foundation projects. Our systematic mapping study results show that none of the 68 analyzed studies is completely replicable. Regarding STRESS, it successfully supported the project selection among all 211 ASF projects. It also supported the measurement of 100 projects characteristics, including the 32 criteria of the studies analyzed in our mapping study. The mapping study and STRESS are, to our best knowledge, the first attempt to investigate and support the replicability of project selection. We plan to extend them to other technologies such as GitHub.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {151–156},
numpages = {6},
keywords = {mining software repositories, replication, apache},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3107411.3107414,
author = {Yao, Yao and Liu, Zheng and Singh, Satpreet and Wei, Qi and Ramsey, Stephen A.},
title = {CERENKOV: Computational Elucidation of the Regulatory Noncoding Variome},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107414},
doi = {10.1145/3107411.3107414},
abstract = {We describe a novel computational approach, CERENKOV (Computational Elucidation of the REgulatory NonKOding Variome), for discriminating regulatory single nucleotide polymorphisms (rSNPs) from non-regulatory SNPs within noncoding genetic loci. CERENKOV is specifically designed for recognizing rSNPs in the context of a post-analysis of a genome-wide association study (GWAS); it includes a novel accuracy scoring metric (which we call average rank, or AVGRANK) and a novel cross-validation strategy (locus-based sampling) that both correctly account for the "sparse positive bag" nature of the GWAS post-analysis rSNP recognition problem. We trained and validated CERENKOV using a reference set of 15,331 SNPs (the OSU17 SNP set) whose composition is based on selection criteria (linkage disequilibrium and minor allele frequency) that we designed to ensure relevance to GWAS post-analysis. CERENKOV is based on a machine-learning algorithm (gradient boosted decision trees) incorporating 246 SNP annotation features that we extracted from genomic, epigenomic, phylogenetic, and chromatin datasets. CERENKOV includes novel features based on replication timing and DNA shape. We found that tuning a classifier for AUPVR performance does not guarantee optimality for AVGRANK. We compared the validation performance of CERENKOV to nine other methods for rSNP recognition (including GWAVA, RSVP, DeltaSVM, DeepSEA, Eigen, and DANQ), and found that CERENKOV's validation performance is the strongest out of all of the classifiers that we tested, by both traditional global rank-based measures (〈AUPVR〉 = 0.506; 〈AUROC〉 = 0.855) and AVGRANK (〈AVGRANK〉 = 3.877). The source code for CERENKOV is available on GitHub and the SNP feature data files are available for download via the CERENKOV website.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {79–88},
numpages = {10},
keywords = {SNV, noncoding, machine learning, GWAS, RSNP, SNP},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1109/ICSE.2017.28,
author = {Liu, Han and Sun, Chengnian and Su, Zhendong and Jiang, Yu and Gu, Ming and Sun, Jiaguang},
title = {Stochastic Optimization of Program Obfuscation},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.28},
doi = {10.1109/ICSE.2017.28},
abstract = {Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques.This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = 〈t2, t2, ... , tn〉 (∀i ε [1, n]. ti ε T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq.We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {221–231},
numpages = {11},
keywords = {program obfuscation, obscurity language model, markov chain monte carlo methods},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/2950290.2950324,
author = {Ahmed, Iftekhar and Gopinath, Rahul and Brindescu, Caius and Groce, Alex and Jensen, Carlos},
title = {Can Testedness Be Effectively Measured?},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950324},
doi = {10.1145/2950290.2950324},
abstract = { Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and deciding when to stop testing. Test the least-tested code, and stop when all code is well-tested, is a reasonable answer. Many measures of "testedness" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite quality. The first measure is statement coverage, the simplest and best-known code coverage measure. The second measure is mutation score, a supposedly more powerful, though expensive, measure.  We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer future bug-fixes than a "poorly tested" element. If not, then it seems likely that we are not effectively measuring testedness. Using a large number of open source Java programs from Github and Apache, we show that both statement coverage and mutation score have only a weak negative correlation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Program elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {547–558},
numpages = {12},
keywords = {coverage criteria, mutation testing, test suite evaluation, statistical analysis},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1109/ASE.2019.00081,
author = {Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem},
title = {RefBot: Intelligent Software Refactoring Bot},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00081},
doi = {10.1109/ASE.2019.00081},
abstract = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost.In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {823–834},
numpages = {12},
keywords = {refactoring, quality, Software bot},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3382494.3410685,
author = {Dey, Tapajit and Mockus, Audris},
title = {Effect of Technical and Social Factors on Pull Request Quality for the NPM Ecosystem},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410685},
doi = {10.1145/3382494.3410685},
abstract = {Background: Pull request (PR) based development, which is a norm for the social coding platforms, entails the challenge of evaluating the contributions of, often unfamiliar, developers from across the open source ecosystem and, conversely, submitting a contribution to a project with unfamiliar maintainers. Previous studies suggest that the decision of accepting or rejecting a PR may be influenced by a diverging set of technical and social factors, but often focus on relatively few projects, do not consider ecosystem-wide measures, or the possible non-monotonic relationships between the predictors and PR acceptance probability. Aim: We aim to shed light on this important decision making process by testing which measures significantly affect the probability of PR acceptance on a significant fraction of a large ecosystem, rank them by their relative importance in predicting PR acceptance, and determine the shape of the functions that map each predictor to PR acceptance. Method: We proposed seven hypotheses regarding which technical and social factors might affect PR acceptance and created 17 measures based on them. Our dataset consisted of 470,925 PRs from 3349 popular NPM packages and 79,128 GitHub users who created those. We tested which of the measures affect PR acceptance and ranked the significant measures by their importance in a predictive model. Results: Our predictive model had and AUC of 0.94, and 15 of the 17 measures were found to matter, including five novel ecosystem-wide measures. Measures describing the number of PRs submitted to a repository and what fraction of those get accepted, and signals about the PR review phase were most significant. We also discovered that only four predictors have a linear influence on the PR acceptance probability while others showed a more complicated response. Conclusion: Our findings should be helpful for PR creators, integrators, as well as tool designers to focus on the important factors affecting PR acceptance.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {11},
numpages = {11},
keywords = {Social Factors, Predictive Model, Pull Request, NPM Packages},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3275219.3275222,
author = {Zheng, Zhiwen and Wang, Liang and Xu, Jingwei and Wu, Tianheng and Wu, Simeng and Tao, Xianping},
title = {Measuring and Predicting the Relevance Ratings between FLOSS Projects Using Topic Features},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275222},
doi = {10.1145/3275219.3275222},
abstract = {Understanding the relevance between the Free/Libra Open Source Software projects is important for developers to perform code and design reuse, discover and develop new features, keep their projects up-to-date, and etc. However, it is challenging to perform relevance ratings between the FLOSS projects mainly because: 1) beyond simple code similarity, there are complex aspects considered when measuring the relevance; and 2) the prohibitive large amount of FLOSS projects available. To address the problem, in this paper, we propose a method to measure and further predict the relevance ratings between FLOSS projects. Our method uses topic features extracted by the LDA topic model to describe the characteristics of a project. By using the topic features, multiple aspects of FLOSS projects such as the application domain, technology used, and programming language are extracted and further used to measure and predict their relevance ratings. Based on the topic features, our method uses matrix factorization to leverage the partially known relevance ratings between the projects to learn the mapping between different topic features to the relevance ratings. Finally, our method combines the topic modeling and matrix factorization technologies to predict the relevance ratings between software projects without human intervention, which is scalable to a large amount of projects. We evaluate the performance of the proposed method by applying our topic extraction and relevance modeling methods using 300 projects from GitHub. The result of topic extraction experiment shows that, for topic modeling, our LDA-based approach achieves the highest hit rate of 98.3% and the highest average accuracy of 29.8%. And the relevance modeling experiment shows that our relevance modeling approach achieves the minimum average predict error of 0.093, suggesting the effectiveness of applying the proposed method on real-world data sets.},
booktitle = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
articleno = {12},
numpages = {10},
keywords = {Matrix Factorization, Topic Modeling, Relevance Rating, FLOSS Projects},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3196398.3196428,
author = {Cassee, Nathan and Pinto, Gustavo and Castor, Fernando and Serebrenik, Alexander},
title = {How Swift Developers Handle Errors},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196428},
doi = {10.1145/3196398.3196428},
abstract = {Swift is a new programming language developed by Apple as a replacement to Objective-C. It features a sophisticated error handling (EH) mechanism that provides the kind of separation of concerns afforded by exception handling mechanisms in other languages, while also including constructs to improve safety and maintainability. However, Swift also inherits a software development culture stemming from Objective-C being the de-facto standard programming language for Apple platforms for the last 15 years. It is, therefore, a priori unclear whether Swift developers embrace the novel EH mechanisms of the programming language or still rely on the old EH culture of Objective-C even working in Swift.In this paper, we study to what extent developers adhere to good practices exemplified by EH guidelines and tutorials, and what are the common bad EH practices particularly relevant to Swift code. Furthermore, we investigate whether perception of these practices differs between novices and experienced Swift developers.To answer these questions we employ a mixed-methods approach and combine 10 semi-structured interviews with Swift developers and quantitative analysis of 78,760 Swift 4 files extracted from 2,733 open-source GitHub repositories. Our findings indicate that there is ample opportunity to improve the way Swift developers use error handling mechanisms. For instance, some recommendations derived in this work are not well spread in the corpus of studied Swift projects. For example, generic catch handlers are common in Swift (even though it is not uncommon for them to share space with their counterparts: non empty catch handlers), custom, developerdefined error types are rare, and developers are mostly reactive when it comes to error handling, using Swift's constructs mostly to handle errors thrown by libraries, instead of throwing and handling application-specific errors.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {292–302},
numpages = {11},
keywords = {language feature usage, error handling, swift},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/ICSE.2019.00076,
author = {Cabral, George G. and Minku, Leandro L. and Shihab, Emad and Mujahid, Suhaib},
title = {Class Imbalance Evolution and Verification Latency in Just-in-Time Software Defect Prediction},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00076},
doi = {10.1109/ICSE.2019.00076},
abstract = {Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that rebuild classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency - the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {666–676},
numpages = {11},
keywords = {concept drift, online learning, ensembles, software defect prediction, verification latency, class imbalance},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3196398.3196445,
author = {Braiek, Houssem Ben and Khomh, Foutse and Adams, Bram},
title = {The Open-Closed Principle of Modern Machine Learning Frameworks},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196445},
doi = {10.1145/3196398.3196445},
abstract = {Recent advances in computing technologies and the availability of huge volumes of data have sparked a new machine learning (ML) revolution, where almost every day a new headline touts the demise of human experts by ML models on some task. Open source software development is rumoured to play a significant role in this revolution, with both academics and large corporations such as Google and Microsoft releasing their ML frameworks under an open source license. This paper takes a step back to examine and understand the role of open source development in modern ML, by examining the growth of the open source ML ecosystem on GitHub, its actors, and the adoption of frameworks over time. By mining LinkedIn and Google Scholar profiles, we also examine driving factors behind this growth (paid vs. voluntary contributors), as well as the major players who promote its democratization (companies vs. communities), and the composition of ML development teams (engineers vs. scientists). According to the technology adoption lifecycle, we find that ML is in between the stages of early adoption and early majority. Furthermore, companies are the main drivers behind open source ML, while the majority of development teams are hybrid teams comprising both engineers and professional scientists. The latter correspond to scientists employed by a company, and by far represent the most active profiles in the development of ML applications, which reflects the importance of a scientific background for the development of ML frameworks to complement coding skills. The large influence of cloud computing companies on the development of open source ML frameworks raises the risk of vendor lock-in. These frameworks, while open source, could be optimized for specific commercial cloud offerings.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {353–363},
numpages = {11},
keywords = {technology adoption, open source, machine learning, framework},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/ESEM.2017.55,
author = {Campos, Eduardo C. and Maia, Marcelo A.},
title = {Common Bug-Fix Patterns: A Large-Scale Observational Study},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.55},
doi = {10.1109/ESEM.2017.55},
abstract = {[Background]: There are more bugs in real-world programs than human programmers can realistically address. Several approaches have been proposed to aid debugging. A recent research direction that has been increasingly gaining interest to address the reduction of costs associated with defect repair is automatic program repair. Recent work has shown that some kind of bugs are more suitable for automatic repair techniques. [Aim]: The detection and characterization of common bug-fix patterns in software repositories play an important role in advancing the field of automatic program repair. In this paper, we aim to characterize the occurrence of known bug-fix patterns in Java repositories at an unprecedented large scale. [Method]: The study was conducted for Java GitHub projects organized in two distinct data sets: the first one (i.e., Boa data set) contains more than 4 million bug-fix commits from 101,471 projects and the second one (i.e., Defects4J data set) contains 369 real bug fixes from five open-source projects. We used a domain-specific programming language called Boa in the first data set and conducted a manual analysis on the second data set in order to confront the results. [Results]: We characterized the prevalence of the five most common bug-fix patterns (identified in the work of Pan et al.) in those bug fixes. The combined results showed direct evidence that developers often forget to add IF preconditions in the code. Moreover, 76% of bug-fix commits associated with the IF-APC bug-fix pattern are isolated from the other four bug-fix patterns analyzed. [Conclusion]: Targeting on bugs that miss preconditions is a feasible alternative in automatic repair techniques that would produce a relevant payback.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {404–413},
numpages = {10},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3041021.3054726,
author = {Beheshti, Seyed-Mehdi-Reza and Tabebordbar, Alireza and Benatallah, Boualem and Nouri, Reza},
title = {On Automating Basic Data Curation Tasks},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3054726},
doi = {10.1145/3041021.3054726},
abstract = {Big data analytics is firmly recognized as a strategic priority for modern enterprises. At the heart of big data analytics lies the data curation process, consists of tasks that transform raw data (unstructured, semi-structured and structured data sources) into curated data, i.e. contextualized data and knowledge that is maintained and made available for use by end-users and applications. To achieve this, the data curation process may involve techniques and algorithms for extracting, classifying, linking, merging, enriching, sampling, and the summarization of data and knowledge. To facilitate the data curation process and enhance the productivity of researchers and developers, we identify and implement a set of basic data curation APIs and make them available as services to researchers and developers to assist them in transforming their raw data into curated data. The curation APIs enable developers to easily add features - such as extracting keyword, part of speech, and named entities such as Persons, Locations, Organizations, Companies, Products, Diseases, Drugs, etc.; providing synonyms and stems for extracted information items leveraging lexical knowledge bases for the English language such as WordNet; linking extracted entities to external knowledge bases such as Google Knowledge Graph and Wikidata; discovering similarity among the extracted information items, such as calculating similarity between string and numbers; classifying, sorting and categorizing data into various types, forms or any other distinct class; and indexing structured and unstructured data - into their data applications. These services can be accessed via a REST API, and the data is returned as a JSON file that can be integrated into data applications. The curation APIs are available as an open source project on GitHub.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {165–169},
numpages = {5},
keywords = {big data analytics, curation api, data curation},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3373376.3378460,
author = {Hu, Xing and Liang, Ling and Li, Shuangchen and Deng, Lei and Zuo, Pengfei and Ji, Yu and Xie, Xinfeng and Ding, Yufei and Liu, Chang and Sherwood, Timothy and Xie, Yuan},
title = {DeepSniffer: A DNN Model Extraction Framework Based on Learning Architectural Hints},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378460},
doi = {10.1145/3373376.3378460},
abstract = {As deep neural networks (DNNs) continue their reach into a wide range of application domains, the neural network architecture of DNN models becomes an increasingly sensitive subject, due to either intellectual property protection or risks of adversarial attacks. Previous studies explore to leverage architecture-level events disposed in hardware platforms to extract the model architecture information. They pose the following limitations: requiring a priori knowledge of victim models, lacking in robustness and generality, or obtaining incomplete information of the victim model architecture.Our paper proposes DeepSniffer, a learning-based model extraction framework to obtain the complete model architecture information without any prior knowledge of the victim model. It is robust to architectural and system noises introduced by the complex memory hierarchy and diverse run-time system optimizations. The basic idea of DeepSniffer is to learn the relation between extracted architectural hints (e.g., volumes of memory reads/writes obtained by side-channel or bus snooping attacks) and model internal architectures. Taking GPU platforms as a show case, DeepSniffer conducts model extraction by learning both the architecture-level execution features of kernels and the inter-layer temporal association information introduced by the common practice of DNN design. We demonstrate that DeepSniffer works experimentally in the context of an off-the-shelf Nvidia GPU platform running a variety of DNN models. The extracted models are directly helpful to the attempting of crafting adversarial inputs. Our experimental results show that DeepSniffer achieves a high accuracy of model extraction and thus improves the adversarial attack success rate from 14.6%$sim$25.5% (without network architecture knowledge) to 75.9% (with extracted network architecture). The DeepSniffer project has been released in Github.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {385–399},
numpages = {15},
keywords = {deep learning security, domain-specific architecture, machine learning},
location = {Lausanne, Switzerland},
series = {ASPLOS '20}
}

@inproceedings{10.1145/3377811.3380347,
author = {Yan, Jiwei and Liu, Hao and Pan, Linjie and Yan, Jun and Zhang, Jian and Liang, Bin},
title = {Multiple-Entry Testing of Android Applications by Constructing Activity Launching Contexts},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380347},
doi = {10.1145/3377811.3380347},
abstract = {Existing GUI testing approaches of Android apps usually test apps from a single entry. In this way, the marginal activities far away from the default entry are difficult to be covered. The marginal activities may fail to be launched due to requiring a great number of activity transitions or involving complex user operations, leading to uneven coverage on activity components. Besides, since the test space of GUI programs is infinite, it is difficult to test activities under complete launching contexts using single-entry testing approaches.In this paper, we address these issues by constructing activity launching contexts and proposing a multiple-entry testing framework. We perform an inter-procedural, flow-, context- and path-sensitive analysis to build activity launching models and generate complete launching contexts. By activity exposing and static analysis, we could launch activities directly under various contexts without performing long event sequence on GUI. Besides, to achieve an in-depth exploration, we design an adaptive exploration framework which supports the multiple-entry exploration and dynamically assigns weights to entries in each turn.Our approach is implemented in a tool called Fax, with an activity launching strategy Faxla and an exploration strategy Faxex. The experiments on 20 real-world apps show that Faxla can cover 96.4% and successfully launch 60.6% activities, based on which Faxex further achieves a relatively 19.7% improvement on method coverage compared with the most popular tool Monkey. Our tool also behaves well in revealing hidden bugs. Fax can trigger over seven hundred unique crashes, including 180 Errors and 539 Warnings, which is significantly higher than those of other tools. Among the 46 bugs reported to developers on Github, 33 have been fixed up to now.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {457–468},
numpages = {12},
keywords = {static analysis, ICC, Android app, multiple-entry testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3239235.3239244,
author = {Walkinshaw, Neil and Minku, Leandro},
title = {Are 20% of Files Responsible for 80% of Defects?},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3239244},
doi = {10.1145/3239235.3239244},
abstract = {Background: Over the past two decades a mixture of anecdote from the industry and empirical studies from academia have suggested that the 80:20 rule (otherwise known as the Pareto Principle) applies to the relationship between source code files and the number of defects in the system: a small minority of files (roughly 20%) are responsible for a majority of defects (roughly 80%).Aims: This paper aims to establish how widespread the phenomenon is by analysing 100 systems (previous studies have focussed on between one and three systems), with the goal of whether and under what circumstances this relationship does hold, and whether the key files can be readily identified from basic metrics.Method: We devised a search criterion to identify defect fixes from commit messages and used this to analyse 100 active Github repositories, spanning a variety of languages and domains. We then studied the relationship between files, basic metrics (churn and LOC), and defect fixes.Results: We found that the Pareto principle does hold, but only if defects that incur fixes to multiple files count as multiple defects. When we investigated multi-file fixes, we found that key files (belonging to the top 20%) are commonly fixed alongside other much less frequently-fixed files. We found LOC to be poorly correlated with defect proneness, Code Churn was a more reliable indicator, but only for extremely high values of Churn.Conclusions: It is difficult to reliably identify the "most fixed" 20% of files from basic metrics. However, even if they could be reliably predicted, focussing on them would probably be misguided. Although fixes will naturally involve files that are often involved in other fixes too, they also tend to include other less frequently-fixed files.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {2},
numpages = {10},
keywords = {survey, defect distribution, pareto principle},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3243734.3243738,
author = {Abuhamad, Mohammed and AbuHmed, Tamer and Mohaisen, Aziz and Nyang, DaeHun},
title = {Large-Scale and Language-Oblivious Code Authorship Identification},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243738},
doi = {10.1145/3243734.3243738},
abstract = {Efficient extraction of code authorship attributes is key for successful identification. However, the extraction of such attributes is very challenging, due to various programming language specifics, the limited number of available code samples per author, and the average code lines per file, among others. To this end, this work proposes a Deep Learning-based Code Authorship Identification System (DL-CAIS) for code authorship attribution that facilitates large-scale, language-oblivious, and obfuscation-resilient code authorship identification. The deep learning architecture adopted in this work includes TF-IDF-based deep representation using multiple Recurrent Neural Network (RNN) layers and fully-connected layers dedicated to authorship attribution learning. The deep representation then feeds into a random forest classifier for scalability to de-anonymize the author. Comprehensive experiments are conducted to evaluate DL-CAIS over the entire Google Code Jam (GCJ) dataset across all years (from 2008 to 2016) and over real-world code samples from 1987 public repositories on GitHub. The results of our work show the high accuracy despite requiring a smaller number of files per author. Namely, we achieve an accuracy of 96% when experimenting with 1,600 authors for GCJ, and 94.38% for the real-world dataset for 745 C programmers. Our system also allows us to identify 8,903 authors, the largest-scale dataset used by far, with an accuracy of 92.3%. Moreover, our technique is resilient to language-specifics, and thus it can identify authors of four programming languages (e.g. C, C++, Java, and Python), and authors writing in mixed languages (e.g. Java/C++, Python/C++). Finally, our system is resistant to sophisticated obfuscation (e.g. using C Tigress) with an accuracy of 93.42% for a set of 120 authors.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {101–114},
numpages = {14},
keywords = {software forensics, deep learning identification, program features, code authorship identification},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{10.1145/3131151.3131163,
author = {Wessel, Mairieli Santos and Aniche, Maur\'{\i}cio Finavaro and Oliva, Gustavo Ansaldi and Gerosa, Marco Aur\'{e}lio and Wiese, Igor Scaliante},
title = {Tweaking Association Rules to Optimize Software Change Recommendations},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131163},
doi = {10.1145/3131151.3131163},
abstract = {Past researchs have been trying to recommend artifacts that are likely to change together in a task to assist developers in making changes to a software system, often using techniques like association rules. Association rules learning is a data mining technique that has been frequently used to discover evolutionary couplings. These couplings constitute a fundamental piece of modern change prediction techniques. However, using association rules to detect evolutionary coupling requires a number of configuration parameters, such as measures of interest (e.g. support and confidence), their cut-off values, and the portion of the commit history from which co-change relationships will be extracted. To accomplish this set up, researchers have to carry out empirical studies for each project, testing a few variations of the parameters before choosing a configuration. This makes it difficult to use association rules in practice, since developers would need to perform experiments before applying the technique and would end up choosing non-optimal solutions that lead to wrong predictions. In this paper, we propose a fitness function for a Genetic Algorithm that optimizes the co-change recommendations and evaluate it on five open source projects (CPython, Django, Laravel, Shiny and Gson). The results indicate that our genetic algorithm is able to find optimized cut-off values for support and confidence, as well as to determine which length of commit history yields the best recommendations. We also find that, for projects with less commit history (5k commits), our approach produced better results than the regression function proposed in the literature. This result is particularly encouraging, because repositories such as GitHub host many young projects. Our results can be used by researchers when conducting co-change prediction studies and by tool developers to produce automated support to be used by practitioners.},
booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
pages = {94–103},
numpages = {10},
keywords = {Genetic Algorithm, Association Rules, Change Recommendation},
location = {Fortaleza, CE, Brazil},
series = {SBES'17}
}

@inproceedings{10.1145/3107411.3108203,
author = {Dunn, Tamsen and Berry, Gwenn and Emig-Agius, Dorothea and Jiang, Yu and Iyer, Anita and Udar, Nitin and Str\"{o}mberg, Michael},
title = {Pisces: An Accurate and Versatile Single Sample Somatic and Germline Variant Caller},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3108203},
doi = {10.1145/3107411.3108203},
abstract = {A method for robustly and accurately detecting rare DNA mutations in tumor samples is critical to cancer research. Because many clinical tissue repositories have only FFPE-degraded tumor samples, and no matched normal sample from healthy tissue available, being able to discriminate low frequency mutations from background noise in the absence of a matched normal sample is of particular importance to research. Current state of the art variant callers such as GATK and VarScan focus on germline variant calling (used for detecting inherited mutations following a Mendelian inheritance pattern) or, in the case of FreeBayes and MuTect, focus on tumor-normal joint variant calling (using the normal sample to help discriminate low frequency somatic mutations from back ground noise). We present Pisces, a tumor-only variant caller exclusively developed at Illumina for detecting low frequency mutations from next generation sequencing data. Pisces has been an integral part of the Illumina Truseq Amplicon workflow since 2012, and is available on BaseSpace and on the MiSeq sequencing platforms. Pisces has been available to the public on github, since 2015. (https://github.com/Illumina/Pisces) Since that time, the Pisces variant calling team have continued to develop Pisces, and have made available a suite of variant calling tools, including a ReadStitcher, Variant Phaser, and Variant Quality Recalibration tool, to be used along with the core variant caller, Pisces. Here, we describe the Pisces variant calling tools and core algorithms. We describe the common use cases for Pisces (not necessarily restricted to somatic variant calling). We also evaluate Pisces performance on somatic and germline datasets, both from the titration of well characterized samples, and from a corpus of 500 FFPE-treated clinical trial tumor samples, against other variant callers. Our results show that Pisces gives highly accurate results in a variety of contexts. We recommend Pisces for amplicon somatic and germline variant calling.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {595},
numpages = {1},
keywords = {variant caller, single sample, tumor only, pisces, germline, somatic},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/3133956.3134048,
author = {Duan, Ruian and Bijlani, Ashish and Xu, Meng and Kim, Taesoo and Lee, Wenke},
title = {Identifying Open-Source License Violation and 1-Day Security Risk at Large Scale},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134048},
doi = {10.1145/3133956.3134048},
abstract = {With millions of apps available to users, the mobile app market is rapidly becoming very crowded. Given the intense competition, the time to market is a critical factor for the success and profitability of an app. In order to shorten the development cycle, developers often focus their efforts on the unique features and workflows of their apps and rely on third-party Open Source Software (OSS) for the common features. Unfortunately, despite their benefits, careless use of OSS can introduce significant legal and security risks, which if ignored can not only jeopardize security and privacy of end users, but can also cause app developers high financial loss. However, tracking OSS components, their versions, and interdependencies can be very tedious and error-prone, particularly if an OSS is imported with little to no knowledge of its provenance.We therefore propose OSSPolice, a scalable and fully-automated tool for mobile app developers to quickly analyze their apps and identify free software license violations as well as usage of known vulnerable versions of OSS. OSSPolice introduces a novel hierarchical indexing scheme to achieve both high scalability and accuracy, and is capable of efficiently comparing similarities of app binaries against a database of hundreds of thousands of OSS sources (billions of lines of code). We populated OSSPolice with 60K C/C++ and 77K Java OSS sources and analyzed 1.6M free Google Play Store apps. Our results show that 1) over 40K apps potentially violate GPL/AGPL licensing terms, and 2) over 100K of apps use known vulnerable versions of OSS. Further analysis shows that developers violate GPL/AGPL licensing terms due to lack of alternatives, and use vulnerable versions of OSS despite efforts from companies like Google to improve app security. OSSPolice is available on GitHub.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2169–2185},
numpages = {17},
keywords = {application security, license violation, code clone detection},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{10.5555/3241189.3241259,
author = {Stevens, Marc and Shumow, Daniel},
title = {Speeding up Detection of SHA-1 Collision Attacks Using Unavoidable Attack Conditions},
year = {2017},
isbn = {9781931971409},
publisher = {USENIX Association},
address = {USA},
abstract = {Counter-cryptanalysis, the concept of using cryptanalytic techniques to detect cryptanalytic attacks, was introduced at CRYPTO 2013 [23] with a hash collision detection algorithm. That is, an algorithm that detects whether a given single message is part of a colliding message pair constructed using a cryptanalytic collision attack on MD5 or SHA-1.Unfortunately, the original collision detection algorithm is not a low-cost solution as it costs 15 to 224 times more than a single hash computation. In this paper we present a significant performance improvement for collision detection based on the new concept of unavoidable conditions. Unavoidable conditions are conditions that are necessary for all feasible attacks in a certain attack class. As such they can be used to quickly dismiss particular attack classes that may have been used in the construction of the message. To determine an unavoidable condition one must rule out any feasible variant attack where this condition might not be necessary, otherwise adversaries aware of counter-cryptanalysis could easily bypass this improved collision detection with a carefully chosen variant attack. Based on a conjecture solidly supported by the current state of the art, we show how we can determine such unavoidable conditions for SHA-1.We have implemented the improved SHA-1 collision detection using such unavoidable conditions and which is more than 20 times faster than without our unavoidable condition improvements. We have measured that overall our implemented SHA-1 with collision detection is only a factor 1.60 slower, on average, than SHA-1. With the demonstration of a SHA-1 collision, the algorithm presented here has been deployed by Git, GitHub, Google Drive, Gmail, Microsoft OneDrive and others, showing the effectiveness of this technique.},
booktitle = {Proceedings of the 26th USENIX Conference on Security Symposium},
pages = {881–897},
numpages = {17},
location = {Vancouver, BC, Canada},
series = {SEC'17}
}

@inproceedings{10.1145/3134271.3134292,
author = {Zhu, Shaochen and Wang, Betty},
title = {Predicative Model for Uber Ridership in New York City},
year = {2017},
isbn = {9781450352765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134271.3134292},
doi = {10.1145/3134271.3134292},
abstract = {Objective: This study aimed to build a predictive model for Uber ridership in New York City.Data: Uber rides data is downloaded from GitHub. The data were collected during January 2015 to June 2015 and had ridership information for all the five boroughs of New York City. We used a random sample of 50% whole data to build a predictive model, and used the other 50% to validate the model and further used bootstrap data for model validation. Mean squared errors (MSE) were calculated. The predicted riders and the observed riders were compared to measure the performance of the predictive model. All the analysis was done using free statistical software R.Results: A total of 20490 observations including hourly ridership information were extracted for this study. A total of 10245 observations were selected in training sample for model building, and 10245 in test sample, and 500000 in bootstrap sample which was based on test sample were used for model validation. A total of 7171685 riders were in training sample, 7092530 in test sample, and 345936605 in bootstrap sample. The predicted risers were 7171685 riders in training sample, 7134560 in test sample, and 348357233 in bootstrap sample. The predictive model performed well in the split sample which was not used for model building and bootstrap sample. The MSE was 142,866 in training sample, and 141,142 in test sample and 141480 in bootstrap sample. The observed ridership and predicted ridership were close to each other in each month of Jan-June, in each hour, in each week day, and in each district in New York city.Conclusions: A predictive model was built and validated using public available data for Uber ridership in New York city. It could be used to predict the business opportunities for Uber and help to make an informed decision regarding resource allocation.},
booktitle = {Proceedings of the International Conference on Business and Information Management},
pages = {112–115},
numpages = {4},
keywords = {New York City, Uber Ridership, MSE (Mean Squared Errors), Bootstrap, Predicative Model},
location = {Bei Jing, China},
series = {ICBIM 2017}
}

@inproceedings{10.1145/3377811.3380408,
author = {Chen, Boyuan and Jiang, Zhen Ming (Jack)},
title = {Studying the Use of Java Logging Utilities in the Wild},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380408},
doi = {10.1145/3377811.3380408},
abstract = {Software logging is widely used in practice. Logs have been used for a variety of purposes like debugging, monitoring, security compliance, and business analytics. Instead of directly invoking the standard output functions, developers usually prefer to use logging utilities (LUs) (e.g., SLF4J), which provide additional functionalities like thread-safety and verbosity level support, to instrument their source code. Many of the previous research works on software logging are focused on the log printing code. There are very few works studying the use of LUs, although new LUs are constantly being introduced by companies and researchers. In this paper, we conducted a large-scale empirical study on the use of Java LUs in the wild. We analyzed the use of 3, 856 LUs from 11,194 projects in GitHub and found that many projects have complex usage patterns for LUs. For example, 75.8% of the large-sized projects have implemented their own LUs in their projects. More than 50% of these projects use at least three LUs. We conducted further qualitative studies to better understand and characterize the complex use of LUs. Our findings show that different LUs are used for a variety of reasons (e.g., internationalization of the log messages). Some projects develop their own LUs to satisfy project-specific logging needs (e.g., defining the logging format). Multiple uses of LUs in one project are pretty common for large and very largesized projects mainly for context like enabling and configuring the logging behavior for the imported packages. Interviewing with 13 industrial developers showed that our findings are also generally true for industrial projects and are considered as very helpful for them to better configure and manage the logging behavior for their projects. The findings and the implications presented in this paper will be useful for developers and researchers who are interested in developing and maintaining LUs.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {397–408},
numpages = {12},
keywords = {empirical software engineering, logging code, logging practices},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/ICPC.2019.00052,
author = {Xu, Shengzhe and Dong, Ziqi and Meng, Na},
title = {Meditor: Inference and Application of API Migration Edits},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00052},
doi = {10.1109/ICPC.2019.00052},
abstract = {Developers build programs based on software libraries. When a library evolves, programmers need to migrate their client code from the library's old release(s) to new release(s). Due to the API backwards incompatibility issues, such code migration may require developers to replace API usage and apply extra edits (e.g., statement insertions or deletions) to ensure the syntactic or semantic correctness of migrated code. Existing tools extract API replacement rules without handling the additional edits necessary to fulfill a migration task. This paper presents our novel approach, Meditor, which extracts and applies the necessary edits together with API replacement changes.Meditor has two phases: inference and application of migration edits. For edit inference, Meditor mines open source repositories for migration-related (MR) commits, and conducts program dependency analysis on changed Java files to locate and cluster MR code changes. From these changes, Meditor further generalizes API migration edits by abstracting away unimportant details (e.g., concrete variable identifiers). For edit application, Meditor matches a given program with inferred edits to decide which edit is applicable, customizes each applicable edit, and produces a migrated version for developers to review.We applied Meditor to four popular libraries: Lucene, Craft-Bukkit, Android SDK, and Commons IO. By searching among 602,249 open source projects on GitHub, Meditor identified 1,368 unique migration edits. Among these edits, 885 edits were extracted from single updated statements, while the other 483 more complex edits were from multiple co-changed statements. We sampled 937 inferred edits for manual inspection and found all of them to be correct. Our evaluation shows that Meditor correctly applied code migrations in 218 out of 225 cases. This research will help developers automatically adapt client code to different library versions.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {335–346},
numpages = {12},
keywords = {automatic program transformation, API migration edits, program dependency analysis},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/2745802.2745833,
author = {Romo, Bilyaminu Auwal and Capiluppi, Andrea},
title = {Towards an Automation of the Traceability of Bugs from Development Logs: A Study Based on Open Source Software},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745833},
doi = {10.1145/2745802.2745833},
abstract = {Context: Information and tracking of defects can be severely incomplete in almost every Open Source project, resulting in a reduced traceability of defects into the development logs (i.e., version control commit logs). In particular, defect data often appears not in sync when considering what developers logged as their actions. Synchronizing or completing the missing data of the bug repositories, with the logs detailing the actions of developers, would benefit various branches of empirical software engineering research: prediction of software faults, software reliability, traceability, software quality, effort and cost estimation, bug prediction and bug fixing.Objective: To design a framework that automates the process of synchronizing and filling the gaps of the development logs and bug issue data for open source software projects.Method: We instantiate the framework with a sample of OSS projects from GitHub, and by parsing, linking and filling the gaps found in their bug issue data, and development logs. UML diagrams show the relevant modules that will be used to merge, link and connect the bug issue data with the development data.Results: Analysing a sample of over 300 OSS projects we observed that around 1/2 of bug-related data is present in either development logs or issue tracker logs: the rest of the data is missing from one or the other source. We designed an automated approach that fills the gaps of either source by making use of the available data, and we successfully mapped all the missing data of the analysed projects, when using one heuristics of annotating bugs. Other heuristics need to be investigated and implemented.Conclusion: In this paper a framework to synchronise the development logs and bug data used in empirical software engineering was designed to automatically fill the missing parts of development logs and bugs of issue data.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {33},
numpages = {6},
keywords = {bug accuracy, bug traceability, bug-fixing commits},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/3394171.3413858,
author = {Fares, Ahmed and Zhong, Sheng-hua and Jiang, Jianmin},
title = {Brain-Media: A Dual Conditioned and Lateralization Supported GAN (DCLS-GAN) towards Visualization of Image-Evoked Brain Activities},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413858},
doi = {10.1145/3394171.3413858},
abstract = {Essentially, the current concept of multimedia is limited to presenting what people see in their eyes. What people think inside brains, however, remains a rich source of multimedia, such as imaginations of paradise and memories of good old days etc. In this paper, we propose a dual conditioned and lateralization supported GAN (DCLS-GAN) framework to learn and visualize the brain thoughts evoked by stimulating images and hence enable multimedia to reflect not only what people see but also what people think. To reveal such a new world of multimedia inside human brains, we coin such an attempt as "brain-media". By examining the relevance between the visualized image and the stimulation image, we are able to measure the efficiency of our proposed deep framework regarding the quality of such visualization and also the feasibility of exploring the concept of "brain-media". To ensure that such extracted multimedia elements remain meaningful, we introduce a dually conditioned learning technique in the proposed deep framework, where one condition is analyzing EEGs through deep learning to extract a class-dependent and more compact brain feature space utilizing the distinctive characteristics of hemispheric lateralization and brain stimulation, and the other is to extract expressive visual features assisting our automated analysis of brain activities as well as their visualizations aided by artificial intelligence. To support the proposed GAN framework, we create a combined-conditional space by merging the brain feature space with the visual feature space provoked by the stimuli. Extensive experiments are carried out and the results show that our proposed deep framework significantly outperforms the representative existing state-of-the-arts under several settings, especially in terms of both visualization and classification of brain responses to the evoked images. For the convenience of research dissemination, we make the source code openly accessible for downloading at GitHub.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {1764–1772},
numpages = {9},
keywords = {regional attention gate, variant bi-directional LSTM, deep learning, image generation, GAN, brain-media, EEG},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3233027.3233041,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar and Fogdal, Thomas},
title = {Reducing Coordination Overhead in SPLs: Peering in on Peers},
year = {2018},
isbn = {9781450364645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233027.3233041},
doi = {10.1145/3233027.3233041},
abstract = {SPL product customers might not always wait for the next core asset release. When an organization aims to react to market events, quick bug fixes or urgent customer requests, strategies are needed to support fast adaptation, e.g. with product-specific extensions, which are later propagated into the SPL. This leads to the grow-and-prune model where quick reaction to changes often requires copying and specialization (grow) to be later cleaned up by merging and refactoring (prune). This paper focuses on the grow stage. Here, application engineers branch off the core-asset Master branch to account for their products' specifics within the times and priorities of their customers without having to wait for the next release of the core assets. However, this practice might end up in the so-called "integration hell". When long-living branches are merged back into the Master, the amount of code to be integrated might cause build failures or requires complex troubleshooting. On these premises, we advocate for making application engineers aware of potential coordination problems right during coding rather than deferring it until merge time. To this end, we introduce the notion of "peering bar" for Version Control Systems, i.e. visual bars that reflect whether your product's features are being upgraded in other product branches. In this way, engineers are aware of what their peers are doing on the other SPL's products. Being products from the same SPL, they are based on the very same core assets, and hence, bug ixes or functional enhancements undertaken for a product might well serve other products. This work introduces design principles for peering bars. These principles are fleshed out for GitHub as the Version Control System, and pure::variants as the SPL framework.},
booktitle = {Proceedings of the 22nd International Systems and Software Product Line Conference - Volume 1},
pages = {110–120},
numpages = {11},
location = {Gothenburg, Sweden},
series = {SPLC '18}
}

@inproceedings{10.1145/3383219.3383264,
author = {Madeyski, Lech and Lewowski, Tomasz},
title = {MLCQ: Industry-Relevant Code Smell Data Set},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383264},
doi = {10.1145/3383219.3383264},
abstract = {Context Research on code smells accelerates and there are many studies that discuss them in the machine learning context. However, while data sets used by researchers vary in quality, all which we encountered share visible shortcomings---data sets are gathered from a rather small number of often outdated projects by single individuals whose professional experience is unknown.Aim This study aims to provide a new data set that addresses the aforementioned issues and, additionally, opens new research opportunities.Method We collaborate with professional software developers (including the code quest company behind the codebeat automated code review platform integrated with GitHub) to review code samples with respect to bad smells. We do not provide additional hints as to what do we mean by a given smell, because our goal is to extract professional developers' contemporary understanding of code smells instead of imposing thresholds from the legacy literature. We gather samples from active open source projects manually verified for industry-relevance and provide repository links and revisions. Records in our MLCQ data set contain the type of smell, its severity and the exact location in source code, but do not contain any source code metrics which can be calculated using various tools. To open new research opportunities, we provide results of an extensive survey of developers involved in the study including a wide range of details concerning their professional experience in software development and many other characteristics. This allows us to track each code review to the developer's background. To the best of our knowledge, this is a unique trait of the presented data set.Conclusions The MLCQ data set with nearly 15000 code samples was created by software developers with professional experience who reviewed industry-relevant, contemporary Java open source projects. We expect that this data set should stay relevant for a longer time than data sets that base on code released years ago and, additionally, will enable researchers to investigate the relationship between developers' background and code smells' perception.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {342–347},
numpages = {6},
keywords = {data set, bad code smells, software development, code smells, software quality},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/3233547.3233628,
author = {Turk, Erdem and Arit, Turkan and Susus, Delikanli Mertcan and Ucar, Ilayda and Suzek, Baris E.},
title = {ProSetComp: A Platform for Protein Set Comparisons},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233628},
doi = {10.1145/3233547.3233628},
abstract = {The amount of data available in public bioinformatics resources and the complexity of user interfaces they are served through often challenges appreciation and effective utilization of these valuable resources. While education, documentation and training activities mitigate this problem, there is still a need to develop user interfaces to serve simple day-to-day needs of scientists. To this end, we developed ProSetComp; a simple web-based platform to create and compare protein sets, following a traditional software development process; from requirement analysis to implementation. First, we interviewed and collected user scenarios from wet lab scientists with seniority, research interests and backgrounds. Reviewing the user scenarios, we identified one high impact need that drove the development of ProSetComp; ability to 1) create protein sets by searching databases, 2) compare these protein sets in different dimensions such as functional domains, pathways, molecular functions and biological processes, and 3) visualize results graphically. Next, we collected and integrated necessary data from several bioinformatics resources including UniProt, Reactome, Gene Ontology and PFAM in a local relational database. Finally, we designed user interfaces that facilitate the creation of protein sets by using form-based query generators and exploring the relationship between created protein sets using tabular and graphical representations. The current internal release of the platform contains ~120 million protein entries. The user interface supports &gt;50 search criteria to create up-to four protein sets and comparison of these sets in four dimensions; protein domains, molecular functions, biological processes, and pathways. The commonality and differences between protein sets, along with tables, can be explored using novel user interface components such as Venn and UpSet diagrams. The first public release of ProSetComp (http://ceng.mu.edu.tr/labs/bioinfo/prosetcomp) is targeted for mid-August, 2018 and planned to be updated monthly thereafter. Upon public release, source code ProSetComp will become available through GitHub. The database content and user interface will be expanded as per community needs. The ProSetComp project is supported by The Scientific and Technological Research Council of Turkey (TUBITAK, Grant number: 216Z111).},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {519},
numpages = {1},
keywords = {comparative proteomics, protein bioinformatics, functional analysis},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3368089.3409764,
author = {Mahajan, Sonal and Abolhassani, Negarsadat and Prasad, Mukul R.},
title = {Recommending Stack Overflow Posts for Fixing Runtime Exceptions Using Failure Scenario Matching},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409764},
doi = {10.1145/3368089.3409764},
abstract = {Using online Q&amp;A forums, such as Stack Overflow (SO), for guidance to resolve program bugs, among other development issues, is commonplace in modern software development practice. Runtime exceptions (RE) is one such important class of bugs that is actively discussed on SO. In this work we present a technique and prototype tool called MAESTRO that can automatically recommend an SO post that is most relevant to a given Java RE in a developer's code. MAESTRO compares the exception-generating program scenario in the developer's code with that discussed in an SO post and returns the post with the closest match. To extract and compare the exception scenario effectively, MAESTRO first uses the answer code snippets in a post to implicate a subset of lines in the post's question code snippet as responsible for the exception and then compares these lines with the developer's code in terms of their respective Abstract Program Graph (APG) representations. The APG is a simplified and abstracted derivative of an abstract syntax tree, proposed in this work, that allows an effective comparison of the functionality embodied in the high-level program structure, while discarding many of the low-level syntactic or semantic differences. We evaluate MAESTRO on a benchmark of 78 instances of Java REs extracted from the top 500 Java projects on GitHub and show that MAESTRO can return either a highly relevant or somewhat relevant SO post corresponding to the exception instance in 71% of the cases, compared to relevant posts returned in only 8% - 44% instances, by four competitor tools based on state-of-the-art techniques. We also conduct a user experience study of MAESTRO with 10 Java developers, where the participants judge MAESTRO reporting a highly relevant or somewhat relevant post in 80% of the instances. In some cases the post is judged to be even better than the one manually found by the participant.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1052–1064},
numpages = {13},
keywords = {static analysis, code search, crowd intelligence, runtime exceptions},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3377811.3380926,
author = {Nguyen, Son and Phan, Hung and Le, Trinh and Nguyen, Tien N.},
title = {Suggesting Natural Method Names to Check Name Consistencies},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380926},
doi = {10.1145/3377811.3380926},
abstract = {Misleading names of the methods in a project or the APIs in a software library confuse developers about program functionality and API usages, leading to API misuses and defects. In this paper, we introduce MNire, a machine learning approach to check the consistency between the name of a given method and its implementation. MNire first generates a candidate name and compares the current name against it. If the two names are sufficiently similar, we consider the method as consistent. To generate the method name, we draw our ideas and intuition from an empirical study on the nature of method names in a large dataset. Our key finding is that high proportions of the tokens of method names can be found in the three contexts of a given method including its body, the interface (the method's parameter types and return type), and the enclosing class' name. Even when such tokens are not there, MNire uses the contexts to predict the tokens due to the high likelihoods of their co-occurrences. Our unique idea is to treat the name generation as an abstract summarization on the tokens collected from the names of the program entities in the three above contexts.We conducted several experiments to evaluate MNire in method name consistency checking and in method name recommending on large datasets with +14M methods. In detecting inconsistency method names, MNire improves the state-of-the-art approach by 10.4% and 11% relatively in recall and precision, respectively. In method name recommendation, MNire improves relatively over the state-of-the-art technique, code2vec, in both recall (18.2% higher) and precision (11.1% higher). To assess MNire's usefulness, we used it to detect inconsistent methods and suggest new names in several active, GitHub projects. We made 50 pull requests (PRs) and received 42 responses. Among them, five PRs were merged into the main branch, and 13 were approved for later merging. In total, in 31/42 cases, the developer teams agree that our suggested names are more meaningful than the current names, showing MNire's usefulness.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1372–1384},
numpages = {13},
keywords = {program entity name suggestion, naturalness of source code, deep learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3338906.3341181,
author = {Guerrero, Alejandro and Fresno, Rafael and Ju, An and Fox, Armando and Fernandez, Pablo and Muller, Carlos and Ruiz-Cort\'{e}s, Antonio},
title = {Eagle: A Team Practices Audit Framework for Agile Software Development},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341181},
doi = {10.1145/3338906.3341181},
abstract = {Agile/XP (Extreme Programming) software teams are expected to follow a number of specific practices in each iteration, such as estimating the effort (”points”) required to complete user stories, properly using branches and pull requests to coordinate merging multiple contributors’ code, having frequent ”standups” to keep all team members in sync, and conducting retrospectives to identify areas of improvement for future iterations. We combine two observations in developing a methodology and tools to help teams monitor their performance on these practices. On the one hand, many Agile practices are increasingly supported by web-based tools whose ”data exhaust” can provide insight into how closely the teams are following the practices. On the other hand, some of the practices can be expressed in terms similar to those developed for expressing service level objectives (SLO) in software as a service; as an example, a typical SLO for an interactive Web site might be ”over any 5-minute window, 99% of requests to the main page must be delivered within 200ms” and, analogously, a potential Team Practice (TP) for an Agile/XP team might be ”over any 2-week iteration, 75% of stories should be ’1-point’ stories”. Following this similarity, we adapt a system originally developed for monitoring and visualizing service level agreement (SLA) compliance to monitor selected TPs for Agile/XP software teams. Specifically, the system consumes and analyzes the data exhaust from widely-used tools such as GitHub and Pivotal Tracker and provides team(s) and coach(es) a ”dashboard” summarizing the teams’ adherence to various practices. As a qualitative initial investigation of its usefulness, we deployed it to twenty student teams in a four-sprint software engineering project course. We find an improvement of the adherence to team practice and a positive students’ self-evaluations of their team practices when using the tool, compared to previous experiences using an Agile/XP methodology. The demo video is located at <a>https://youtu.be/A4xwJMEQh9c</a> and a landing page with a live demo at <a>https://isa-group.github.io/2019-05-eagle-demo/</a>.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1139–1143},
numpages = {5},
keywords = {team dashboard, team practice agreement, agile, team practice},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3388440.3414701,
author = {Hippe, Kyle and Gbenro, Sola and Cao, Renzhi},
title = {ProLanGO2: Protein Function Prediction with Ensemble of Encoder-Decoder Networks},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414701},
doi = {10.1145/3388440.3414701},
abstract = {Predicting protein function from protein sequence is a main challenge in the computational biology field. Traditional methods that search protein sequences against existing databases may not work well in practice, particularly when little or no homology exists in the database. We introduce the ProLanGO2 method which utilizes the natural language processing and machine learning techniques to tackle the protein function prediction problem with protein sequence as input. Our method has been benchmarked blindly in the latest Critical Assessment of protein Function Annotation algorithms (CAFA 4) experiment. There are a few changes compared to the old version of ProLanGO. First of all, the latest version of the UniProt database is used. Second, the Uniprot database is filtered by the newly created fragment sequence database FSD to prepare for the protein sequence language. Third, the Encoder-Decoder network, a model consisting of two RNNs (encoder and decoder), is used to train models on the dataset. Fourth, if no k-mers of a protein sequence exist in the FSD, we select the top ten GO terms with the highest probability in all sequences from the Uniprot database that didn't contain any k-mers in FSD, and use those ten GO terms as back up for the prediction of new protein sequence. Finally, we selected the 100 best performing models and explored all combinations of those models to select the best performance ensemble model. We benchmark those different combinations of models on CAFA 3 dataset and select three top performance ensemble models for prediction in the latest CAFA 4 experiment as CaoLab. We have also evaluated the performance of our ProLanGO2 method on 253 unseen sequences taken from the UniProt database and compared with several other protein function prediction methods, the results show that our method achieves great performance among sequence-based protein function prediction methods. Our method is available in GitHub: https://github.com/caorenzhi/ProLanGO2.git.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {103},
numpages = {6},
keywords = {Machine learning, Recurrent Neural Network, Protein function prediction},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3219104.3229268,
author = {Chen, Huan and Fietkiewicz, Chris},
title = {Version Control Graphical Interface for Open OnDemand},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229268},
doi = {10.1145/3219104.3229268},
abstract = {Use of high performance computing (HPC) clusters is challenging for the average researcher who is not familiar with the necessary tools such as the Linux operating system and job management software. Of those using HPC systems, the majority are not trained specifically in computer programming. Nearly 70% of users on the NSF-funded XSEDE system are in fields other than computer science and computing science. The graphical user interfaces (GUIs) that most users rely on for common software on personal computers are typically not available on HPCs. The Ohio Supercomputer Center has addressed these issues through the development of an open source, web-based GUI called Open OnDemand. Because it is open source, administrators are free to deploy it on their own systems, and developers are free to enhance it. To improve workforce development and diversity, we sought to: (1) install Open OnDemand on a private cluster and (2) develop a custom add-on module for version control of HPC application software. We successfully installed and configured Open OnDemand for a private cluster at Case Western Reserve University in order to evaluate the difficulty level of deployment. Despite our lack of experience in HPC system administration, the installation process was straight forward due to a streamlined installer package and thorough documentation. In order to evaluate the extensibility of Open OnDemand, we developed a custom web-based interface for use of the popular Git version control system. Version control tools are important in maintaining software by easily tracking and accessing file changes in both single- and multi-developer projects. Our module successfully integrates with the existing Open OnDemand interface and provides common version control operations that can be used during typical HPC workflows. It is our hope that making version control software easier to use will encourage HPC users to adopt the use of version control into their own workflows to improve productivity and repeatability. Source code for the app will be made available on the author's github site.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {103},
numpages = {4},
keywords = {high performance computing, graphical user interface, version control},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1145/3410530.3414365,
author = {Pellatt, Lloyd and Roggen, Daniel},
title = {CausalBatch: Solving Complexity/Performance Tradeoffs for Deep Convolutional and LSTM Networks for Wearable Activity Recognition},
year = {2020},
isbn = {9781450380768},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3410530.3414365},
doi = {10.1145/3410530.3414365},
abstract = {Deep neural networks consisting of a combination of convolutional feature extractor layers and Long Short Term Memory (LSTM) recurrent layers are widely used models for activity recognition from wearable sensors ---referred to as DeepConvLSTM architectures hereafter. However, the subtleties of training these models on sequential time series data is not often discussed in the literature. Continuous sensor data must be segmented into temporal 'windows', and fed through the network to produce a loss which is used to update the parameters of the network. If trained naively using batches of randomly selected data as commonly reported, then the temporal horizon (the maximum delay at which input samples can effect the output of the model) of the network is limited to the length of the window. An alternative approach, which we will call CausalBatch training, is to construct batches deliberately such that each consecutive batch contains windows which are contiguous in time with the windows of the previous batch, with only the first batch in the CausalBatch consisting of randomly selected windows. After a given number of consecutive batches (referred to as the CausalBatch duration τ), the LSTM states are reset, new random starting points are chosen from the dataset and a new CausalBatch is started. This approach allows us to increase the temporal horizon of the network without increasing the window size, which enables networks to learn data dependencies on a longer timescale without increasing computational complexity.We evaluate these two approaches on the Opportunity dataset. We find that using the CausalBatch method we can reduce the training time of DeepConvLSTM by up to 90%, while increasing the user-independent accuracy by up to 6.3% and the class weighted F1 score by up to 5.9% compared to the same model trained by random batch training with the best performing choice of window size for the latter. Compared to the same model trained using the same window length, and therefore the same computational complexity and almost identical training time, we observe an 8.4% increase in accuracy and 14.3% increase in weighted F1 score. We provide the source code for all experiments as well as a Pytorch reference implementation of DeepConvLSTM in a public github repository.},
booktitle = {Adjunct Proceedings of the 2020 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2020 ACM International Symposium on Wearable Computers},
pages = {272–277},
numpages = {6},
keywords = {LSTM, activity recognition, neural networks, batch training, wearable computing, best practices, deep learning},
location = {Virtual Event, Mexico},
series = {UbiComp-ISWC '20}
}

@inproceedings{10.1145/3377811.3380403,
author = {Tabassum, Sadia and Minku, Leandro L. and Feng, Danyi and Cabral, George G. and Song, Liyan},
title = {An Investigation of Cross-Project Learning in Online Just-in-Time Software Defect Prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380403},
doi = {10.1145/3377811.3380403},
abstract = {Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {554–565},
numpages = {12},
keywords = {class imbalance, transfer learning, verification latency, concept drift, cross-project learning, online learning, software defect prediction},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3144763.3144768,
author = {Amreen, Sadika and Mockus, Audris},
title = {Experiences on Clustering High-Dimensional Data Using PbdR},
year = {2017},
isbn = {9781450351355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144763.3144768},
doi = {10.1145/3144763.3144768},
abstract = {Motivation: Software engineering for High Performace Computing (HPC) environments in general [1] and for big data in particular [5] faces a set of unique challenges including high complexity of middleware and of computing environments. Tools that make it easier for scientists to utilize HPC are, therefore, of paramount importance. We provide an experience report of using one of such highly effective middleware pbdR [9] that allow the scientist to use R programming language without, at least nominally, having to master many layers of HPC infrastructure, such as OpenMPI [4] and ScalaPACK [2]. Objective: to evaluate the extent to which middleware helps improve scientist productivity, we use pbdR to solve a real problem that we, as scientists, are investigating. Our big data comes from the commits on GitHub and other project hosting sites and we are trying to cluster developers based on the text of these commit messages. Context: We need to be able to identify developer for every commit and to identify commits for a single developer. Developer identifiers in the commits, such as login, email, and name are often spelled in multiple ways since that information may come from different version control systems (Git, Mercurial, SVN, ...) and may depend on which computer is used (what is specified in .git/config of the home folder). Method: We train Doc2Vec [7] model where existing credentials are used as a document identifier and then use the resulting 200-dimensional vectors for the 2.3M identifiers to cluster these identifiers so that each cluster represents a specific individual. The distance matrix occupies 32TB and, therefore, is a good target for HPC in general and pbdR in particular. pbdR allows data to be distributed over computing nodes and even has implemented K-means and mixture-model clustering techniques in the package pmclust. Results: We used strategic prototyping [3] to evaluate the capabilities of pbdR and discovered that a) the use of middleware required extensive understanding of its inner workings thus negating many of the expected benefits; b) the implemented algorithms were not suitable for the particular combination of n, p, and k (sample size, data dimension, and the number of clusters); c) the development environment based on batch jobs increases development time substantially. Conclusions: In addition to finding from Basili et al., we find that the quality of the implementation of HPC infrastructure and its development environment has a tremendous effect on development productivity.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for High Performance Computing in Computational and Data-Enabled Science &amp; Engineering},
pages = {9–12},
numpages = {4},
keywords = {developer productivity, prototyping, software engineering},
location = {Denver, CO, USA},
series = {SE-CoDeSE'17}
}

@inproceedings{10.1145/3388440.3414907,
author = {Nowling, Ronald J. and Beal, Christopher R. and Emrich, Scott and Behura, Susanta K. and Halfon, Marc S. and Duman-Scheel, Molly},
title = {PeakMatcher: Matching Peaks Across Genome Assemblies},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414907},
doi = {10.1145/3388440.3414907},
abstract = {When reference genome assemblies are updated, the peaks from DNA enrichment assays such as ChIP-Seq and FAIRE-Seq need to be called again using the new genome assembly. PeakMatcher is an open-source package that aids in validation by matching peaks across two genome assemblies using the alignment of reads or within the same genome. PeakMatcher calculates recall and precision while also outputting lists of peak-to-peak matches.PeakMatcher uses read alignments to match peaks across genome assemblies. PeakMatcher finds all read aligned to one genome that overlap with a given list of peaks. PeakMatcher uses the read names to locate where those reads are aligned against a second genome. Lastly, all peaks called against the second genome that overlap with the aligned reads are found and output. PeakMatcher groups uses the peak-read-peak relationships to discover 1-to-1, 1-to-many, and many-to-many relationships. Overlap queries are performed with interval trees for maximum efficiency.We evaluated PeakMatcher on two data sets. The first data set was FAIRE-Seq (Formaldehyde-Assisted Isolation of Regulatory Elements Sequencing) of DNA isolated embyros of the mosquito Aedes aegypti [2, 4]. We implemented a peak calling pipeline and validated it on the older (highly fragmented) AaegL3 assembly [5]. PeakMatcher matched 92.9% (precision) of the 121,594 previously-called peaks from [2, 4] with 89.4% (recall) of the 124,959 peaks called with our new pipeline. Next, we applied the peak-calling pipeline to call FAIRE peaks using the newer, chromosome-complete AaegL5 assembly [3]. PeakMatcher found matches for 14 of the 16 experimentally-validated AaegL3 FAIRE peaks from [2, 4]. We validated the matches by comparing nearby genes across the genomes. Nearby genes were consistent for 11 of the 14 peaks; inconsistencies for at least two of the remaining peaks were clearly attributable to differences in assemblies. When applied to all of the peaks, Peak-Matcher matched 78.8% (precision) of the 124,959 AaegL3 peaks with 76.7% (recall) of the 128,307 AaegL5 peaks.The second data set was STARR-Seq (Self-Transcribing Active Regulatory Region Sequencing) of Drosophila melanogaster DNA in S2 culture cells [1]. We called STARR peaks against two versions (dm3 and r5.53) of the D. melanogaster genome [6]. PeakMatcher matched 77.4% (precision) of the 4,195 dm3 peaks with 94.8% (recall) of the 3,114 r5.53 peaks.PeakMatcher and associated documentation are available on GitHub (https://github.com/rnowling/peak-matcher) under the open-source Apache Software License v2. PeakMatcher was written in Python 3 using the intervaltree library.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {70},
numpages = {1},
keywords = {genome assembly, peak calling, DNA enrichment assays},
location = {Virtual Event, USA},
series = {BCB '20}
}

@proceedings{10.1145/2538862,
title = {SIGCSE '14: Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the 45th ACM Technical Symposium on Computer Science Education (i.e., SIGCSE 2014). We invite you to explore this collection of papers, posters, workshops and other materials that capture the latest ideas, tools and pedagogy in computer science education. We also encourage you to connect with the people that generate, document and "implement" this body of knowledge, take this experience with you and share.The theme is "Leveraging Computing to Change Education," and features a number of talks and sessions focused on how the ideas and applications of computing can benefit not only those learning computer science, but learning overall. SIGCSE 2014 is co-located with a new ACM conference on Learning at Scale; it will be interesting to see what ideas and connections emerge between the two conferences, and we urge you to consider attending both.We are happy to announce the winners of the two annual SIGCSE Awards. Robert M. Panoff, Founder and Executive Director of the Shodor Education Foundation, Inc., will receive the SIGCSE Award for Outstanding Contribution to Computer Science Education, and will provide Thursday morning's keynote address. Andrea W. Lawrence of Spelman College will accept the SIGCSE Award for Lifetime Service to the Computer Science Education Community and speak at our First Timer's Lunch on Thursday. (SIGCSE First Timers will receive lunch for free; more seasoned SIGCSE attendees can purchase a ticket and (a) enjoy a delicious meal and (b) mentor a First Timer).We are also quite excited about the remaining keynote presentations, which include Hadi Partovi, Founder and CEO of Code.org, and A.J. Brush of Microsoft Research, who will each present on Friday morning and Saturday lunch, respectively.Symposium statistics are presented in the following table. We thank the authors, reviewers, and Program Committee members involved in the production of this program. The schedule this year has the usual varied selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition on Thursday afternoon. There are also some atypical offerings, such as Experience-IT, and of course the co-located Learning at Scale conference. The latest in hardware, software tools, textbooks, educational programs and research are found in the exhibit hall.SIGCSE 2014 Pre-symposium Events will occur on Wednesday and cover the following topics: Process Oriented Guided Inquiry Learning (POGIL)Integrating Professional/Ethical Issues into the CS CurriculumNew Educators WorkshopSIGCAS Nowadays: What is it? What should it be?Git &amp; GitHub Foundations for Educators},
location = {Atlanta, Georgia, USA}
}

@proceedings{10.1145/2445196,
title = {SIGCSE '13: Proceeding of the 44th ACM Technical Symposium on Computer Science Education},
year = {2013},
isbn = {9781450318686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to the proceedings of the 44th ACM Technical Symposium on Computer Science Education, or SIGCSE 2013, where you will find over one hundred papers as well as multiple other session formats that document the latest in computer science education: research, tool building, teaching, curriculum and philosophy. The theme of this year's symposium is "The Changing Face of Computing", and features a number of talks and sessions focused on how changes in computing technology and changes in student demographics requires a change in the way computing is taught.SIGCSE 2013's opening keynote session on Thursday will be different from anything seen at SIGCSE previously. It consists of "flash talks" (where several "all stars" in computing will be given five minutes to share 20 slides, each of which automatically advance) that answer the question: "What can WE do to change the face of computing?" Jane Margolis of UCLA will provide SIGCE 2013's closing keynote session on Saturday, where she'll examine how underrepresentation in computing relates to a larger educational crisis and issues we face as world citizens. In addition, during a special keynote session on Friday, Stanford's Provost (John Etchemendy) will discuss whether massively open online courses will change our universities as we know them or be a "flash in the pan".We are pleased to announce the winners of the two annual SIGCSE awards. Professor Michael K\"{o}lling of University of Kent will receive the SIGCSE award for Outstanding Contribution to Computer Science Education, and will provide Friday's keynote address. Henry Walker of Grinnell College will accept the lunch for free; SIGCSE Old Timers can purchase a ticket and (a) enjoy a delicious meal, (b) mentor a First Timer, and (c) listen to Professor Walker's talk.)Symposium statistics are presented in the following table. We thank the authors, reviewers, and Program Committee members whose enormous and vital service generated this program. This year's program includes the usual wide selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition, as well as some unusual offerings, such as the Codebreaker dramadocumentary on Alan Turing's remarkable and tragic life story, a puzzle extravaganza with a raffle for those who complete it, and a CSTA K- 13 Computing Teachers Workshop. Our exhibit hall features a number of exhibitors showcasing the latest in hardware, software tools, textbooks and educational programs and research. We also continue to offer accessibility at SIGCSE 2013 for the deaf and hard of hearing.We are excited about the variety of pre-symposium events that will exist at SIGCSE 2013. As of the press deadline for this overview, meetings on the following topics will occur on Wednesday: Managing the Academic Career for Women, Open Source Software, Computational Thinking Through Computing and Music, CSAB Computing Accreditation Workshop, Git &amp; GitHub Foundations for Educators, SIGCAS Share and Learn, Using the GENI Networking Testbed, Exploring the Next Generation of Scratch, and Integrating Computing Ethics into the Curriculum.},
location = {Denver, Colorado, USA}
}

@inproceedings{10.1145/3380868.3398198,
author = {Aseeri, Samar and Muite, Benson K.},
title = {Benchmarking in the Datacenter (BID) 2020: Workshop Summary},
year = {2020},
isbn = {9781450375368},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380868.3398198},
doi = {10.1145/3380868.3398198},
abstract = {The workshop in beautiful San Diego was composed of two submitted papers and two invited talks.The first paper presentation, "Power Modeling for Phytium FT-2000+ Multi-core Architecture" by Zhixin Ou a PhD student from the National University of Defense Technology was streamed from Hunan, China due to travel difficulties. Her talk describes the application of HPCC benchmarks on the first software-based power model for the new Phytium FT-2000+/64 ARM platform. The results of these benchmarks of the software were compared with real power measurements to prove how accurate this approach is.The first invited talk by Ammar Awan a PhD student at Ohio State University, "Benchmarking Deep Learning Workloads on Large-Scale HPC Systems" described machine learning and deep learning benchmarks. He described parallel communication in distributed deep learning for training image recognition networks and how it is different from typical high performance computing parallel communication. He identified reproducibility and benchmarks for new applications of deep learning as important future areas.The second invited presentation was given by the San Diego Supercomputer Center (SDSC) user support lead Mahidhar Tatineni and addressed the "Evolution of Benchmarking on SDSC systems". It was an interesting talk that gave an overview of platforms and benchmarks at the supercomputing center closest to the conference venue. The speaker succeeded in capturing all aspects in this regard during his 15-years of devoted work at the San Diego Supercomputing Center.The final paper presentation, "The ESIF-HPC-2 Benchmark Suite" was given by Christopher Chang who works at the National Renewable Energy Laboratory (NREL) in the HPC Application group in Denver. He described the benchmark suite he and his team developed for procurement of the most recent NREL supercomputer. He demonstrated a set of dimensions that are useful in classifying benchmarks and in systematically assessing their coverage of performance measures. This suite is released as open source software on GitHub.The program committee for the workshop was composed of:• David Bailey (Lawrence Berkely National Laboratory and University of California Davis)• Valeria Bartsch (Fraunhofer ITWM)• Ben Blamey (Uppsala University)• Rodrigo N. Calheiros (Western Sydney University)• Anando Chatterjee (Indian Institute of Technology Kanpur)• Juan Chen (National University of Defense Technology)• Pawe\l{} Czarnul (Gdansk University of Technology)• Denis Demidov (Kazan Federal University and Russian Academy of Sciences)• Joel Guerrero (University of Genoa and Wolf Dynamics)• Khaled Ibrahim (Lawrence Berkeley National Laboratory)• Kate Isaacs (University of Arizona)• Beau Johnston (Australian National University and University of New England)• Mchael Lehn (University of Ulm)• Maged Korga• Guo Liang (Open Data Center Committee and China Academy of Information and Communications Technology)• Xioyi Lu (Ohio State University)• Amitava Majumdar (San Diego Supercomputing Center)• Jorji Nonaka (Riken)• Peter Pirkelbauer (Lawrence Livermore National Laboratory)• Harald Servat (Intel)• Ashwin Siddarth (University of Texas at Arlington)• Manodeep Sinha (Swinburne University of Technology)• Gabor Sz\'{a}rnyas (Budapest University of Technology and Economics)• Mahidhar Tatineni (San Diego Supercomputing Center)• Jianfeng Zhan (Chinese Academy of Sciences)We thank the program committee and the subreviewers for their careful review of the submitted papers, with each paper obtaining at least 4 reviews. We thank the authors for their patience with the publication process.},
booktitle = {Proceedings of the Workshop on Benchmarking in the Datacenter},
articleno = {1},
numpages = {1},
keywords = {benchmarks, high performance computing, performance evaluation},
location = {San Diego, California},
series = {BID '20}
}

@inproceedings{10.1145/3308560.3316541,
author = {Mei, Qiaozhu},
title = {Decoding the New World Language: Analyzing the Popularity, Roles, and Utility of Emojis},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316541},
doi = {10.1145/3308560.3316541},
abstract = {Emojis have quickly become a universal language that is used by worldwide users, for everyday tasks, across language barriers, and in different apps and platforms. The prevalence of emojis has quickly attracted great attentions from various research communities such as natural language processing, Web mining, ubiquitous computing, and human-computer interaction, as well as other disciplines including social science, arts, psychology, and linguistics.This talk summarizes the recent efforts made by my research group and our collaborators on analyzing large-scale emoji data. The usage of emojis by worldwide users presents interesting commonality as well as divergence. In our analysis of emoji usage by millions of smartphone users in 212 countries, we show that the different preferences and usage of emojis provide rich signals for understanding the cultural differences of Internet users, which correlate with the Hofstede’s cultural dimensions [4].Emojis play different roles when used alongside text. Through jointly learning the embeddings and topological structures of words and emojis, we reveal that emojis present both complementary and supplementary relations to words. Based on the structural properties of emojis in the semantic spaces, we are able to untangle several factors behind the popularity of emojis [1].This talk also highlights the utility of emojis. In general, emojis have been used by Internet users as text supplements to describe objects and situations, express sentiments, or express humor and sarcasm; they are also used as communication tools to attract attention, adjust tones, or establish personal relationships. The benefit of using emojis goes beyond these intentions. In particular, we show that including emojis in the description of an issue report on GitHub results in the issue being responded to by more users and resolved sooner.Large-scale emoji data can also be utilized by AI systems to improve the quality of Web mining services. In particular, a smart machine learning system can infer the latent topics, sentiments, and even demographic information of users based on how they use emojis online. Our analysis reveals a considerable difference between female and male users of emojis, which is big enough for a machine learning algorithm to accurately predict the gender of a user. In Web services that are customized for gender groups, gender inference models built upon emojis can complement those based on text or behavioral traces with fewer privacy concerns [2].Emojis can be also used as an instrument to bridge Web mining tasks across language barriers, especially to transfer sentiment knowledge from a language with rich training labels (e.g., English) to languages that have been difficult for advanced natural language processing tasks [3]. Through this bridge, developers of AI systems and Web services are able to reduce the inequality in the quality of services received by the international users that has been caused by the imbalance of available human annotations in different languages.In general, emojis have evolved from visual ideograms to a brand-new world language in the era of AI and a new Web. The popularity, roles, and utility of emojis have all gone beyond people’s original intentions, which have created a huge opportunity for future research that calls for joint efforts from multiple disciplines.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {417–418},
numpages = {2},
keywords = {natural language processing, emojis, text mining},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3318170.3318177,
author = {Sorensen, Tyler and Pai, Sreepathi and Donaldson, Alastair F.},
title = {Performance Evaluation of OpenCL Standard Support (and Beyond)},
year = {2019},
isbn = {9781450362306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3318170.3318177},
doi = {10.1145/3318170.3318177},
abstract = {In this talk, we will discuss how support (or lack of it) for various OpenCL (OCL) features affects performance of graph applications executing on GPU platforms. Given that adoption of OCL features varies widely across vendors, our results can help quantify the performance benefits and potentially motivate the timely adoption of these OCL features.Our findings are drawn from the experience of developing an OCL backend for a state-of-the-art graph application DSL, IrGL, originally developed with a CUDA backend [1]. IrGL allows competitive algorithms for applications such as breadth-first-search, page-rank, and single-source-shortest-path to be written at a high level. A series of optimisations can then be applied by the compiler to generate OCL code. These user-selectable optimisations exercise various features of OCL: on one end of the spectrum, applications compiled without optimisations require only core OCL version 1.1 features; on the other end, a certain optimisation requires inter-workgroup forward progress guarantees, which are yet to be officially supported by OCL, but have been empirically validated and are relied upon e.g. to achieve global device-wide synchronisation [3]. Other optimisations require OCL features such as: fine-grained memory consistency guarantees (added in OCL 2.0) and subgroup primitives (added to core in OCL 2.1).Our compiler can apply 6 independent optimisations (Table 1), each of which requires an associated minimum version of OCL to be supported. Increased OCL support enables more and more optimisations: 2 optimisations are supported with OCL 1.x; 1 additional optimization with OCL 2.0; and a further 2 with OCL 2.1. Using OCL FP to denote v2.1 extended with forward progress guarantees (not officially supported at present), the last optimisation is enabled. We will discuss the OCL features required for each optimisation and the idioms in which the features are used. Use-case discussions of these features (e.g. memory consistency and subgroup primitives) are valuable as there appear to be very few open-source examples: a GitHub search yields only a small number of results.Our compiler enables us to carry out a large and controlled study, in which the performance benefit of various levels of OCL support can be evaluated. We gather runtime data exhaustively on all combinations across: all optimisations, 17 applications, 3 graph inputs, 6 different GPUs, spanning 4 vendors: Nvidia, AMD, Intel and ARM (Table 2).We show two notable results in this abstract: our first result, summarised in Figure 1, shows that all optimizations can be beneficial across a range of GPUs, despite significant architectural differences (e.g. subgroup size as seen in Table 2). This provides motivation that previous vendor specific approaches (e.g. for Nvidia) can be ported to OCL and achieve speedups on range of devices.Our second result, summarised in Figure 2, shows that if feature support is limited to OCL 2.0 (or below), the available optimisations (fg wg sz256) fail to achieve any speedups in over 70% of the chip/application/input benchmarks. If support for OCL 2.1 (adding the optimizations: sg coop-cv) is considered, this number drops to 60% but observed speedups are modest, rarely exceeding 2x. Finally, if forward progress guarantees are assumed (adding the oitergb optimization), speedups are observed in over half of the cases, including impressive speedups of over 14x for AMD and Intel GPUs. This provides compelling evidence for forward progress properties to be considered for adoption for a future OCL version.An extended version of this material can be found in [2, ch. 5].},
booktitle = {Proceedings of the International Workshop on OpenCL},
articleno = {8},
numpages = {2},
location = {Boston, MA, USA},
series = {IWOCL'19}
}

@proceedings{10.1145/2676723,
title = {SIGCSE '15: Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to SIGCSE 2015, the 46th ACM Technical Symposium on Computer Science Education. This year's symposium presents the latest advances in computer science education in a variety of formats: papers, posters, panels, special sessions, workshops, birds of a feather meetings, and, new this year, demonstrations and lightning talks. This year's symposium theme is "Keep Connected. Keep Committed. Keep Computing." We are asking our attendees to consider how they can use the conference to keep connected to each other and the field, to keep committed to the cause of computing education, and most fundamentally, to keep computing and to demonstrate to all our students how exciting this field truly is.We are delighted to have Jessica Hodgins of Carnegie Mellon University and Disney Research giving SIGCSE 2015's opening plenary address on Thursday. Her work at Carnegie Mellon and Disney Research will demonstrate what new and exciting things will keep us computing in the coming years. Keith Hampton of Rutgers University is speaking at our Saturday luncheon. His work in social media demonstrates how keeping us connected is an important part of building strong relationships. Finally, it is our pleasure to announce the recipients of the two annual SIGCSE awards, the recipients of which demonstrate keeping committed to the field of computing education and to the SIGCSE organization and community. Frank Young of Rose-Hulman Institute of Technology will receive the SIGCSE Award for Lifetime Service to the Computer Science Education Community, and will speak at our First Timer's Lunch on Thursday. (SIGCSE First Timers will receive their lunch for free. SIGCSE Old Timers are encouraged to purchase a ticket, join us for lunch, meet some First Timers, and recognize Frank's contributions.) Mark Allen Weiss of Florida International University is the recipient of the SIGCSE Award for Outstanding Contributions to Computer Science Education. Mark will give the plenary address on Friday.As noted above, we are very excited to be introducing two new tracks for SIGCSE 2015. A Demos track will be presented in the exhibit hall during breaks, and a session dedicated to Lightning Talks will take place on Friday afternoon at 3:45pm.Symposium statistics are presented in the following table. This year's program includes the usual wide selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition, as well as another puzzle challenge. Our exhibit hall features a number of exhibitors showcasing the latest in hardware, software tools, textbooks and educational programs and research.Proposal type Paper - Accepted - 105 Received - 289 Acceptance Rate - 36%Panel - Accepted - 10 Received - 18 Acceptance Rate - 56%Special Session - Accepted - 13 Received - 25 Acceptance Rate - 52%Workshop - Accepted - 30 Received - 71 Acceptance Rate - 42%Poster - Accepted - 51 Received - 117 Acceptance Rate - 44%Birds of a Feather - Accepted - 38 Received - 55 Acceptance Rate - 69%Demos - Accepted - 10 Received - 32 Acceptance Rate - 31%Lightning Talks - Accepted - 11 Received - 26 Acceptance Rate - 42%We encourage you to participate in our SIGCSE 2015 Pre-symposium Events. As of the publication deadline for this overview, meetings on the following topics will occur on Wednesday: ACM SIGCAS Symposium on Computing for the Social Good: Educational Practices, CSTeachingTips.org: Tip-A-Thon, GENI in your Classroom, Git and GitHub: Foundations for Educators, LittleFe Build-Out, Managing the Academic Career for Women Faculty in Undergraduate Computing Programs, SIGCSE 2015 Department Chairs Roundtable, and Teaching to Diversity in Computer Science.},
location = {Kansas City, Missouri, USA}
}

@inproceedings{10.1145/3225151.3264698,
title = {Outstanding Doctoral Dissertation Award},
year = {2018},
isbn = {9781450358309},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3225151.3264698},
doi = {10.1145/3225151.3264698},
abstract = {Dr. Jun-Yan Zhu is a pioneer in the use of modern machine learning in computer graphics. His dissertation is arguably the first to systematically attack the problem of natural image synthesis using deep neural networks. As such, his work has already had an enormous impact on the field, with several of his contributions, most notably CycleGAN, becoming widely-used tools not just for researchers in computer graphics and beyond, but also for visual artists.A key open problem in data-driven image synthesis is how to make sure that the synthesized image looks realistic, i.e., lies on the manifold of natural images? In Part I of his thesis, Zhu takes a discriminative approach to address a particular instance of this problem, training a classifier to estimate the realism of spliced image composites. Since it is difficult to obtain enough human-labeled training data to learn what looks realistic, he instead learned to classify between real images and automatically-generated composites, whether they look realistic or not. The surprising finding: resulting classifier can actually predict how realistic a new composite would look to a human. Moreover, this realism score can be used to improve the composite realism by iteratively updating the image via a learned transform. This work could be thought of as an early precursor to the conditional Generative Adversarial Network (GAN) architectures. He also developed a similar discriminative learning approach for improving the photograph aesthetics of portraits (SIGAsia'14).In Part II, Zhu takes the opposite, generative approach to modeling natural images and constrains the output of a photo editing tool to lie on this manifold. He built real-time data-driven exploration and editing interfaces based on both classic image averaging models (SIGGRAPH'14) and more recent Generative Adversarial Networks. The latter work and the associated software iGAN was the first use of GANs in a real-time application, and it contributed to the popularization of GANs in the community. In Part III, Zhu combines the lessons learned from his earlier work for developing a novel set of image-to-image translation algorithms. Of particular importance is the CycleGAN framework (ICCV'17), which revolutionized image-based computer graphics as a general-purpose framework for transferring the visual style from one set of images onto another, e.g., translating summer into winter and horses into zebras, generating real photographs from computer graphics renderings, etc. It was the first to show artistic collection style transfer (e.g., using all of Van Gogh paintings instead of only the "Starry Night"), and translating a painting into a photograph. In the short time since CycleGAN was published, it has already been applied to many different problems far beyond computer graphics, from generating synthetic training data (computer vision), to converting MRIs into CT scans (medical imaging), to applications in NLP and speech synthesis. In addition to his dissertation work, he also contributed to learning-based methods for interactive colorization (SIGGRAPH'17) and light field videography (SIGGRAPH'17).Apart from several well-cited papers in top graphics and vision venues, Zhu's work has an impact in other ways as well. His research has been repeatedly featured in the popular press, including New Yorker, Economist, Forbes, Wired, etc. Jun-Yun is also exemplary in facilitating the reproducibility of research and making it easy for researchers and practitioners to build on his contributions. He has open-sourced many of his projects and, as a sign of his impact, he has earned over 22,000 GitHub stars and 1,900 followers. Most impressively, his code has been used widely not just by researchers and developers, but also by visual artists (e.g. see #cycleGAN on Twitter).Bio: Jun-Yan Zhu received his B.E in Computer Sciences from Tsinghua University in 2012. He obtained his Ph.D. in Electrical Engineering and Computer Sciences from UC Berkeley in 2017 supervised by Alexei Efros, after spending five years at CMU and UC Berkeley. His Ph.D. work was supported by a Facebook Fellowship. Jun-Yan is currently a postdoctoral researcher at MIT CSAIL.},
booktitle = {ACM SIGGRAPH 2018 Awards},
articleno = {5},
numpages = {1},
location = {Vancouver, British Columbia, Canada},
series = {SIGGRAPH '18}
}

@inproceedings{10.1145/3381991.3395396,
author = {Enck, William},
title = {Analysis of Access Control Enforcement in Android},
year = {2020},
isbn = {9781450375689},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3381991.3395396},
doi = {10.1145/3381991.3395396},
abstract = {Over the past decade, the Android operating system install-base has proliferated to billions of devices, rivaling Microsoft Windows as a top computing platform. One of the most attractive aspects of Android is its vast collection of applications, available from application stores such as Google's Play Store. Developers have been drawn to Android due to its semantically-rich runtime APIs, which simplify the creation of third-party applications. Many of these APIs provide access to security- and privacy-sensitive information and resources such as the device's geographic location, audio recorded from the device's microphone, and the ability to send and receive SMS messages. In providing these APIs to third-party applications, the Android OS has implicitly taken responsibility for their protection, increasing its access control burden. As a result, current versions of Android have thousands of manually placed access control checks throughout the platform. The goal of this talk is to motivate the need for and utility of semi-automated tools to analyze and validate the access control checks that occur within Android's system code. The challenges are two-fold. First, analysis of Android's middleware code is more challenging than that of third-party applications, which has been studied in-depth over the past decade [3-5]. The code spans hundreds of system services, which are implemented in a combination of Java, C++, and C. The system services also have heavy inter-dependencies with one another, frequently invoking entry points in each other using Android's Binder inter-process communication (IPC) framework within the Linux kernel. Second, identifying what is an access control check is nontrivial. While there are well-known checks based on user-authorized permissions and Linux-layer user and group identifiers, system services also use an array of different service-specific checks that must be captured and modeled to assess the correctness of access control enforcement. In this talk, we will discuss these challenges in the context of two case studies. We will begin by discussing ACMiner [6], a tool designed to assess the correctness of access control checks in Android's middleware using consistency analysis. For each Binder entry point in each system service, ACMiner statically analyzes the code to identify all potential access control checks. To do so, ACMiner uses the names of methods and variables and the values of constant strings used in conditional statements to infer the security-semantics of each check on the control-flow path to instructions that throw a SecurityException. ACMiner then uses association rule mining to identify not only which entry points have inconsistent access control checks, but also to suggest what checks should be added. In applying ACMiner to the Android Open Source Project (AOSP), we found the suggestions to be invaluable when determining whether or not an inconsistency was a vulnerability. Next, we discuss the Android Re-Delegation Finder (ARF) [7]. When designing ACMiner, we optimized our static program analysis by terminating the control-flow analysis of an entry point when the execution reaches another entry point in the same or different system service. Upon further study, we found that entry points frequently call one another, often changing the protection domain of execution when they do (e.g., by explicitly clearing the calling identity, or calling the entry point of a system service executing in a different process). As with most modern operating systems, Android uses deputies (i.e., system services) to safely perform privileged functionality on behalf of third-party applications. Deputies are inherently necessary for the protection of system resources. However, by losing the calling identity, entry points to Android's system services can become confused deputies. ARF builds on the access control policy extracted by ACMiner to identify potential confused deputy vulnerabilities. Neither ACMiner or ARF were designed to eliminate all false positives. In a code-base as vast as Android, it is unrealistic to expect every nuance can be captured programmatically. Instead, ACMiner and ARF were designed to be semi-automated. Our goal is to drastically reduce the amount of time it takes for a security analyst with domain expertise to identify and fix vulnerabilities. Over the course of our research, we have applied our tools to AOSP versions~7, 8, and 9, discovering many vulnerabilities, seven of which have been assigned CVEs by Google. Moving forward, we hope that our tools can be used not only to identify new vulnerabilities, but also to aid regression testing as new versions of Android are released. Both tools have been made open-source and are hosted on Github [1,2].},
booktitle = {Proceedings of the 25th ACM Symposium on Access Control Models and Technologies},
pages = {117–118},
numpages = {2},
keywords = {access control, android security, static program analysis},
location = {Barcelona, Spain},
series = {SACMAT '20}
}

@proceedings{10.1145/3018743,
title = {PPoPP '17: Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {It is our great pleasure to welcome you to PPoPP 2017, the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming held in Austin, Texas during February 4-8, 2017, and co-located with the CGO 2017 and HPCA 2017 conferences. This year's symposium continues and reinforces the PPoPP tradition of publishing leading work on all aspects of parallel programming, including foundational and theoretical aspects, techniques, languages, compilers, runtime systems, tools, and practical experiences. Given the pervasiveness of parallel architectures in the general consumer market, PPoPP, with its interest in new parallel workloads, techniques and productivity tools for parallel programming, is becoming more relevant than ever to the computer science community.PPoPP 2017 received 132 submissions from countries all over the world. The submitted papers underwent a rigorous two-phase review process. To maintain fairness and uniform standards, the review process was double-blind, throughout. Almost all of the 132 submissions were reviewed in the first phase by four members of the combined PC and ERC. (A very small fraction received three reviews in the first phase.) All papers were assigned a discussion lead from the PC. After the first rebuttal phase and ongoing online discussions, reviewers reached a consensus to relegate half of the submissions. Their authors were subsequently notified and given the choice of withdrawing their papers. The submissions for which there was no clear consensus or that had only three reviews (very few) were retained for the second evaluation stage. In the second phase, the remaining papers received at least two additional reviews exclusively from PC members and some external specialists. After a second rebuttal period, PC and ERC members continued their online discussions and grouped papers in top, bottom and discuss categories. Finally, 35 PC members met in person over two half days from noon, November 5th through the afternoon of November 6th at the Department of Computer Science in Rice University, Houston, TX, and concluded the meeting by accepting (or conditionally accepting) a total of 29 papers. The 14 conditionally accepted papers were shepherded by volunteer PC members, and the final version was made available to all original reviewers for their approval. All in all, this process resulted in a manageable average load of 12 papers for PC members, 6 papers for ERC members, offered all authors the possibility to respond to all reviews, and helped ensure that reviewing efforts were focused on where they were needed the most.Because many quality papers could not be accommodated as regular contributions, all papers retained for the second review phase were invited to be presented as posters at the conference. As a result, 17 posters were included in the proceedings as 2 page abstracts. The posters were presented during a special two-hour late afternoon session. All authors of accepted papers were given the option of participating in a joint CGO-PPoPP Artifact Evaluation (AE) process. The AE process is intended to encourage researchers to conduct experiments in a reproducible way, to package experimental work-flows and all related materials for broad availability, and ultimately, to enable fair comparison of published techniques. This year saw a considerable increase in the amount of submitted artifacts: 27 versus 18 two years ago, almost equally split between CGO and PPoPP. The Artifact Evaluation Committee of 41 researchers and engineers spent two weeks validating and evaluating the artifacts. Each submission received at least three reviews and only eight of them fell below acceptance criteria. To help educate authors about result reproducibility, these papers were shepherded during a week's time by the AE Committee. Concurrently, the AE committee successfully tried an "open reviewing model", i.e., asked the community to publicly evaluate several artifacts already available at Github, Gila and other project hosting services. This enabled us to find additional external reviewers with access to HPC servers or proprietary benchmarks and tools. At the end of this process all submissions qualified to receive the AE seal and their authors were encouraged to submit a two page Artifact Appendix to document the process.The success of a major conference like PPoPP very much depends on the hard work of all members of the organizing committee who volunteer their time in this service. We thank all PC and ERC members for their thoughtful reviews and extensive online discussions, with special thanks to the PC members who came from all over the world to the meeting in Houston and deliberated for two half days and read additional papers overnight to produce the PPoPP 2017 program. Several of the PC members volunteered to shepherd papers, and deserve special thanks for their efforts. The AE process was lengthy and demanding, and the AE Chairs, Wonsun Ahn (for PPoPP) and Joe Devietti (for CGO) and their team did an amazing job on that Herculean task.},
location = {Austin, Texas, USA}
}

