@inproceedings{10.1145/3236024.3264585,
author = {Reinhardt, Anastasia and Zhang, Tianyi and Mathur, Mihir and Kim, Miryung},
title = {Augmenting Stack Overflow with API Usage Patterns Mined from GitHub},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264585},
doi = {10.1145/3236024.3264585},
abstract = {Programmers often consult Q&amp;A websites such as Stack Overflow (SO) to learn new APIs. However, online code snippets are not always complete or reliable in terms of API usage. To assess online code snippets, we build a Chrome extension, ExampleCheck that detects API usage violations in SO posts using API usage patterns mined from 380K GitHub projects. It quantifies how many GitHub examples follow common API usage and illustrates how to remedy the detected violation in a given SO snippet. With ExampleCheck, programmers can easily identify the pitfalls of a given SO snippet and learn how much it deviates from common API usage patterns in GitHub. The demo video is at https://youtu.be/WOnN-wQZsH0.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {880–883},
numpages = {4},
keywords = {code assessment, online Q&amp;A forum, API usage pattern},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/2819009.2819163,
author = {Vendome, Christopher},
title = {A Large Scale Study of License Usage on GitHub},
year = {2015},
publisher = {IEEE Press},
abstract = {The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from GitHub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {772–774},
numpages = {3},
keywords = {software licenses, mining software repositories, empirical studies},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3084226.3084259,
author = {Kochhar, Pavneet Singh and Lo, David},
title = {Revisiting Assert Use in GitHub Projects},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084259},
doi = {10.1145/3084226.3084259},
abstract = {Assertions are often used to test the assumptions that developers have about a program. An assertion contains a boolean expression which developers believe to be true at a particular program point. It throws an error if the expression is not satisfied, which helps developers to detect and correct bugs. Since assertions make developer assumptions explicit, assertions are also believed to improve under-standability of code. Recently, Casalnuovo et al. analyse C and C++ programs to understand the relationship between assertion usage and defect occurrence. Their results show that asserts have a small effect on reducing the density of bugs and developers often add asserts to methods they have prior knowledge of and larger ownership. In this study, we perform a partial replication of the above study on a large dataset of Java projects from GitHub (185 projects, 20 million LOC, 4 million commits, 0.2 million files and 1 million methods). We collect metrics such as number of asserts, number of defects, number of developers and number of lines changed to a method, and examine the relationship between asserts and defect occurrence. We also analyse relationship between developer experience and ownership and the number of asserts. Furthermore, we perform a study of what are different types of asserts added and why they are added by developers. We find that asserts have a small yet significant relationship with defect occurrence and developers who have added asserts to methods often have higher ownership of and experience with the methods than developers who did not add asserts.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {298–307},
numpages = {10},
keywords = {Replication Study, Assertions, GitHub},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@inproceedings{10.5555/2818754.2818846,
author = {Casalnuovo, Casey and Devanbu, Prem and Oliveira, Abilio and Filkov, Vladimir and Ray, Baishakhi},
title = {Assert Use in GitHub Projects},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Asserts have long been a strongly recommended (if non-functional) adjunct to programs. They certainly don't add any user-evident feature value; and it can take quite some skill and effort to devise and add useful asserts. However, they are believed to add considerable value to the developer. Certainly, they can help with automated verification; but even in the absence of that, claimed advantages include improved understandability, maintainability, easier fault localization and diagnosis, all eventually leading to better software quality. We focus on this latter claim, and use a large dataset of asserts in C and C++ programs to explore the connection between asserts and defect occurrence. Our data suggests a connection: functions with asserts do have significantly fewer defects. This indicates that asserts do play an important role in software quality; we therefore explored further the factors that play a role in assertion placement: specifically, process factors (such as developer experience and ownership) and product factors, particularly interprocedural factors, exploring how the placement of assertions in functions are influenced by local and global network properties of the callgraph. Finally, we also conduct a differential analysis of assertion use across different application domains.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {755–766},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2788993.2789838,
author = {Longo, Justin and Kelley, Tanya M.},
title = {Use of GitHub as a Platform for Open Collaboration on Text Documents},
year = {2015},
isbn = {9781450336666},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2788993.2789838},
doi = {10.1145/2788993.2789838},
abstract = {Recently, researchers are paying attention to the use of the software development and code-hosting web service GitHub for other collaborative purposes, including a class of activity referred to as document, text, or prose collaboration. These alternative uses of GitHub as a platform for sharing non-code artifacts represent an important modification in the practice of open collaboration. We survey cases where GitHub has been used to facilitate collaboration on non-code outputs, identify its strengths and weaknesses when used in this mode, and propose conditions for successful collaborations on co-created text documents.},
booktitle = {Proceedings of the 11th International Symposium on Open Collaboration},
articleno = {22},
numpages = {2},
keywords = {co-creation, documents, GitHub, open collaboration},
location = {San Francisco, California},
series = {OpenSym '15}
}

@inproceedings{10.1145/2889160.2889195,
author = {Feliciano, Joseph and Storey, Margaret-Anne and Zagalsky, Alexey},
title = {Student Experiences Using GitHub in Software Engineering Courses: A Case Study},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889195},
doi = {10.1145/2889160.2889195},
abstract = {GitHub has been embraced by the software development community as an important social platform for managing software projects and to support collaborative development. More recently, educators have begun to adopt it for hosting course content and student assignments. From our previous research, we found that educators leverage GitHub's collaboration and transparency features to create, reuse and remix course materials, and to encourage student contributions and monitor student activity on assignments and projects. However, our previous research did not consider the student perspective.In this paper, we present a case study where GitHub is used as a learning platform for two software engineering courses. We gathered student perspectives on how the use of GitHub in their courses might benefit them and to identify the challenges they may face. The findings from our case study indicate that software engineering students do benefit from GitHub's transparent and open workflow. However, students were concerned that since GitHub is not inherently an educational tool, it lacks key features important for education and poses learning and privacy concerns. Our findings provide recommendations for designers on how tools such as GitHub can be used to improve software engineering education, and also point to recommendations for instructors on how to use it more effectively in their courses.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {422–431},
numpages = {10},
keywords = {software engineering, collaboration, learning, GitHub, education},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3132498.3134110,
author = {Neto, Casimiro Conde Marco and de O. Barros, M\'{a}rcio},
title = {A Structured Survey on the Usage of the Issue Tracking System Provided by the GitHub Platform},
year = {2017},
isbn = {9781450353250},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132498.3134110},
doi = {10.1145/3132498.3134110},
abstract = {Issue tracking systems help software development teams in identifying problems to be solved and new features to be added to a software system. In this paper, we replicate and extend a study carried out in 2013 on the usage of the issue tracking system provided by the GitHub platform. The replication aims at determining whether the results observed four years ago are still valid. The extension seeks to analyze how often issues are terminated by commits to the version control system and understand whether this feature allows developers to relate an issue to the source code modules that were changed to resolve it. We conclude that the results of the previous study remain valid and that issues closed by commits are uncommon (about 4% of our sample) and often linked to technical aspects of the project.},
booktitle = {Proceedings of the 11th Brazilian Symposium on Software Components, Architectures, and Reuse},
articleno = {3},
numpages = {10},
keywords = {issue tracking systems, issues, issue management, GitHub},
location = {Fortaleza, Cear\'{a}, Brazil},
series = {SBCARS '17}
}

@inproceedings{10.1145/2875913.2875914,
author = {Zhang, Yang and Wang, Huaimin and Yin, Gang and Wang, Tao and Yu, Yue},
title = {Exploring the Use of @-Mention to Assist Software Development in GitHub},
year = {2015},
isbn = {9781450336413},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2875913.2875914},
doi = {10.1145/2875913.2875914},
abstract = {Recently, many researches propose that social media tools can promote the collaboration among developers, which are beneficial to the software development. Nevertheless, there is little empirical evidence to confirm that using @-mention has indeed a beneficial impact on the issues in GitHub. In this paper, we analyze the data from GitHub and give some insights on how @-mention is used in the issues (general-issues and pull-requests). Our statistical results indicate that, @-mention attracts more participants and tends to be used in the difficult issues. @-mention favors the solving process of issues by enlarging the visibility of issues and facilitating the developers' collaboration. In addition to this global study, our study also build a @-network based on the @-mention database we extract. Through the @-network, we can mine the relationships and characteristics of developers in GitHub's issues.},
booktitle = {Proceedings of the 7th Asia-Pacific Symposium on Internetware},
pages = {83–92},
numpages = {10},
keywords = {Social media, Issues, @-mention, GitHub},
location = {Wuhan, China},
series = {Internetware '15}
}

@inproceedings{10.1145/3287324.3293787,
author = {Hu, Zhewei and Gehringer, Edward},
title = {Use Bots to Improve GitHub Pull-Request Feedback},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3293787},
doi = {10.1145/3287324.3293787},
abstract = {Rising enrollments make it difficult for instructors and teaching assistants to give adequate feedback on each student's work. In our software engineering course, we have 50-120 students each semester. Our course projects require students to submit GitHub pull requests as deliverables for their open-source software (OSS) projects. We have set up a static code analyzer and a continuous integration service on GitHub to help students check code style and functionality. However, these tools cannot enforce system-specific customized guidelines and do not explicitly display detailed information. In this study, we discuss how we bypass the limitations of existing tools by implementing three Internet bots. The Expertiza Bot can help detect violations of more than 35 system-specific guidelines. The Travis CI Bot can explicitly display instant test execution results on the GitHub pull-request page. The Code Climate Bot can insert pull-request comments to remind students to fix issues detected by the static code analyzer. These bots are either open source or free for OSS projects, and can be easily integrated with GitHub repositories. Our survey results show that more than 70% of students think the advice given by the bots is useful. We tallied the amount of feedback given by the bots and the teaching staff for each GitHub pull request. Results show that bots can provide significantly more feedback (six times more on average) than teaching staff. Bots can also offer more timely feedback than teaching staff and help student contributions avoid more than 33% system-specific guideline violations.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1262–1263},
numpages = {2},
keywords = {open-source software, software engineering, expertiza, open-source curriculum, internet bots},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1109/ICPC.2015.32,
author = {Vendome, Christopher and Linares-Vasquez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel and Poshyvanyk, Denys},
title = {License Usage and Changes: A Large-Scale Study of Java Projects on GitHub},
year = {2015},
isbn = {9781467381598},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICPC.2015.32},
doi = {10.1109/ICPC.2015.32},
abstract = {Software licenses determine, from a legal point of view, under which conditions software can be integrated, used, and above all, redistributed. Licenses evolve over time to meet the needs of development communities and to cope with emerging legal issues and new development paradigms. Such evolution of licenses is likely to be accompanied by changes in the way how software uses such licenses, resulting in some licenses being adopted while others are abandoned. This paper reports a large empirical study aimed at quantitatively and qualitatively investigating when and why developer change software licenses. Specifically, we first identify licenses' changes in 1,731,828 commits, representing the entire history of 16,221 Java projects hosted on GitHub. Then, to understand the rationale of license changes, we perform a qualitative analysis -- following a grounded theory approach -- of commit notes and issue tracker discussions concerning licensing topics and, whenever possible, try to build trace ability links between discussions and changes. Our results point out a lack of trace ability of when and why licensing changes are made. This can be a major concern, because a change in the license of a system can negatively impact those that reuse it.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {218–228},
numpages = {11},
keywords = {Empirical Studies, Mining Software Repositories, Software Licenses},
series = {ICPC '15}
}

@inproceedings{10.5555/2820282.2820314,
author = {Vendome, Christopher and Linares-V\'{a}squez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel and Poshyvanyk, Denys},
title = {License Usage and Changes: A Large-Scale Study of Java Projects on GitHub},
year = {2015},
publisher = {IEEE Press},
abstract = {Software licenses determine, from a legal point of view, under which conditions software can be integrated, used, and above all, redistributed. Licenses evolve over time to meet the needs of development communities and to cope with emerging legal issues and new development paradigms. Such evolution of licenses is likely to be accompanied by changes in the way how software uses such licenses, resulting in some licenses being adopted while others are abandoned. This paper reports a large empirical study aimed at quantitatively and qualitatively investigating when and why developer change software licenses. Specifically, we first identify licenses' changes in 1,731,828 commits, representing the entire history of 16,221 Java projects hosted on GitHub. Then, to understand the rationale of license changes, we perform a qualitative analysis---following a grounded theory approach---of commit notes and issue tracker discussions concerning licensing topics and, whenever possible, try to build traceability links between discussions and changes. Our results point out a lack of traceability of when and why licensing changes are made. This can be a major concern, because a change in the license of a system can negatively impact those that reuse it.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {218–228},
numpages = {11},
keywords = {software licenses, empirical studies, mining software repositories},
location = {Florence, Italy},
series = {ICPC '15}
}

@inproceedings{10.1145/2468356.2468381,
author = {Lee, Michael J. and Ferwerda, Bruce and Choi, Junghong and Hahn, Jungpil and Moon, Jae Yun and Kim, Jinwoo},
title = {GitHub Developers Use Rockstars to Overcome Overflow of News},
year = {2013},
isbn = {9781450319522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2468356.2468381},
doi = {10.1145/2468356.2468381},
abstract = {Keeping track of a constantly updating stream of news items on social networking enabled software development sites may be difficult. We analyzed the actions of 544 GitHub.com developers working across 5,657 projects to examine how the network of developers and projects influence where developers choose to contribute. Our analyses revealed the existence of a group of extremely well connected developers, or rockstars. We found that these rockstars': 1) actions have a greater influence on their followers compared to regular developers, 2) type of action affect their followers differently, 3) influence on followers may depend on a project's age, 4) increased activity on a project increases activity by followers, and 5) followers use as guides to projects to work on. We discuss the implications of these findings to the design of software development environments.},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
pages = {133–138},
numpages = {6},
keywords = {social computing, GitHub, social coding, open source},
location = {Paris, France},
series = {CHI EA '13}
}

@inproceedings{10.5555/2818754.2818825,
author = {Kalliamvakou, Eirini and Damian, Daniela and Blincoe, Kelly and Singer, Leif and German, Daniel M.},
title = {Open Source-Style Collaborative Development Practices in Commercial Projects Using GitHub},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mechanism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (oss) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of oss projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {574–585},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3287324.3287460,
author = {Hsing, Courtney and Gennarelli, Vanessa},
title = {Using GitHub in the Classroom Predicts Student Learning Outcomes and Classroom Experiences: Findings from a Survey of Students and Teachers},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287460},
doi = {10.1145/3287324.3287460},
abstract = {GitHub is a widely-used software development platform that supports version control, collaborative development, and project hosting. Currently, an estimated 18,000 educators use GitHub in programming classrooms. Depending on how GitHub is implemented in the classroom, students may rely on GitHub for activities such as, submitting assignments, collaborating on group projects, and receiving feedback. Despite GitHub's growing presence in programming classrooms, to date, few studies have explored how GitHub and the design of its implementation shape students' learning outcomes and classroom experiences. Building on previous research, we investigated how students in classrooms that used GitHub (GitHub classrooms), as opposed to classrooms that did not use GitHub (non-GitHub classrooms), differed across key variables. We surveyed 7530 students and 300 educators from GitHub and non-GitHub classrooms. Overall, we found that using GitHub in the classroom predicted better learning outcomes and classroom experiences. For example, students felt more prepared for the future, and they felt a greater sense of belonging in the classroom and in the field. Importantly, the design of implementation affected learning outcomes. For example, of the students who used GitHub in the classroom and received instructor feedback, those who received (versus did not receive) feedback via GitHub benefited more from the feedback. We discuss best practices for maximizing benefits to student learning when implementing GitHub in the classroom, study limitations, and future research directions. Our research is a step towards understanding how GitHub, a tool with a growing presence in programming classrooms, impacts students' learning experiences.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {672–678},
numpages = {7},
keywords = {github, learning outcomes, education},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/3059009.3072984,
author = {Arcelli Fontana, Francesca and Raibulet, Claudia},
title = {Students' Feedback in Using GitHub in a Project Development for a Software Engineering Course},
year = {2017},
isbn = {9781450347044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3059009.3072984},
doi = {10.1145/3059009.3072984},
abstract = {GitHub is a platform used for the development of software projects. It provides a traceable project repository and a social meeting place for communities of practices. This poster presents the students' feedback on using GitHub as a development platform for software projects counting as an exam for a 3rd-year undergraduate software engineering course on software design. Students worked in teams and their feedback is positive overall.},
booktitle = {Proceedings of the 2017 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {380},
numpages = {1},
keywords = {project development, software engineering course, github, students' feedback},
location = {Bologna, Italy},
series = {ITiCSE '17}
}

@inproceedings{10.1145/3350768.3350788,
author = {Borges, Hudson and Brito, Rodrigo and Valente, Marco Tulio},
title = {Beyond Textual Issues: Understanding the Usage and Impact of GitHub Reactions},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350788},
doi = {10.1145/3350768.3350788},
abstract = {Recently, GitHub introduced a new social feature, named reactions, which are "pictorial characters" similar to emoji symbols widely used nowadays in text-based communications. Particularly, GitHub users can use a pre-defined set of such symbols to react to issues and pull requests. However, little is known about the real usage and impact of GitHub reactions. In this paper, we analyze the reactions provided by developers to more than 2.5 million issues and 9.7 million issue comments, in order to answer an extensive list of nine research questions about the usage and adoption of reactions. We show that reactions are being increasingly used by open source developers. Moreover, we also found that issues with reactions usually take more time to be handled and have longer discussions.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {397–406},
numpages = {10},
keywords = {Reactions, Social Coding, GitHub},
location = {Salvador, Brazil},
series = {SBES 2019}
}

@inproceedings{10.5555/2820518.2820544,
author = {Blincoe, Kelly and Harrison, Francis and Damian, Daniela},
title = {Ecosystems in GitHub and a Method for Ecosystem Identification Using Reference Coupling},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Software projects are not developed in isolation. Recent research has shifted to studying software ecosystems, communities of projects that depend on each other and are developed together. However, identifying technical dependencies at the ecosystem level can be challenging. In this paper, we propose a new method, known as reference coupling, for detecting technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We use our method to identify ecosystems in GitHub-hosted projects, and we identify several characteristics of the identified ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. The predominant type of ecosystems are those that develop tools to support software development. We also found that the project owners' social behaviour aligns well with the technical dependencies within the ecosystem, but project contributors' social behaviour does not align with these dependencies. We conclude with a discussion on future research that is enabled by our reference coupling method.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {202–207},
numpages = {6},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/2976767.2976778,
author = {Hebig, Regina and Quang, Truong Ho and Chaudron, Michel R. V. and Robles, Gregorio and Fernandez, Miguel Angel},
title = {The Quest for Open Source Projects That Use UML: Mining GitHub},
year = {2016},
isbn = {9781450343213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2976767.2976778},
doi = {10.1145/2976767.2976778},
abstract = {Context: While industrial use of UML was studied intensely, little is known about UML use in Free/Open Source Software (FOSS) projects. Goal: We aim at systematically mining GitHub projects to answer the question when models, if used, are created and updated throughout the whole project's life-span. Method: We present a semi-automated approach to collect UML stored in images, .xmi, and .uml files and scanned ten percent of all GitHub projects (1.24 million). Our focus was on number and role of contributors that created/updated models and the time span during which this happened. Results: We identified and studied 21 316 UML diagrams within 3 295 projects. Conclusion: Creating/updating of UML happens most often during a very short phase at the project start. For 12% of the models duplicates were found, which are in average spread across 1.88 projects. Finally, we contribute a list of GitHub projects that include UML files.},
booktitle = {Proceedings of the ACM/IEEE 19th International Conference on Model Driven Engineering Languages and Systems},
pages = {173–183},
numpages = {11},
keywords = {UML, open source, GitHub, mining software repositories, free software},
location = {Saint-malo, France},
series = {MODELS '16}
}

@inproceedings{10.1145/2652524.2652564,
author = {Zhu, Jiaxin and Zhou, Minghui and Mockus, Audris},
title = {Patterns of Folder Use and Project Popularity: A Case Study of Github Repositories},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652564},
doi = {10.1145/2652524.2652564},
abstract = {Context: Every software development project uses folders to organize software artifacts. Goal: We would like to understand how folders are used and what ramifications different uses may have. Method: In this paper we study the frequency of folders used by 140k Github projects and use regression analysis to model how folder use is related to project popularity, i.e., the extent of forking. Results: We find that the standard folders, such as document, testing, and examples, are not only among the most frequently used, but their presence in a project is associated with increased chances that a project's code will be forked (i.e., used by others) and an increased number of forks. Conclusions: This preliminary study of folder use suggests opportunities to quantify (and improve) file organization practices based on folder use patterns of large collections of repositories.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {30},
numpages = {4},
keywords = {mining software repository, project popularity, folder use},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1145/2901739.2901751,
author = {Kikas, Riivo and Dumas, Marlon and Pfahl, Dietmar},
title = {Using Dynamic and Contextual Features to Predict Issue Lifetime in GitHub Projects},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901751},
doi = {10.1145/2901739.2901751},
abstract = {Methods for predicting issue lifetime can help software project managers to prioritize issues and allocate resources accordingly. Previous studies on issue lifetime prediction have focused on models built from static features, meaning features calculated at one snapshot of the issue's lifetime based on data associated to the issue itself. However, during its lifetime, an issue typically receives comments from various stakeholders, which may carry valuable insights into its perceived priority and difficulty and may thus be exploited to update lifetime predictions. Moreover, the lifetime of an issue depends not only on characteristics of the issue itself, but also on the state of the project as a whole. Hence, issue lifetime prediction may benefit from taking into account features capturing the issue's context (contextual features). In this work, we analyze issues from more than 4000 GitHub projects and build models to predict, at different points in an issue's lifetime, whether or not the issue will close within a given calendric period, by combining static, dynamic and contextual features. The results show that dynamic and contextual features complement the predictive power of static ones, particularly for long-term predictions.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {291–302},
numpages = {12},
keywords = {issue lifetime prediction, mining software repositories, issue tracking},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3306446.3340820,
author = {Gustavsson, Henrik and Brohede, Marcus},
title = {Continuous Assessment in Software Engineering Project Course Using Publicly Available Data from GitHub},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340820},
doi = {10.1145/3306446.3340820},
abstract = {This paper describes an approach for assessment in a large software engineering project course. We propose an approach for continuously collecting information from a source code repository and collaboration tool, and using this information for assessing student contributions and also for assessing the course as a whole from the teacher's standpoint. We present how we display metrics for how the students perform in relation to some of the requirements of the course. We argue that continuous summative assessment feedback to the students on how they are performing in the project is a suitable strategy for ensuring active participation from the students for the duration of the project course.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {4},
numpages = {6},
keywords = {assessment, education, issue management, project course, software engineering},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.5555/2820518.2820556,
author = {Wang, Weiliang and Poo-Caama\~{n}o, Germ\'{a}n and Wilde, Evan and German, Daniel M.},
title = {What is the Gist? Understanding the Use of Public Gists on GitHub},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {GitHub is a popular source code hosting site which serves as a collaborative coding platform. The many features of GitHub have greatly facilitated developers' collaboration, communication, and coordination. Gists are one feature of GitHub, which defines them as "a simple way to share snippets and pastes with others." This three-part study explores how users are using Gists. The first part is a quantitative analysis of Gist metadata and contents. The second part investigates the information contained in a Gist: We sampled 750k users and their Gists (totalling 762k Gists), then manually categorized the contents of 398. The third part of the study investigates what users are saying Gists are for by reading the contents of web pages and twitter feeds. The results indicate that Gists are used by a small portion of GitHub users, and those that use them typically only have a few. We found that Gists are usually small and composed of a single file. However, Gists serve a wide variety of uses, from saving snippets of code, to creating reusable components for web pages.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {314–323},
numpages = {10},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3304221.3319784,
author = {Lars\'{e}n, Simon and Glassey, Richard},
title = {RepoBee: Developing Tool Support for Courses Using Git/GitHub},
year = {2019},
isbn = {9781450368957},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3304221.3319784},
doi = {10.1145/3304221.3319784},
abstract = {The use of version control systems within computing education is growing in popularity. However, this is challenging because such systems are not particularly well designed to support educational situations, nor are they easy to use with confidence in teaching, as specialist knowledge and experience is required. This experience paper reports the development of the open source tool RepoBee, which assists in the use of Git/GitHub in an educational context. The tool provides a straightforward interface for managing batch tasks such as repository generation and cloning for setting and gathering assignments, opening and closing of issues to communicate with students, as well as facilitating peer reviews. Parts of RepoBee are open to integration with third party tools for additional tasks, such as running unit tests or static analysis on student repositories. We also include the perspectives of both teachers and teaching assistants who have been using the tool as part of a first year course for computer scientists.},
booktitle = {Proceedings of the 2019 ACM Conference on Innovation and Technology in Computer Science Education},
pages = {534–540},
numpages = {7},
keywords = {version control systems, git/github, computing education, course management},
location = {Aberdeen, Scotland Uk},
series = {ITiCSE '19}
}

@inproceedings{10.1109/ICSE.2019.00060,
author = {Kavaler, David and Trockman, Asher and Vasilescu, Bogdan and Filkov, Vladimir},
title = {Tool Choice Matters: JavaScript Quality Assurance Tools and Usage Outcomes in GitHub Projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00060},
doi = {10.1109/ICSE.2019.00060},
abstract = {Quality assurance automation is essential in modern software development. In practice, this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context. Data and analytics of the pros and cons can inform these decisions. Yet, in most cases, there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices.We propose a general methodology to model the time-dependent effect of automation tool choice on four outcomes of interest: prevalence of issues, code churn, number of pull requests, and number of contributors, all with a multitude of controls. On a large data set of npm JavaScript projects, we extract the adoption events for popular tools in three task classes: linters, dependency managers, and coverage reporters. Using mixed methods approaches, we study the reasons for the adoptions and compare the adoption effects within each class, and sequential tool adoptions across classes. We find that some tools within each group are associated with more beneficial outcomes than others, providing an empirical perspective for the benefits of each. We also find that the order in which some tools are implemented is associated with varying outcomes.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {476–487},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2931037.2931073,
author = {Chapman, Carl and Stolee, Kathryn T.},
title = {Exploring Regular Expression Usage and Context in Python},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931073},
doi = {10.1145/2931037.2931073},
abstract = { Due to the popularity and pervasive use of regular expressions, researchers have created tools to support their creation, validation, and use. However, little is known about the context in which regular expressions are used, the features that are most common, and how behaviorally similar regular expressions are to one another.  In this paper, we explore the context in which regular expressions are used through a combination of developer surveys and repository analysis. We survey 18 professional developers about their regular expression usage and pain points. Then, we analyze nearly 4,000 open source Python projects from GitHub and extract nearly 14,000 unique regular expression patterns. We map the most common features used in regular expressions to those features supported by four major regex research efforts from industry and academia: brics, Hampi, RE2, and Rex. Using similarity analysis of regular expressions across projects, we identify six common behavioral clusters that describe how regular expressions are often used in practice. This is the first rigorous examination of regex usage and it provides empirical evidence to support design decisions by regex tool builders. It also points to areas of needed future work, such as refactoring regular expressions to increase regex understandability and context-specific tool support for common regex usages. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {282–293},
numpages = {12},
keywords = {regular expressions, developer survey, repository analysis},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.5555/2820518.2820599,
author = {Sawant, Anand Ashok and Bacchelli, Alberto},
title = {A Dataset for API Usage},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {An Application Programming Interface (API) provides a specific set of functionalities to a developer. The main aim of an API is to encourage the reuse of already existing functionality. There has been some work done into API popularity trends, API evolution and API usage. For all the aforementioned research avenues there has been a need to mine the usage of an API in order to perform any kind of analysis. Each one of the approaches that has been employed in the past involved a certain degree of inaccuracy as there was no type check that takes place. We introduce an approach that takes type information into account while mining API method invocations and annotation usages. This approach accurately makes a connection between a method invocation and the class of the API to which the method belongs to. We try collecting as many usages of an API as possible, this is achieved by targeting projects hosted on GitHub. Additionally, we look at the history of every project to collect the usage of an API from earliest version onwards. By making such a large and rich dataset public, we hope to stimulate some more research in the field of APIs with the aid of accurate API usage samples.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {506–509},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1109/ICSM.2013.18,
author = {McDonnell, Tyler and Ray, Baishakhi and Kim, Miryung},
title = {An Empirical Study of API Stability and Adoption in the Android Ecosystem},
year = {2013},
isbn = {9780769549811},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2013.18},
doi = {10.1109/ICSM.2013.18},
abstract = {When APIs evolve, clients make corresponding changes to their applications to utilize new or updated APIs. Despite the benefits of new or updated APIs, developers are often slow to adopt the new APIs. As a first step toward understanding the impact of API evolution on software ecosystems, we conduct an in-depth case study of the co-evolution behavior of Android API and dependent applications using the version history data found in github. Our study confirms that Android is evolving fast at a rate of 115 API updates per month on average. Client adoption, however, is not catching up with the pace of API evolution. About 28% of API references in client applications are outdated with a median lagging time of 16 months. 22% of outdated API usages eventually upgrade to use newer API versions, but the propagation time is about 14 months, much slower than the average API release interval (3 months). Fast evolving APIs are used more by clients than slow evolving APIs but the average time taken to adopt new versions is longer for fast evolving APIs. Further, API usage adaptation code is more defect prone than the one without API usage adaptation. This may indicate that developers avoid API instability.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Software Maintenance},
pages = {70–79},
numpages = {10},
series = {ICSM '13}
}

@inproceedings{10.1109/MSR.2019.00086,
author = {Biswas, Sumon and Islam, Md Johirul and Huang, Yijia and Rajan, Hridesh},
title = {Boa Meets Python: A Boa Dataset of Data Science Software in Python Language},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00086},
doi = {10.1109/MSR.2019.00086},
abstract = {The popularity of Python programming language has surged in recent years due to its increasing usage in Data Science. The availability of Python repositories in Github presents an opportunity for mining software repository research, e.g., suggesting the best practices in developing Data Science applications, identifying bug-patterns, recommending code enhancements, etc. To enable this research, we have created a new dataset that includes 1,558 mature Github projects that develop Python software for Data Science tasks. By analyzing the metadata and code, we have included the projects in our dataset which use a diverse set of machine learning libraries and managed by a variety of users and organizations. The dataset is made publicly available through Boa infrastructure both as a collection of raw projects as well as in a processed form that could be used for performing large scale analysis using Boa language. We also present two initial applications to demonstrate the potential of the dataset that could be leveraged by the community.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {577–581},
numpages = {5},
keywords = {AST, Boa, MSR, machine learning, data science, open source repositories, program analysis},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.5555/3370272.3370301,
author = {Grichi, Manel and Abidi, Mouna and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l and Khomh, Foutse},
title = {State of Practices of Java Native Interface},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {The use of the Java Native Interface (JNI) allows taking advantage of the existing libraries written in different programming languages for code reuse, performance, and security. Despite the importance of JNI in development, practices on its usages are not well studied yet. In this paper, we investigated the usage of JNI in 100 open source systems collected from OpenHub and Github, around 8k of source code files combined between Java and C/C++, including the Java class libraries part of the JDK v9. We identified the state of the practice in JNI systems by semi-automatically and manually analyzing the source code.Our qualitative analysis shows eleven JNI practices where they are mainly related to loading libraries, implementing native methods, exception management, return types, and local/global references management. Basing on our findings, we provided some suggestions and recommendations to developers to facilitate the debugging tasks of JNI in multi-language systems, which can also help them to deal with the Java and C memory.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {274–283},
numpages = {10},
keywords = {multi-language systems, practices, Java development kit, Java native interface, usages, code analysis},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/3383219.3383255,
author = {Rao, A. Eashaan and Chimalakonda, Sridhar},
title = {An Exploratory Study Towards Understanding Lambda Expressions in Python},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383255},
doi = {10.1145/3383219.3383255},
abstract = {Lambda expressions are anonymous functions in Python. It is one of the alternatives to write a function definition. Syntactically, it is a single expression and defined using the keyword lambda. Lambda expression is a functional programming feature, currently in use, in many mainstream programming languages such as Python, Java8, C++11. There are few studies in C++ and Java to understand the impact of lambda expressions on programmers. These studies are focusing on the developer's adaptability to use a functional style of construct and the benefit they gain from using it. However, we are not aware of any literature on the use of lambda expressions in Python. Thus, there is a need to study lambda expressions in Python projects. In this paper, we examine 15 GitHub repositories out of 760 from our dataset, that are using Python as their primary language. In this study, we are classifying the uses of lambda expressions based on varying scenarios. We identified 13 different usages of lambda expressions from these Python repositories. This catalog is an attempt to support programmers to use lambda expressions more effectively and efficiently.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {318–323},
numpages = {6},
keywords = {Lambda Expressions, Empirical Study, Python, Programming Language Constructs},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1109/ICSE-C.2017.164,
author = {Gousios, Georgios and Spinellis, Diomidis},
title = {Mining Software Engineering Data from GitHub},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.164},
doi = {10.1109/ICSE-C.2017.164},
abstract = {GitHub is the largest collaborative source code hosting site built on top of the Git version control system. The availability of a comprehensive API has made GitHub a target for many software engineering and online collaboration research efforts. In our work, we have discovered that a) obtaining data from GitHub is not trivial, b) the data may not be suitable for all types of research, and c) improper use can lead to biased results. In this tutorial, we analyze how data from GitHub can be used for large-scale, quantitative research, while avoiding common pitfalls. We use the GHTorrent dataset, a queryable offline mirror of the GitHub API data, to draw examples from and present pitfall avoidance strategies.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {501–502},
numpages = {2},
keywords = {empirical software engineering, git, GHTorrent, GitHub},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.5220/0006367402930300,
author = {Roveda, Riccardo and Arcelli Fontana, Francesca and Raibulet, Claudia and Zanoni, Marco and Rampazzo, Federico},
title = {Does the Migration to GitHub Relate to Internal Software Quality?},
year = {2017},
isbn = {9789897582509},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0006367402930300},
doi = {10.5220/0006367402930300},
abstract = {Software development is more and more influenced by the usage of FLOSS (Free, Libre and Open Source Software) projects. These software projects are developed in web collaborative environments hosted on web platforms, called code forges. Many code forges exist, with different capabilities. Github is perhaps the largest code forge available, and many projects have been migrated from different code forges to Github. Given its success, we want to understand if its adoption has effect on the projects' internal quality. To consider objective measures of internal quality, we apply four known tools performing static analysis to extract metrics and code anomalies. These data is extracted on six versions of six FLOSS projects, and compared to understand if the migration to Github had any consistent effect over any of the considered measures.},
booktitle = {Proceedings of the 12th International Conference on Evaluation of Novel Approaches to Software Engineering},
pages = {293–300},
numpages = {8},
keywords = {Quality Metrics, Migration To GitHub, Trend Analysis., Code Smells},
location = {Porto, Portugal},
series = {ENASE 2017}
}

@inproceedings{10.1145/3383583.3398578,
author = {F\"{a}rber, Michael},
title = {Analyzing the GitHub Repositories of Research Papers},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398578},
doi = {10.1145/3383583.3398578},
abstract = {Linking to code repositories, such as on GitHub, in scientific papers becomes increasingly common in the field of computer science. The actual quality and usage of these repositories are, however, to a large degree unknown so far. In this paper, we present for the first time a thorough analysis of all GitHub code repositories linked in scientific papers using the Microsoft Academic Graph as a data source. We analyze the repositories and their associated papers with respect to various dimensions. We observe that the number of stars and forks, respectively, over all repositories follows a power-law distribution. In the majority of cases, only one person from the authors is contributing to the repository. The GitHub manuals are mostly kept rather short with few sentences. The source code is mostly provided in Python. The papers containing the repository URLs as well as the papers' authors are typically from the AI field.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {491–492},
numpages = {2},
keywords = {GitHub, code repositories, bibliometrics, scholarly data, papers},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1109/ICSE.2019.00109,
author = {Nguyen, Phuong T. and Di Rocco, Juri and Di Ruscio, Davide and Ochoa, Lina and Degueule, Thomas and Di Penta, Massimiliano},
title = {FOCUS: A Recommender System for Mining API Function Calls and Usage Patterns},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00109},
doi = {10.1109/ICSE.2019.00109},
abstract = {Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate, accuracy, and execution time. Results indicate the suitability of context-aware collaborative-filtering recommender systems to provide API usage patterns.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1050–1060},
numpages = {11},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/2623201.2623203,
author = {Weicheng, Yang and Beijun, Shen and Ben, Xu},
title = {Mining GitHub: Why Commit Stops -- Exploring the Relationship between Developer's Commit Pattern and File Version Evolution},
year = {2013},
isbn = {9781479921447},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Using the freeware in GitHub, we are often confused by a phenomenon: the new version of GitHub freeware usually was released in an indefinite frequency, and developers often committed nothing for a long time. This evolution phenomenon interferes with our own development plan and architecture design. Why do these updates happen at that time? Can we predict GitHub software version evolution by developers' activities? This paper aims to explore the developer commit patterns in GitHub, and try to mine the relationship between these patterns (if exists) and code evolution. First, we define four metrics to measure commit activity and code evolution: the changes in each commit, the time between two commits, the author of each changes, and the source code dependency. Then, we adopt visualization techniques to explore developers' commit activity and code evolution. Visual techniques are used to describe the progress of the given project and the authors' contributions. To analyze the commit logs in GitHub software repository automatically, Commits Analysis Tool (CAT) is designed and implemented. Finally, eight open source projects in GitHub are analyzed using CAT, and we find that: 1) the file changes in the previous versions may affect the file depend on it in the next version, 2) the average days around "huge commit" is 3 times of that around normal commit. Using these two patterns and developer's commit model, we can predict when his next commit comes and which file may be changed in that commit. Such information is valuable for project planning of both GitHub projects and other projects which use GitHub freeware to develop software.},
booktitle = {Proceedings of the 2013 20th Asia-Pacific Software Engineering Conference (APSEC) - Volume 02},
pages = {165–169},
numpages = {5},
keywords = {repository mining, commit pattern, visualization technology, version evolution, GitHub},
series = {APSEC '13}
}

@inproceedings{10.1145/3379597.3387466,
author = {Fang, Hongbo and Klug, Daniel and Lamba, Hemank and Herbsleb, James and Vasilescu, Bogdan},
title = {Need for Tweet: How Open Source Developers Talk About Their GitHub Work on Twitter},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387466},
doi = {10.1145/3379597.3387466},
abstract = {Social media, especially Twitter, has always been a part of the professional lives of software developers, with prior work reporting on a diversity of usage scenarios, including sharing information, staying current, and promoting one's work. However, previous studies of Twitter use by software developers typically lack information about activities of the study subjects (and their outcomes) on other platforms. To enable such future research, in this paper we propose a computational approach to cross-link users across Twitter and GitHub, revealing (at least) 70,427 users active on both. As a preliminary analysis of this dataset, we report on a case study of 786 tweets by open-source developers about GitHub work, combining automatic characterization of tweet authors in terms of their relationship to the GitHub items linked in their tweets with qualitative analysis of the tweet contents. We find that different developer roles tend to have different tweeting behaviors, with repository owners being perhaps the most distinctive group compared to other project contributors and followers. We also note a sizeable group of people who follow others on GitHub and tweet about these people's work, but do not otherwise contribute to those open-source projects. Our results and public dataset open up multiple future research directions.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {322–326},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3272973.3274083,
author = {Pe-Than, Ei Pa Pa and Dabbish, Laura and Herbsleb, James D.},
title = {Collaborative Writing on GitHub: A Case Study of a Book Project},
year = {2018},
isbn = {9781450360180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3272973.3274083},
doi = {10.1145/3272973.3274083},
abstract = {Social coding platforms such as GitHub are increasingly becoming a digital workspace for the production of non-software digital artifacts. Since GitHub offers unique features that are different from traditional ways of collaborative writing, it is interesting to investigate how GitHub features are used for writing. In this paper, we present the preliminary findings of a mixed-methods, case study of collaboration practices in a GitHub book project. We found that the use of GitHub depended on task interdependence and audience participation. GitHub's direct push method was used to coordinate both loosely- and tightly-coupled work, with the latter requiring collaborators to follow socially-accepted conventions. The pull-based method was adopted once the project was released to the public. While face-to-face and online meetings were prominent in the early phases, GitHub's issues became instrumental for communication and project management in later phases. Our findings have implications for the design of collaborative writing tools.},
booktitle = {Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing},
pages = {305–308},
numpages = {4},
keywords = {collaborative writing, co-creation, coordination, social computing, collaboration, mixed methods research},
location = {Jersey City, NJ, USA},
series = {CSCW '18}
}

@inproceedings{10.1145/3380868.3398200,
author = {Chang, Christopher H. and Carpenter, Ilene L. and Jones, Wesley B.},
title = {The ESIF-HPC-2 Benchmark Suite},
year = {2020},
isbn = {9781450375368},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3380868.3398200},
doi = {10.1145/3380868.3398200},
abstract = {We describe the development of the ESIF-HPC-2 benchmark suite, a collection of kernel and application benchmark codes for measuring computational and I/O performance from single nodes to full HPC systems that was used for acceptance testing in our recent HPC procurement. The configurations of the benchmarks used for our system is presented. We also describe a set of "dimensions" that can be used to classify benchmarks and assess coverage of a suite systematically. The collection is offered cost-free as a GitHub repository for general usage and further development.},
booktitle = {Proceedings of the Workshop on Benchmarking in the Datacenter},
articleno = {2},
numpages = {8},
keywords = {procurement, benchmark, HPC},
location = {San Diego, California},
series = {BID '20}
}

@inproceedings{10.1145/2591062.2591115,
author = {Padhye, Rohan and Mukherjee, Debdoot and Sinha, Vibha Singhal},
title = {API as a Social Glue},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591115},
doi = {10.1145/2591062.2591115},
abstract = { The rapid growth of social platforms such as Facebook, Twitter and LinkedIn underscores the need for people to connect to existing and new contacts for recreational and professional purposes. A parallel of this phenomenon exists in the software development arena as well. Open-source code sharing platforms such as GitHub provide the ability to follow people and projects of interest. However, users are manually required to identify projects or other users whom they might be interested in following. We observe that most software projects use third-party libraries and that developers who contribute to multiple projects often use the same library APIs across projects. Thus, the library APIs seem to be a good fingerprint of their skill set. Hence, we argue that library APIs can form the social glue to connect people and projects having similar interests. We propose APINet, a system that mines API usage profiles from source code version management systems and create a social network of people, projects and libraries. We describe our initial implementation that uses data from 568 open-source projects hosted on GitHub. Our system recommends to a user new projects and people that they may be interested in, suggests communities of people who use related libraries and finds experts for a given topic who are closest in a user's social graph. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {516–519},
numpages = {4},
keywords = {social networks, recommender systems, Mining software repositories, usage expertise},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2597073.2597074,
author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
title = {The Promises and Perils of Mining GitHub},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597074},
doi = {10.1145/2597073.2597074},
abstract = { With over 10 million git repositories, GitHub is becoming one of the most important source of software artifacts on the Internet. Researchers are starting to mine the information stored in GitHub's event logs, trying to understand how its users employ the site to collaborate on software. However, so far there have been no studies describing the quality and properties of the data available from GitHub. We document the results of an empirical study aimed at understanding the characteristics of the repositories in GitHub and how users take advantage of GitHub's main features---namely commits, pull requests, and issues. Our results indicate that, while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. We show, for example, that the majority of the projects are personal and inactive; that GitHub is also being used for free storage and as a Web hosting service; and that almost 40% of all pull requests do not appear as merged, even though they were. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {92–101},
numpages = {10},
keywords = {code reviews, git, github, bias, Mining software repositories},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1109/ASE.2019.00109,
author = {Escobar-Velasquez, Camilo and Osorio-Ria\~{n}o, Michael and Linares-V\'{a}squez, Mario},
title = {MutAPK: Source-Codeless Mutant Generation for Android Apps},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00109},
doi = {10.1109/ASE.2019.00109},
abstract = {The amount of Android application is having a tremendous increasing trend, exerting pressure over practitioners and researchers around application quality, frequent releases, and quick fixing of bugs. This pressure leads practitioners to make usage of automated approaches based on using source-code as input. Nevertheless, third-party services are not able to use these approaches due to privacy factors. In this paper we present MutAPK, an open source mutation testing tool that enables the usage of Android Application Packages (APKs) as input for this task. MutAPK generates mutants without the need of having access to source code, because the mutations are done in an intermediate representation of the code (i.e., SMALI) that does not require compilation. MutAPK is publicly available at GitHub: https://bit.ly/2KYvgP9 VIDEO: https://bit.ly/2WOjiyy},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1090–1093},
numpages = {4},
keywords = {closed-source apps, mutation testing, Android},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1109/WAINA.2014.110,
author = {Rusk, David and Coady, Yvonne},
title = {Location-Based Analysis of Developers and Technologies on GitHub},
year = {2014},
isbn = {9781479926534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WAINA.2014.110},
doi = {10.1109/WAINA.2014.110},
abstract = {GitHub is a popular platform for collaboration on open source projects. It also provides a rich API to query various aspects of the public activity. This combination of a popular social coding website with a rich API presents an opportunity for researchers to gather empirical data about software development practices. There are an overwhelmingly large number of competing platforms to choose from in software development. Knowing which are gaining widespread adoption is valuable both for individual developers trying to increase their employability, as well as software engineers deciding which technology to use in their next big project. In terms of a developer's employability and an employer's ability to find available developers in their economic region, it is important to identify the most common technologies by geographic location. In this paper, analyses are done on GitHub data taking into account the developers' location and their technology usage. A web-based tool has been developed to interact with and visualize this data. In its current state of development, the tool summarizes the amount of code developers have in their public repositories broken down by programming language, and summarizes data about programmers using specific programming languages. This allows website visitors to get an immediate picture of the programming language usage in their region of interest. Future research could expand this work to technologies beyond programming languages such as frameworks and libraries.},
booktitle = {Proceedings of the 2014 28th International Conference on Advanced Information Networking and Applications Workshops},
pages = {681–685},
numpages = {5},
keywords = {GitHub, REST API, software repository, programming languages, open source},
series = {WAINA '14}
}

@inproceedings{10.1145/2970276.2970358,
author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
title = {Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970358},
doi = {10.1145/2970276.2970358},
abstract = { Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {426–437},
numpages = {12},
keywords = {continuous integration, mining software repositories},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/2627508.2627515,
author = {Chen, Fangwei and Li, Lei and Jiang, Jing and Zhang, Li},
title = {Predicting the Number of Forks for Open Source Software Project},
year = {2014},
isbn = {9781450329651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627508.2627515},
doi = {10.1145/2627508.2627515},
abstract = { GitHub is successful open source software platform which attract many developers. In GitHub, developers are allowed to fork repositories and copy repositories without asking for permission, which make contribution to projects much easier than it has ever been. It is significant to predict the number of forks for open source software projects. The prediction can help GitHub to recommend popular projects, and guide developers to find projects which are likely to succeed and worthy of their contribution.  In this paper, we use stepwise regression and design a model to predict the number of forks for open source software projects. Then we collect datasets of 1,000 repositories through GitHub’s APIs. We use datasets of 700 repositories to compute the weight of attributes and realize the model. Then we use other 300 repositories to verify the prediction accuracy of our model. Advantages of our model include: (1) Some attributes used in our model are new. This is because GitHub is different from traditional open source software platforms and has some new features. These new features are used to build our model. (2) Our model uses project information within t month after its creation, and predicts the number of forks in the month T (t &lt; T). It allows users to set the combination of time parameters and satisfy their own needs. (3) Our model predicts the exact number of forks, rather than the range of the number of forks (4) Experiments show that our model has high prediction accuracy. For example, we use project information with 3 months to prediction the number of forks in month 6 after its creation. The correlation coefficient is as high as 0.992, and the median number of absolute difference between prediction value and actual value is only 1.8. It shows that the predicted number of forks is very close to the actual number of forks. Our model also has high prediction accuracy when we set other time parameters. },
booktitle = {Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies},
pages = {40–47},
numpages = {8},
keywords = {Fork, Open Source Software},
location = {Nanjing, China},
series = {EAST 2014}
}

@inproceedings{10.5555/2886444.2886490,
author = {Badashian, Ali Sajedi and Shah, Vraj and Stroulia, Eleni},
title = {GitHub's Big Data Adaptor: An Eclipse Plugin},
year = {2015},
publisher = {IBM Corp.},
address = {USA},
abstract = {The data of GitHub, the most popular code-sharing platform, fits the characteristics of "big data" (Volume, Variety and Velocity). To facilitate studies on this huge GitHub data volume, the GHTorrent web-site publishes a MYSQL dump of (some) GitHub data quarterly. Unfortunately, developers using these published data dumps face challenges with respect to the time required to parse and ingest the data, the space required to store it, and the latency of their queries. To help address these challenges, we developed a data adaptor as an Eclipse plugin, which efficiently handles this dump. The plugin offers an interactive interface through which users can explore and select any field in any table. After extracting the data selected by the user, the parser exports it in easy-to-use spreadsheets. We hope that using this plugin will facilitate further studies on the GitHub data as a whole.},
booktitle = {Proceedings of the 25th Annual International Conference on Computer Science and Software Engineering},
pages = {265–268},
numpages = {4},
keywords = {software tools, eclipse plugin, mining software repositories, GitHub, big data},
location = {Markham, Canada},
series = {CASCON '15}
}

@inproceedings{10.1145/3201064.3201067,
author = {Lee, Roy Ka-Wei and Lo, David},
title = {Wisdom in Sum of Parts: Multi-Platform Activity Prediction in Social Collaborative Sites},
year = {2018},
isbn = {9781450355636},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3201064.3201067},
doi = {10.1145/3201064.3201067},
abstract = {In this paper, we proposed a novel framework which uses user interests inferred from activities (a.k.a., activity interests) in multiple social collaborative platforms to predict users' platform activities. Included in the framework are two prediction approaches: (i) direct platform activity prediction, which predicts a user's activities in a platform using his or her activity interests from the same platform (e.g., predict if a user answers a given Stack Overflow question using the user's interests inferred from his or her prior answer and favorite activities in Stack Overflow), and (ii) cross-platform activity prediction, which predicts a user's activities in a platform using his or her activity interests from another platform (e.g., predict if a user answers a given Stack Overflow question using the user's interests inferred from his or her fork and watch activities in GitHub). To evaluate our proposed method, we conduct prediction experiments on two widely used social collaborative platforms in the software development community: GitHub and Stack Overflow. Our experiments show that combining both direct and cross platform activity prediction approaches yield the best accuracies for predicting user activities in GitHub (AUC=0.75) and Stack Overflow (AUC=0.89).},
booktitle = {Proceedings of the 10th ACM Conference on Web Science},
pages = {77–86},
numpages = {10},
keywords = {github, social collaborative platforms, prediction, stack overflow},
location = {Amsterdam, Netherlands},
series = {WebSci '18}
}

@inproceedings{10.1145/3131704.3131725,
author = {Li, Zhixing and Yin, Gang and Yu, Yue and Wang, Tao and Wang, Huaimin},
title = {Detecting Duplicate Pull-Requests in GitHub},
year = {2017},
isbn = {9781450353137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131704.3131725},
doi = {10.1145/3131704.3131725},
abstract = {The widespread use of pull-requests boosts the development and evolution for many open source software projects. However, due to the parallel and uncoordinated nature of development process in GitHub, duplicate pull-requests may be submitted by different contributors to solve the same problem. Duplicate pull-requests increase the maintenance cost of GitHub, result in the waste of time spent on the redundant effort of code review, and even frustrate developers' willing to offer continuous contribution. In this paper, we investigate using text information to automatically detect duplicate pull-requests in GitHub. For a new-arriving pull-request, we compare the textual similarity between it and other existing pull-requests, and then return a candidate list of the most similar ones. We evaluate our approach on three popular projects hosted in GitHub, namely Rails, Elasticsearch and Angular.JS. The evaluation shows that about 55.3% -- 71.0% of the duplicates can be found when we use the combination of title similarity and description similarity.},
booktitle = {Proceedings of the 9th Asia-Pacific Symposium on Internetware},
articleno = {20},
numpages = {6},
keywords = {code review, duplicate detection, textual similarity, Pull-request},
location = {Shanghai, China},
series = {Internetware'17}
}

@inproceedings{10.1145/2950290.2950319,
author = {Fowkes, Jaroslav and Sutton, Charles},
title = {Parameter-Free Probabilistic API Mining across GitHub},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950319},
doi = {10.1145/2950290.2950319},
abstract = {Existing API mining algorithms can be difficult to use as they require expensive parameter tuning and the returned set of API calls can be large, highly redundant and difficult to understand. To address this, we present PAM (Probabilistic API Miner), a near parameter-free probabilistic algorithm for mining the most interesting API call patterns. We show that PAM significantly outperforms both MAPO and UPMiner, achieving 69% test-set precision, at retrieving relevant API call sequences from GitHub. Moreover, we focus on libraries for which the developers have explicitly provided code examples, yielding over 300,000 LOC of hand-written API example code from the 967 client projects in the data set. This evaluation suggests that the hand-written examples actually have limited coverage of real API usages.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {254–265},
numpages = {12},
keywords = {API mining, sequential pattern mining},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.5555/2820518.2820563,
author = {Hauff, Claudia and Gousios, Georgios},
title = {Matching GitHub Developer Profiles to Job Advertisements},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally "be seen" by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {362–366},
numpages = {5},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3183440.3194951,
author = {K\"{a}fer, Verena and Graziotin, Daniel and Bogicevic, Ivan and Wagner, Stefan and Ramadani, Jasmin},
title = {Communication in Open-Source Projects-End of the e-Mail Era?},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3194951},
doi = {10.1145/3183440.3194951},
abstract = {Communication is essential in software engineering. Especially in distributed open-source teams, communication needs to be supported by channels including mailing lists, forums, issue trackers, and chat systems. Yet, we do not have a clear understanding of which communication channels stakeholders in open-source projects use. In this study, we fill the knowledge gap by investigating a statistically representative sample of 400 GitHub projects. We discover the used communication channels by regular expressions on project data. We show that (1) half of the GitHub projects use observable communication channels; (2) GitHub Issues, e-mail addresses, and the modern chat system Gitter are the most common channels; (3) mailing lists are only in place five and have a lower market share than all modern chat systems combined.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {242–243},
numpages = {2},
keywords = {open-source, communication, mining software repositories},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3368089.3417052,
author = {Barnaby, Celeste and Sen, Koushik and Zhang, Tianyi and Glassman, Elena and Chandra, Satish},
title = {Exempla Gratis (E.G.): Code Examples for Free},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417052},
doi = {10.1145/3368089.3417052},
abstract = {Modern software engineering often involves using many existing APIs, both open source and – in industrial coding environments– proprietary. Programmers reference documentation and code search tools to remind themselves of proper common usage patterns of APIs. However, high-quality API usage examples are computationally expensive to curate and maintain, and API usage examples retrieved from company-wide code search can be tedious to review. We present a tool, EG, that mines codebases and shows the common, idiomatic us-age examples for API methods. EG was integrated into Facebook’s internal code search tool for the Hack language and evaluated on open-source GitHub projects written in Python. EG was also compared against code search results and hand-written examples from a popular programming website called ProgramCreek. Compared with these two baselines, examples generated by EG are more succinct and representative with less extraneous statements. In addition, a survey with Facebook developers shows that EG examples are preferred in 97% of cases.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1353–1364},
numpages = {12},
keywords = {software tools, API examples, big code},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/2623201.2623216,
author = {Onoue, Saya and Hata, Hideaki and Matsumoto, Ken-ichi},
title = {A Study of the Characteristics of Developers' Activities in GitHub},
year = {2013},
isbn = {9781479921447},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {What types of developers do active software projects have? This paper presents a study of the characteristics of developers' activities in open source software development. GitHub, a widely-used hosting service for software development projects, provides APIs for collecting various kinds of GitHub data. To clarify the characteristics of developers' activities, we used these APIs to investigate GitHub events generated by each developer. Using this information, we categorized developers based on measures such as whether they prefer communication by coding or comments, or whether they are specialists or generalists. Our study indicates that active software projects have various kinds of developers characterized by different types of development activities.},
booktitle = {Proceedings of the 2013 20th Asia-Pacific Software Engineering Conference (APSEC) - Volume 02},
pages = {7–12},
numpages = {6},
series = {APSEC '13}
}

@inproceedings{10.1145/2961111.2962633,
author = {Au\'{e}, Joop and Haisma, Michiel and T\'{o}masd\'{o}ttir, Krist\'{\i}n Fj\'{o}la and Bacchelli, Alberto},
title = {Social Diversity and Growth Levels of Open Source Software Projects on GitHub},
year = {2016},
isbn = {9781450344272},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2961111.2962633},
doi = {10.1145/2961111.2962633},
abstract = {Background: Projects of all sizes and impact are leveraging the services of the social coding platform GitHub to collaborate. Since users' information and actions are recorded, GitHub has been mined for over 6 years now to investigate aspects of the collaborative open source software (OSS) development paradigm. Aim: In this research, we use this data to investigate the relation between project growth as a proxy for success, and social diversity. Method: We first categorize active OSS projects into a five-star rating using a benchmarking system we based on various project growth metrics; then we study the relation between this rating and the reported social diversities for the team members of those projects. Results: Our findings highlight a statistically significant relation; however, the effect is small. Conclusions: Our findings suggest the need for further research on this topic; moreover, the proposed benchmarking method may be used in future work to determine OSS project success on collaboration platforms such as GitHub.},
booktitle = {Proceedings of the 10th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {41},
numpages = {6},
keywords = {software project growth, social diversity, GitHub},
location = {Ciudad Real, Spain},
series = {ESEM '16}
}

@inproceedings{10.1109/MSR.2019.00047,
author = {Manes, Saraj Singh and Baysal, Olga},
title = {How Often and What StackOverflow Posts Do Developers Reference in Their GitHub Projects?},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00047},
doi = {10.1109/MSR.2019.00047},
abstract = {Stack Overflow (SO) is a popular Q&amp;A forum for software developers, providing a large amount of copyable code snippets. While GitHub is an independent code collaboration platform, developers often reuse SO code in their GitHub projects. In this paper, we investigate how often GitHub developers re-use code snippets from the SO forum, as well as what concepts they are more likely to reference in their code. To accomplish our goal, we mine SOTorrent dataset that provides connectivity between code snippets on the SO posts with software projects hosted on GitHub. We then study the characteristics of GitHub projects that reference SO posts and discover popular SO discussions that happen in GitHub projects. Our results demonstrate that on average developers make 45 references to SO posts in their projects, with the highest number of references being made within the JavaScript code. We also found that 79% of the SO posts with code snippets that are referenced in GitHub code do change over time (at least ones) raising code maintainability and reliability concerns.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {235–239},
numpages = {5},
keywords = {tags, SOTorrent, GHTorrent, StackOverflow, GitHub, open-source, code snippets, code evolution},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3196321.3197546,
author = {Porkol\'{a}b, Zolt\'{a}n and Brunner, Tibor and Krupp, D\'{a}niel and Csord\'{a}s, M\'{a}rton},
title = {Codecompass: An Open Software Comprehension Framework for Industrial Usage},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3197546},
doi = {10.1145/3196321.3197546},
abstract = {CodeCompass is an open source LLVM/Clang-based tool developed by Ericsson Ltd. and E\"{o}tv\"{o}s Lor\'{a}nd University, Budapest to help the understanding of large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithms are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass also utilizes build information to explore the system architecture as well as version control information.CodeCompass is regularly used by hundreds of designers and developers. Having a web-based, pluginable, extensible architecture, the CodeCompass framework can be an open platform to further code comprehension, static analysis and software metrics efforts. The source code and a tutorial is publicly available on GitHub, and a live demo is also available online.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {361–369},
numpages = {9},
keywords = {code comprehension, software visualization, C/C++ programming language},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3241403.3241422,
author = {Ahmar, Yosser El and Pallec, Xavier Le and G\'{e}rard, S\'{e}bastien},
title = {The Visual Variables in UML: How Are They Used by Women?},
year = {2018},
isbn = {9781450364836},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3241403.3241422},
doi = {10.1145/3241403.3241422},
abstract = {This paper presents results of an empirical research study of the Unified Modeling Language (UML) actual state of practice. It reports on a quantitative analysis of &gt; 3500 UML diagrams related to open source projects in GitHub. The aim of the study is to shed light on the use of the visual variables (i.e., color, size, brightness, texture/grain, shape and orientation) in UML, with a particular focus on the practices of women in such usages. The theoretical perspective of the study is to explore the usefulness of the visual variables in UML. These latter are highly significant in reducing the cognitive load of human beings, when effectively employed. We conclude by discussions of the obtained results and commenting on the role of women in the projects involving visual variations in their diagrams.},
booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
articleno = {17},
numpages = {5},
keywords = {secondary notation, color, UML, women in software engineering, visual variables},
location = {Madrid, Spain},
series = {ECSA '18}
}

@inproceedings{10.5555/2820518.2820564,
author = {Yu, Yue and Wang, Huaimin and Filkov, Vladimir and Devanbu, Premkumar and Vasilescu, Bogdan},
title = {Wait for It: Determinants of Pull Request Evaluation Latency on GitHub},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {The pull-based development model, enabled by git and popularised by collaborative coding platforms like BitBucket, Gitorius, and GitHub, is widely used in distributed software teams. While this model lowers the barrier to entry for potential contributors (since anyone can submit pull requests to any repository), it also increases the burden on integrators (i.e., members of a project's core team, responsible for evaluating the proposed changes and integrating them into the main development line), who struggle to keep up with the volume of incoming pull requests. In this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in GitHub. Using regression modeling on data extracted from a sample of GitHub projects using the Travis-CI continuous integration service, we find that latency is a complex issue, requiring many independent variables to explain adequately.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {367–371},
numpages = {5},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3180155.3180260,
author = {Zhang, Tianyi and Upadhyaya, Ganesha and Reinhardt, Anastasia and Rajan, Hridesh and Kim, Miryung},
title = {Are Code Examples on an Online Q&amp;A Forum Reliable? A Study of API Misuse on Stack Overflow},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180260},
doi = {10.1145/3180155.3180260},
abstract = {Programmers often consult an online Q&amp;A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons---missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {886–896},
numpages = {11},
keywords = {API usage pattern, code example assessment, online Q&amp;A forum},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3300115.3309518,
author = {Glassey, Richard},
title = {Adopting Git/Github within Teaching: A Survey of Tool Support},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3309518},
doi = {10.1145/3300115.3309518},
abstract = {The adoption and use of Git and Github within computer science education is growing in popularity. The motivation for this shift is strong: it combines a robust system for managing student coursework, sophisticated collaboration and communication tools for students and teaching staff, and an authentic experience of an important software engineering skill. Whilst previous literature has reported upon experience and benefits, there still exists a technical barrier to overcome in adopting Git and Github within an educational context. In response, both the community of teachers using Git/Github and the Github organisation itself have developed tool support to help solve the challenge of adoption, however these efforts are somewhat isolated and relatively unstudied. This work aims to provide an overview of these tools, identify the commonalities and differences, and develop a framework for comparison to assist teachers when looking for solutions for their own courses.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {143–149},
numpages = {7},
keywords = {version control system, Git/Github, computing education},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@inproceedings{10.1145/2676723.2678305,
author = {DeCausemaker, Remy and Jacobs, Stephen},
title = {Steal This Courseware: FOSS, Github, Python, and OpenShift (Abstract Only)},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2678305},
doi = {10.1145/2676723.2678305},
abstract = {This workshop introduces participants to the pedagogy and practice of using Free/Open Source Software development practices into their curriculum, and then guides them through deployment of a turnkey courseware framework to be used for their own courses. The framework supports automatic blog checking, automatically generated student profile pages, Gravatar integration for profile pictures, Travis-CI continuous Integration tests, and repository changes reported via Github webhooks to IRC. Participants will learn how to use Github in the Classroom, the basics of Flask, a python web framework, and how to deploy their courseware to Red Hat's OpenShift Cloud, a free Platform-as-a-Service to host courseware and/or other web sites.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {708},
numpages = {1},
keywords = {python, web blocks, github, flask, free software, openshift, free content, open source software},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/3287324.3287556,
author = {Cetinkaya-Rundel, Mine},
title = {Computing Infrastructure and Curriculum Design for Introductory Data Science},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287556},
doi = {10.1145/3287324.3287556},
abstract = {The goal of this workshop is to equip educators with concrete information on content and infrastructure for designing and painlessly running a modern data science course. This is a three-part workshop. Part 1 will outline a curriculum for an introductory data science course and discuss pedagogical decisions that go into the choice of topics and concepts as well as the choice of programming language (R) and syntax (primarily tidyverse), and the emphasis on literate programming for reproducibility (with R Markdown). Part 2 will discuss infrastructure choices around teaching data science with R: RStudio as an integrated development environment, cloud-based access with RStudio Cloud and Server, version control with Git, and collaboration with GitHub. Part 3 will focus on classroom management on GitHub (with ghclass). Workshop attendees will work through several exercises from the course and get first-hand experience with using the tool-chains and techniques described above. While the workshop content will focus on usage of R, many of the pedagogical takeaways will be language agnostic. All workshop content, including teacher facing documentation and student facing course materials, will also be available to participants via datasciencebox.org. Please bring a laptop with you.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {1236},
numpages = {1},
keywords = {pedagogy, version control, reproducibility, data science, r},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/2675133.2675284,
author = {Zagalsky, Alexey and Feliciano, Joseph and Storey, Margaret-Anne and Zhao, Yiyun and Wang, Weiliang},
title = {The Emergence of GitHub as a Collaborative Platform for Education},
year = {2015},
isbn = {9781450329224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2675133.2675284},
doi = {10.1145/2675133.2675284},
abstract = {The software development community has embraced GitHub as an essential platform for managing their software projects. GitHub has created efficiencies and helped improve the way software professionals work. It not only provides a traceable project repository, but it acts as a social meeting place for interested parties, supporting communities of practice. Recently, educators have seen the potential in GitHub's collaborative features for managing and improving---perhaps even transforming---the learning experience. In this study, we examine how GitHub is emerging as a collaborative platform for education. We aim to understand how environments such as GitHub---environments that provide social and collaborative features in conjunction with distributed version control---may improve (or possibly hinder) the educational experience for students and teachers. We conduct a qualitative study focusing on how GitHub is being used in education, and the motivations, benefits and challenges it brings.},
booktitle = {Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing},
pages = {1906–1917},
numpages = {12},
keywords = {education, learning, cscl, cscw, github, qualitative methodology, distributed version control, social media},
location = {Vancouver, BC, Canada},
series = {CSCW '15}
}

@inproceedings{10.5555/2486788.2486804,
author = {Pham, Raphael and Singer, Leif and Liskin, Olga and Figueira Filho, Fernando and Schneider, Kurt},
title = {Creating a Shared Understanding of Testing Culture on a Social Coding Site},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Many software development projects struggle with creating and communicating a testing culture that is appropriate for the project's needs. This may degrade software quality by leaving defects undiscovered. Previous research suggests that social coding sites such as GitHub provide a collaborative environment with a high degree of social transparency. This makes developers' actions and interactions more visible and traceable. We conducted interviews with 33 active users of GitHub to investigate how the increased transparency found on GitHub influences developers' testing behaviors. Subsequently, we validated our findings with an online questionnaire that was answered by 569 members of GitHub. We found several strategies that software developers and managers can use to positively influence the testing behavior in their projects. However, project owners on GitHub may not be aware of them. We report on the challenges and risks caused by this and suggest guidelines for promoting a sustainable testing culture in software development projects. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {112–121},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3379597.3387501,
author = {Fan, Jiahao and Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
title = {A C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387501},
doi = {10.1145/3379597.3387501},
abstract = {We collected a large C/C++ code vulnerability dataset from open-source Github projects, namely Big-Vul. We crawled the public Common Vulnerabilities and Exposures (CVE) database and CVE-related source code repositories. Specifically, we collected the descriptive information of the vulnerabilities from the CVE database, e.g., CVE IDs, CVE severity scores, and CVE summaries. With the CVE information and its related published Github code repository links, we downloaded all of the code repositories and extracted vulnerability related code changes. In total, Big-Vul contains 3,754 code vulnerabilities spanning 91 different vulnerability types. All these code vulnerabilities are extracted from 348 Github projects. All information is stored in the CSV format. We linked the code changes with the CVE descriptive information. Thus, our Big-Vul can be used for various research topics, e.g., detecting and fixing vulnerabilities, analyzing the vulnerability related code changes. Big-Vul is publicly available on Github.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {508–512},
numpages = {5},
keywords = {C/C++ Code, Common Vulnerabilities and Exposures, Code Changes},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3183440.3195071,
author = {Kochhar, Pavneet Singh and Swierc, Stanislaw and Carnahan, Trevor and Sajnani, Hitesh and Nagappan, Meiyappan},
title = {Understanding the Role of Reporting in Work Item Tracking Systems for Software Development: An Industrial Case Study},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195071},
doi = {10.1145/3183440.3195071},
abstract = {Work item tracking systems such as Visual Studio Team Services, JIRA, and GitHub issue tracker are widely used by software engineers. They help in managing different kinds of deliverables (e.g.\^{A}\u{a}features, user stories, bugs), plan sprints, distribute tasks across the team and prioritize the work. While these tools provide reporting capabilities there has been little research into the role these reports play in the overall software development process.In this study, we conduct an empirical investigation on the usage of Analytics Service - a reporting service provided by Visual Studio Team Services (VSTS) to build dashboards and reports out of their work item tracking data. In particular, we want to understand why and how users interact with Analytics Service and what are the outcomes and business decisions taken by stakeholders from reports built using Analytics Service. We perform semi-structured interviews and survey with users of Analytics Service to understand usage and challenges. Our report on qualitative and quantitative analysis can help organizations and engineers building similar tools or services.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {133–134},
numpages = {2},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2839509.2851052,
author = {Izbicki, Mike},
title = {Open Sourcing the Classroom (Abstract Only)},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2851052},
doi = {10.1145/2839509.2851052},
abstract = {This project describes an experimental course on open source software construction. The course has two twists on the standard project-based software construction course. The first twist is simple: all projects are developed and released on GitHub. The second twist is more radical: the course uses an "open source textbook." The textbook is hosted in a git repository that students are required to contribute to throughout the term. Contributions range from minor typo fixes to adding entire chapters. Currently, 88% of the textbook is written by students, including many of the assignments. We use student surveys, participation in social networking sites like GitHub, and web traffic logs to determine that these assignments had a positive effect on students' future contributions to the open source community.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {723},
numpages = {1},
keywords = {open source, education, git},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/2556420.2556483,
author = {Wu, Yu and Kropczynski, Jessica and Shih, Patrick C. and Carroll, John M.},
title = {Exploring the Ecosystem of Software Developers on GitHub and Other Platforms},
year = {2014},
isbn = {9781450325417},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556420.2556483},
doi = {10.1145/2556420.2556483},
abstract = {GitHub provides various social features for developers to collaborate with others. Those features are important for developers to coordinate their work (Dabbish et al., 2012; Marlow et al., 2013). We hypothesized that the social system of GitHub users was bound by system interactions such that contributing to similar code repositories would lead to users following one another on GitHub or vice versa. Using a quadratic assignment procedure (QAP) correlation, however, only a weak correlation among followship and production activities (code, issue, and wiki contributions) was found. Survey with GitHub users revealed an ecosystem on the Internet for software developers, which includes many platforms, such as Forrst, Twitter, and Hacker News, among others. Developers make social introductions and other interactions on these platforms and engage with one anther on GitHub. Due to these preliminary findings, we describe GitHub as a part of a larger ecosystem of developer interactions.},
booktitle = {Proceedings of the Companion Publication of the 17th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing},
pages = {265–268},
numpages = {4},
keywords = {ecosystem, social connection, GitHub, follow},
location = {Baltimore, Maryland, USA},
series = {CSCW Companion '14}
}

@inproceedings{10.1145/3180155.3182535,
author = {Borle, Neil C. and Feghhi, Meysam and Stroulia, Eleni and Greiner, Russell and Hindle, Abram},
title = {Analyzing the Effects of Test Driven Development in GitHub},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3182535},
doi = {10.1145/3180155.3182535},
abstract = {Testing is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models recommend Test Driven Development (TDD) as a key practice for reducing costs and improving code quality. The objective of this work is to perform a cost-benefit analysis of this practice. Previous work by Fucci et al. [2, 3] engaged in laboratory studies of developers actively engaged in test-driven development practices. Fucci et al. found little difference between test-first behaviour of TDD and test-later behaviour. To that end, we opted to conduct a study about TDD behaviours in the "wild" rather than in the laboratory. Thus we have conducted a comparative analysis of GitHub repositories that adopts TDD to a lesser or greater extent, in order to determine how TDD affects software development productivity and software quality. We classified GitHub repositories archived in 2015 in terms of how rigorously they practiced TDD, thus creating a TDD spectrum. We then matched and compared various subsets of these repositories on this TDD spectrum with control sets of equal size. The control sets were samples from all GitHub repositories that matched certain characteristics, and that contained at least one test file. We compared how the TDD sets differed from the control sets on the following characteristics: number of test files, average commit velocity, number of bug-referencing commits, number of issues recorded, usage of continuous integration, number of pull requests, and distribution of commits per author. We found that Java TDD projects were relatively rare. In addition, there were very few significant differences in any of the metrics we used to compare TDD-like and non-TDD projects; therefore, our results do not reveal any observable benefits from using TDD.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1062},
numpages = {1},
keywords = {GitHub repositories, test driven development, continuous integration, human factors in software development},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2972958.2972966,
author = {Borges, Hudson and Hora, Andre and Valente, Marco Tulio},
title = {Predicting the Popularity of GitHub Repositories},
year = {2016},
isbn = {9781450347723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972958.2972966},
doi = {10.1145/2972958.2972966},
abstract = {GitHub is the largest source code repository in the world. It provides a git-based source code management platform and also many features inspired by social networks. For example, GitHub users can show appreciation to projects by adding stars to them. Therefore, the number of stars of a repository is a direct measure of its popularity. In this paper, we use multiple linear regressions to predict the number of stars of GitHub repositories. These predictions are useful both to repository owners and clients, who usually want to know how their projects are performing in a competitive open source development market. In a large-scale analysis, we show that the proposed models start to provide accurate predictions after being trained with the number of stars received in the last six months. Furthermore, specific models---generated using data from repositories that share the same growth trends---are recommended for repositories with slow growth and/or for repositories with less stars. Finally, we evaluate the ability to predict not the number of stars of a repository but its rank among the GitHub repositories. We found a very strong correlation between predicted and real rankings (Spearman's rho greater than 0.95).},
booktitle = {Proceedings of the The 12th International Conference on Predictive Models and Data Analytics in Software Engineering},
articleno = {9},
numpages = {10},
keywords = {Popularity, Open Source Development, Social Coding, Prediction Models, GitHub},
location = {Ciudad Real, Spain},
series = {PROMISE 2016}
}

@inproceedings{10.1145/3338906.3342486,
author = {Abid, Shamsa},
title = {Recommending Related Functions from API Usage-Based Function Clone Structures},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342486},
doi = {10.1145/3338906.3342486},
abstract = {Developers need to be able to find reusable code for desired software features in a way that supports opportunistic programming for increased developer productivity. Our objective is to develop a recommendation system that provides a developer with function recommendations having functionality relevant to her development task. We employ a combination of information retrieval, static code analysis and data mining techniques to build the proposed recommendation system called FACER (Feature-driven API usage-based Code Examples Recommender). We performed an experimental evaluation on 122 projects from GitHub from selected categories to determine the accuracy of the retrieved code for related features. FACER recommended functions with a precision of 54% and 75% when evaluated using automated and manual methods respectively.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1193–1195},
numpages = {3},
keywords = {code recommendation, code clones, software features, API usage},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/3370272.3370281,
author = {Perrie, Jessica and Xie, Jing and Nayebi, Maleknaz and Fokaefs, Marios and Lyons, Kelly and Stroulia, Eleni},
title = {City on the River: Visualizing Temporal Collaboration},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {Collaboration is an important component of most work activities. We are interested in understanding how configurations of people come together to create outputs over time. We propose an interactive visualization tool (City on the River) for visualizing collaborations over time. The City on the River (CotR) visualization shows the contributions and artifacts ("products") of a team on a timeline and the individuals on the team who contributed to each product. CotR enables interactive analyses of each of these components for answering questions such as, which people work together on the most products, which products involve the most people, what kinds of products were produced when and by whom, etc. CotR can be used for analyzing diverse domains such as research collaborations, conference participation, email conversations, and software development. In this paper, we present the results of an experiment to assess CotR for analyzing collaboration and outcomes in GitHub projects. We compared the quality of answers, time to answer, and approaches taken to analyze the project collaborations by two groups of people: one group used the GitHub data displayed in a spreadsheet; the other group used the GitHub data displayed using CotR.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {82–91},
numpages = {10},
keywords = {visualization, collaboration, software engineering},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/2441776.2441794,
author = {Marlow, Jennifer and Dabbish, Laura},
title = {Activity Traces and Signals in Software Developer Recruitment and Hiring},
year = {2013},
isbn = {9781450313315},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2441776.2441794},
doi = {10.1145/2441776.2441794},
abstract = {Social networking tools now allow professionals to post and share their work in online spaces. These professionals build reputation within a community of practice, often with the goal of finding a job. But how are the visible traces of their actions and interactions in online workspaces used in the hiring process? We conducted interviews with members of the GitHub "social coding" community to understand how profiles on the site are used to assess people during recruitment and hiring for software development positions. Both employers and job seekers pointed to specific cues provided on profiles that led them to make inferences (or form impressions) about a candidate's technical skills, motivations, and values. These cues were seen as more reliable indicators of technical abilities and motivation than information provided on a resume, because of the transparency of work actions on GitHub and relative difficulty of manipulating behavior traces. The use of online workspaces like GitHub has implications for the type of information sought by employers as well as the activity traces job hunters might seek to leave.},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work},
pages = {145–156},
numpages = {12},
keywords = {impression formation, transparency, hiring, impression management, open source software development},
location = {San Antonio, Texas, USA},
series = {CSCW '13}
}

@inproceedings{10.1145/3211933.3211934,
author = {Kr\'{o}l, Micha\l{} and Re\~{n}\'{e}, Sergi and Ascigil, Onur and Psaras, Ioannis},
title = {ChainSoft: Collaborative Software Development Using Smart Contracts},
year = {2018},
isbn = {9781450358385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211933.3211934},
doi = {10.1145/3211933.3211934},
abstract = {In recent years, more and more companies require dedicated software to increase the efficiency of their business. However, with rapidly changing technologies it is often inefficient to maintain a dedicated team of developers. On the other hand, outsourcing software development requires considerable effort and trust between involved parties to ensure the quality of the code and adequate payment.We present ChainSoft - a platform for outsourcing software development and automatic payments between parties that distrust each other, by means of blockchain technology. ChainSoft allows any developer to create software and submit software, includes automatic code verification and enforce users' proper behavior. We implement our system using Ethereum Smart Contracts and Github/Travis CI and present first evaluation proving its security and low usage cost.},
booktitle = {Proceedings of the 1st Workshop on Cryptocurrencies and Blockchains for Distributed Systems},
pages = {1–6},
numpages = {6},
keywords = {software development, smart contracts, github, blockchain},
location = {Munich, Germany},
series = {CryBlock'18}
}

@inproceedings{10.1145/2597073.2597121,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
title = {An Insight into the Pull Requests of GitHub},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597121},
doi = {10.1145/2597073.2597121},
abstract = { Given the increasing number of unsuccessful pull requests in GitHub projects, insights into the success and failure of these requests are essential for the developers. In this paper, we provide a comparative study between successful and unsuccessful pull requests made to 78 GitHub base projects by 20,142 developers from 103,192 forked projects. In the study, we analyze pull request discussion texts, project specific information (e.g., domain, maturity), and developer specific information (e.g., experience) in order to report useful insights, and use them to contrast between successful and unsuccessful pull requests. We believe our study will help developers overcome the issues with pull requests in GitHub, and project administrators with informed decision making. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {364–367},
numpages = {4},
keywords = {Commit comments, topic model, pull request},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1109/FCST.2015.45,
author = {Luo, Zizhan and Mao, Xiaoguang and Li, Ang},
title = {An Exploratory Research of GitHub Based on Graph Model},
year = {2015},
isbn = {9781467392952},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/FCST.2015.45},
doi = {10.1109/FCST.2015.45},
abstract = {GitHub has accumulated a great number of developers and open source projects. In this research, we utilize property graph model to explore complex relationships and entities of GitHub. We attempt to answer three questions associated with GitHub using the dataset from MSR2014 data challenge. Firstly, we propose a graph based method to find out the cross technology background developers on GitHub. Secondly we define interesting metrics based on discrete entropy to analyze the project imbalance induced by commit action within a software family. The results show that the imbalance of development size induced by root projects is greater than that of development speed. Finally, we sort out the relatively important root projects with two link analysis methods and the experiment result demonstrates that our method is effective.},
booktitle = {Proceedings of the 2015 Ninth International Conference on Frontier of Computer Science and Technology},
pages = {96–103},
numpages = {8},
keywords = {Graph Database, GitHub, Software Metric, Empirical Studies, Mining Software Repositories, Link Analysis, Imbalance, Graph Model},
series = {FCST '15}
}

@inproceedings{10.1109/QSIC.2014.19,
author = {Mostafa, Shaikh and Wang, Xiaoyin},
title = {An Empirical Study on the Usage of Mocking Frameworks in Software Testing},
year = {2014},
isbn = {9781479971985},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/QSIC.2014.19},
doi = {10.1109/QSIC.2014.19},
abstract = {In software testing, especially unit testing, it is very common that software testers need to test a class or a component without integration with some of its dependencies. Typical reasons for excluding dependencies in testing include the unavailability of some dependency due to concurrent software development and callbacks in frameworks, high cost of invoking some dependencies (e.g., slow network or database operations, commercial third-party web services), and the potential interference of bugs in the dependencies. In practice, mock objects have been used in software testing to simulate such missing dependencies, and a number of popular mocking frameworks (e.g., Mockito, EasyMock) have been developed for software testers to generate mock objects more conveniently. However, despite the wide usage of mocking frameworks in software practice, there have been very few academic studies to observe and understand the usage status of mocking frameworks, and the major issues software testers are facing when using such mocking frameworks. In this paper, we report on an empirical study on the usage of four most popular mock frameworks (Mockito, EasyMock, JMock, and JMockit) in 5,000 open source software projects from GitHub. The results of our study show that the above mentioned mocking frameworks are used in a large portion (about 23%) of software projects that have test code. We also find that software testers typically create mocks for only part of the software dependencies, and there are more mocking of source code classes than library classes.},
booktitle = {Proceedings of the 2014 14th International Conference on Quality Software},
pages = {127–132},
numpages = {6},
keywords = {Testing, Mocking Frameworks},
series = {QSIC '14}
}

@inproceedings{10.1109/MSR.2019.00054,
author = {Montandon, Jo\~{a}o Eduardo and Silva, Luciana Lourdes and Valente, Marco Tulio},
title = {Identifying Experts in Software Libraries and Frameworks among GitHub Users},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00054},
doi = {10.1109/MSR.2019.00054},
abstract = {Software development increasingly depends on libraries and frameworks to increase productivity and reduce time-to-market. Despite this fact, we still lack techniques to assess developers expertise in widely popular libraries and frameworks. In this paper, we evaluate the performance of unsupervised (based on clustering) and supervised machine learning classifiers (Random Forest and SVM) to identify experts in three popular JavaScript libraries: facebook/react, mongodb/node-mongodb, and socketio/socket.io. First, we collect 13 features about developers activity on GitHub projects, including commits on source code files that depend on these libraries. We also build a ground truth including the expertise of 575 developers on the studied libraries, as self-reported by them in a survey. Based on our findings, we document the challenges of using machine learning classifiers to predict expertise in software libraries, using features extracted from GitHub. Then, we propose a method to identify library experts based on clustering feature data from GitHub; by triangulating the results of this method with information available on Linkedin profiles, we show that it is able to recommend dozens of GitHub users with evidences of being experts in the studied JavaScript libraries. We also provide a public dataset with the expertise of 575 developers on the studied libraries.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {276–287},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3377811.3380349,
author = {Tan, Shin Hwei and Li, Ziqiang},
title = {Collaborative Bug Finding for Android Apps},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380349},
doi = {10.1145/3377811.3380349},
abstract = {Many automated test generation techniques have been proposed for finding crashes in Android apps. Despite recent advancement in these approaches, a study shows that Android app developers prefer reading test cases written in natural language. Meanwhile, there exist redundancies in bug reports (written in natural language) across different apps that have not been previously reused. We propose collaborative bug finding, a novel approach that uses bugs in other similar apps to discover bugs in the app under test. We design three settings with varying degrees of interactions between programmers: (1) bugs from programmers who develop a different app, (2) bugs from manually searching for bug reports in GitHub repositories, (3) bugs from a bug recommendation system, Bugine. Our studies of the first two settings in a software testing course show that collaborative bug finding helps students who are novice Android app testers to discover 17 new bugs. As students admit that searching for relevant bug reports could be time-consuming, we introduce Bugine, an approach that automatically recommends relevant GitHub issues for a given app. Bugine uses (1) natural language processing to find GitHub issues that mention common UI components shared between the app under test and other apps in our database, and (2) a ranking algorithm to select GitHub issues that are of the best quality. Our results show that Bugine is able to find 34 new bugs. In total, collaborative bug finding helps us find 51 new bugs, in which eight have been confirmed and 11 have been fixed by the developers. These results confirm our intuition that our proposed technique is useful in discovering new bugs for Android apps.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1335–1347},
numpages = {13},
keywords = {collaborative programming, recommendation system, test generation, Android apps},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3382494.3410690,
author = {Di Rocco, Juri and Di Ruscio, Davide and Di Sipio, Claudio and Nguyen, Phuong and Rubei, Riccardo},
title = {TopFilter: An Approach to Recommend Relevant GitHub Topics},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410690},
doi = {10.1145/3382494.3410690},
abstract = {Background: In the context of software development, GitHub has been at the forefront of platforms to store, analyze and maintain a large number of software repositories. Topics have been introduced by GitHub as an effective method to annotate stored repositories. However, labeling GitHub repositories should be carefully conducted to avoid adverse effects on project popularity and reachability. Aims: We present TopFilter, a novel approach to assist open source software developers in selecting suitable topics for GitHub repositories being created. Method: We built a project-topic matrix and applied a syntactic-based similarity function to recommend missing topics by representing repositories and related topics in a graph. The ten-fold cross-validation methodology has been used to assess the performance of TopFilter by considering different metrics, i.e., success rate, precision, recall, and catalog coverage. Result: The results show that TopFilter recommends good topics depending on different factors, i.e., collaborative filtering settings, considered datasets, and pre-processing activities. Moreover, TopFilter can be combined with a state-of-the-art topic recommender system (i.e., MNB network) to improve the overall prediction performance. Conclusion: Our results confirm that collaborative filtering techniques can successfully be used to provide relevant topics for GitHub repositories. Moreover, TopFilter can gain a significant boost in prediction performances by employing the outcomes obtained by the MNB network as its initial set of topics.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {21},
numpages = {11},
keywords = {Collaborative filtering, GitHub topics recommendation, Recommender systems},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.5555/2820518.2820601,
author = {Vasilescu, Bogdan and Serebrenik, Alexander and Filkov, Vladimir},
title = {A Data Set for Social Diversity Studies of GitHub Teams},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Like any other team oriented activity, the software development process is effected by social diversity in the programmer teams. The effect of team diversity can be significant, but also complex, especially in decentralized teams. Discerning the precise contribution of diversity on teams' effectiveness requires quantitative studies of large data sets.Here we present for the first time a large data set of social diversity attributes of programmers in GitHub teams. Using alias resolution, location data, and gender inference techniques, we collected a team social diversity data set of 23,493 GitHub projects. We illustrate how the data set can be used in practice with a series of case studies, and we hope its availability will foster more interest in studying diversity issues in software teams.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {514–517},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1109/ICSE-C.2017.99,
author = {Baltes, Sebastian and Kiefer, Richard and Diehl, Stephan},
title = {Attribution Required: Stack Overflow Code Snippets in GitHub Projects},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.99},
doi = {10.1109/ICSE-C.2017.99},
abstract = {Stack Overflow (SO) is the largest Q&amp;A website for developers, providing a huge amount of copyable code snippets. Using these snippets raises various maintenance and legal issues. The SO license requires attribution, i.e., referencing the original question or answer, and requires derived work to adopt a compatible license. While there is a heated debate on SO's license model for code snippets and the required attribution, little is known about the extent to which snippets are copied from SO without proper attribution. In this paper, we present the research design and summarized results of an empirical study analyzing attributed and unattributed usages of SO code snippets in GitHub projects. On average, 3.22% of all analyzed repositories and 7.33% of the popular ones contained a reference to SO. Further, we found that developers rather refer to the whole thread on SO than to a specific answer. For Java, at least two thirds of the copied snippets were not attributed.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {161–163},
numpages = {3},
keywords = {GitHub, licensing, empirical study, copy-and-paste programming, stack overflow, survey, code snippets},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1145/3121257.3121261,
author = {Devanbu, Premkumar and Kudigrama, Pallavi and Rubio-Gonz\'{a}lez, Cindy and Vasilescu, Bogdan},
title = {Timezone and Time-of-Day Variance in GitHub Teams: An Empirical Method and Study},
year = {2017},
isbn = {9781450351577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3121257.3121261},
doi = {10.1145/3121257.3121261},
abstract = { Open source projects based in ecosystems like GitHub seamlessly allow distributed software development. Contributors to some GitHub projects may originate from many different timezones; in others they may all reside in just one timezone. How might this timezone dispersion (or concentration) affect the diurnal distribution of work activity in these projects? In commercial projects, there has been a desire to use top-down management and work allocation to exploit timezone dispersion of project teams, to engender a more round-the-clock work cycle. We focus on GitHub, and explore the relationship between timezone dispersion and work activity dispersion. We find that while time-of-day work activity dispersion is indeed associated strongly with timezone dispersion, it is equally (if not more strongly) affected by project team size. },
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Software Analytics},
pages = {19–22},
numpages = {4},
keywords = {Timezones, Circular Statistics, GitHub},
location = {Paderborn, Germany},
series = {SWAN 2017}
}

@inproceedings{10.5555/2819321.2819330,
author = {Vasilescu, Bogdan and Filkov, Vladimir and Serebrenik, Alexander},
title = {Perceptions of Diversity on GitHub: A User Survey},
year = {2015},
publisher = {IEEE Press},
abstract = {Understanding one's work environment is important for one's success, especially when working in teams. In virtual collaborative environments this amounts to being aware of the technical and social attributes of one's team members. Focusing on Open Source Software teams, naturally very diverse both socially and technically, we report the results of a user survey that tries to resolve how teamwork and individual attributes are perceived by developers collaborating on GitHub, and how those perceptions influence their work. Our findings can be used as complementary data to quantitative studies of developers' behavior on GitHub.},
booktitle = {Proceedings of the Eighth International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {50–56},
numpages = {7},
location = {Florence, Italy},
series = {CHASE '15}
}

@inproceedings{10.1145/3357766.3359541,
author = {Seifer, Philipp and H\"{a}rtel, Johannes and Leinberger, Martin and L\"{a}mmel, Ralf and Staab, Steffen},
title = {Empirical Study on the Usage of Graph Query Languages in Open Source Java Projects},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359541},
doi = {10.1145/3357766.3359541},
abstract = {Graph data models are interesting in various domains, in part because of the intuitiveness and flexibility they offer compared to relational models. Specialized query languages, such as Cypher for property graphs or SPARQL for RDF, facilitate their use. In this paper, we present an empirical study on the usage of graph-based query languages in open-source Java projects on GitHub. We investigate the usage of SPARQL, Cypher, Gremlin and GraphQL in terms of popularity and their development over time. We select repositories based on dependencies related to these technologies and employ various popularity and source-code based filters and ranking features for a targeted selection of projects. For the concrete languages SPARQL and Cypher, we analyze the activity of repositories over time. For SPARQL, we investigate common application domains, query use and existence of ontological data modeling in applications that query for concrete instance data. Our results show, that the usage of graph query languages in open-source projects increased over the last years, with SPARQL and Cypher being by far the most popular. SPARQL projects are more active in terms of query related artifact changes and unique developers involved, but Cypher is catching up. Relatively few applications use SPARQL to query for concrete instance data: A majority of those applications employ multiple different ontologies, including project and domain specific ones. Common application domains are management systems and data visualization tools.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {152–166},
numpages = {15},
keywords = {Empirical Study, Cypher, Graphs, Query Languages, SPARQL, GitHub, GraphQL, Gremlin},
location = {Athens, Greece},
series = {SLE 2019}
}

@inproceedings{10.5555/3155562.3155616,
author = {Chapman, Carl and Wang, Peipei and Stolee, Kathryn T.},
title = {Exploring Regular Expression Comprehension},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 of pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluated the understandability of various regex language features. We further analyzed regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {405–416},
numpages = {12},
keywords = {Regular expression comprehension, regex representations, equivalence class},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3311790.3399616,
author = {Sochat, Vanessa},
title = {AskCI Server: Collaborative Knowledge Base},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3399616},
doi = {10.1145/3311790.3399616},
abstract = { AskCI Server is a collaborative, open source documentation server that uses GitHub for automation and version control of shared knowledge. A programmatic application programming interface, friendly user interface, and organization of concepts into questions makes it versatile as a support or collaborative knowledge base.},
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {514–517},
numpages = {4},
keywords = {documentation, git, docker, knowledge, version control, articles, automation, collaboration},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@inproceedings{10.1145/3034950.3034980,
author = {Aljemabi, Mohammed Abdelrahman and Wang, Zhongjie},
title = {Empirical Study on the Similarity and Difference between VCS-DSN and BTS-DSN},
year = {2017},
isbn = {9781450348348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3034950.3034980},
doi = {10.1145/3034950.3034980},
abstract = {Most of the developers are collaborating freely as the team works through the Internet, for developing open source software projects without access restriction, playing different tasks including communications and coordination, making various social collaboration in the open source software projects (e.g., Bug/issue report, discussion, code revisions, etc.). All these activities can be recorded into software repositories like GitHub, and used to generate an implicit developer social network (DSN). In this paper, we conduct our empirical study using 50 open source software projects collected from GitHub and construct VCS-DSN and BTS-DSN to investigate the same collaboration between developers in the real world, and to find similarities and differences between them, in addition to the degree of similarity and diversity.},
booktitle = {Proceedings of the 2017 International Conference on Management Engineering, Software Engineering and Service Sciences},
pages = {30–37},
numpages = {8},
keywords = {Bug tracking system, Software Engineering, Version control system, Mining software repository, Developer social network},
location = {Wuhan, China},
series = {ICMSS '17}
}

@inproceedings{10.1145/3377811.3380408,
author = {Chen, Boyuan and Jiang, Zhen Ming (Jack)},
title = {Studying the Use of Java Logging Utilities in the Wild},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380408},
doi = {10.1145/3377811.3380408},
abstract = {Software logging is widely used in practice. Logs have been used for a variety of purposes like debugging, monitoring, security compliance, and business analytics. Instead of directly invoking the standard output functions, developers usually prefer to use logging utilities (LUs) (e.g., SLF4J), which provide additional functionalities like thread-safety and verbosity level support, to instrument their source code. Many of the previous research works on software logging are focused on the log printing code. There are very few works studying the use of LUs, although new LUs are constantly being introduced by companies and researchers. In this paper, we conducted a large-scale empirical study on the use of Java LUs in the wild. We analyzed the use of 3, 856 LUs from 11,194 projects in GitHub and found that many projects have complex usage patterns for LUs. For example, 75.8% of the large-sized projects have implemented their own LUs in their projects. More than 50% of these projects use at least three LUs. We conducted further qualitative studies to better understand and characterize the complex use of LUs. Our findings show that different LUs are used for a variety of reasons (e.g., internationalization of the log messages). Some projects develop their own LUs to satisfy project-specific logging needs (e.g., defining the logging format). Multiple uses of LUs in one project are pretty common for large and very largesized projects mainly for context like enabling and configuring the logging behavior for the imported packages. Interviewing with 13 industrial developers showed that our findings are also generally true for industrial projects and are considered as very helpful for them to better configure and manage the logging behavior for their projects. The findings and the implications presented in this paper will be useful for developers and researchers who are interested in developing and maintaining LUs.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {397–408},
numpages = {12},
keywords = {empirical software engineering, logging practices, logging code},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2804360.2804366,
author = {Yamashita, Kazuhiro and McIntosh, Shane and Kamei, Yasutaka and Hassan, Ahmed E. and Ubayashi, Naoyasu},
title = {Revisiting the Applicability of the Pareto Principle to Core Development Teams in Open Source Software Projects},
year = {2015},
isbn = {9781450338165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2804360.2804366},
doi = {10.1145/2804360.2804366},
abstract = { It is often observed that the majority of the development work of an Open Source Software (OSS) project is contributed by a core team, i.e., a small subset of the pool of active devel- opers. In fact, recent work has found that core development teams follow the Pareto principle — roughly 80% of the code contributions are produced by 20% of the active developers. However, those findings are based on samples of between one and nine studied systems. In this paper, we revisit prior studies about core developers using 2,496 projects hosted on GitHub. We find that even when we vary the heuristic for detecting core developers, and when we control for system size, team size, and project age: (1) the Pareto principle does not seem to apply for 40%-87% of GitHub projects; and (2) more than 88% of GitHub projects have fewer than 16 core developers. Moreover, we find that when we control for the quantity of contributions, bug fixing accounts for a similar proportion of the contributions of both core (18%-20%) and non-core developers (21%-22%). Our findings suggest that the Pareto principle is not compatible with the core teams of many GitHub projects. In fact, several of the studied GitHub projects are susceptible to the “bus factor,” where the impact of a core developer leaving would be quite harmful. },
booktitle = {Proceedings of the 14th International Workshop on Principles of Software Evolution},
pages = {46–55},
numpages = {10},
keywords = {Core development team, Pareto principle, Open source},
location = {Bergamo, Italy},
series = {IWPSE 2015}
}

@inproceedings{10.5555/3155562.3155576,
author = {Kavaler, David and Sirovica, Sasha and Hellendoorn, Vincent and Aranovich, Raul and Filkov, Vladimir},
title = {Perceived Language Complexity in GitHub Issue Discussions and Their Effect on Issue Resolution},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Modern software development is increasingly collaborative. Open Source Software (OSS) are the bellwether; they support dynamic teams, with tools for code sharing, communication, and issue tracking. The success of an OSS project is reliant on team communication. E.g., in issue discussions, individuals rely on rhetoric to argue their position, but also maintain technical relevancy. Rhetoric and technical language are on opposite ends of a language complexity spectrum: the former is stylistically natural; the latter is terse and concise. Issue discussions embody this duality, as developers use rhetoric to describe technical issues. The style mix in any discussion can define group culture and affect performance, e.g., issue resolution times may be longer if discussion is imprecise.  Using GitHub, we studied issue discussions to understand whether project-specific language differences exist, and to what extent users conform to a language norm. We built project-specific and overall GitHub language models to study the effect of perceived language complexity on multiple responses. We find that experienced users conform to project-specific language norms, popular individuals use overall GitHub language rather than project-specific language, and conformance to project-specific language norms reduces issue resolution times. We also provide a tool to calculate project-specific perceived language complexity. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {72–83},
numpages = {12},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3196494.3196523,
author = {Afzali, Hammad and Torres-Arias, Santiago and Curtmola, Reza and Cappos, Justin},
title = {Le-Git-Imate: Towards Verifiable Web-Based Git Repositories},
year = {2018},
isbn = {9781450355766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196494.3196523},
doi = {10.1145/3196494.3196523},
abstract = {Web-based Git hosting services such as GitHub and GitLab are popular choices to manage and interact with Git repositories. However, they lack an important security feature - the ability to sign Git commits. Users instruct the server to perform repository operations on their behalf and have to trust that the server will execute their requests faithfully. Such trust may be unwarranted though because a malicious or a compromised server may execute the requested actions in an incorrect manner, leading to a different state of the repository than what the user intended.In this paper, we show a range of high-impact attacks that can be executed stealthily when developers use the web UI of a Git hosting service to perform common actions such as editing files or merging branches. We then propose le-git-imate, a defense against these attacks which provides security guarantees comparable and compatible with Git's standard commit signing mechanism. We implement le-git-imate as a Chrome browser extension. le-git-imate does not require changes on the server side and can thus be used immediately. It also preserves current workflows used in Github/GitLab and does not require the user to leave the browser, and it allows anyone to verify that the server's actions faithfully follow the user's requested actions. Moreover, experimental evaluation using the browser extension shows that le-git-imate has comparable performance with Git's standard commit signature mechanism. With our solution in place, users can take advantage of GitHub/GitLab's web-based features without sacrificing security, thus paving the way towards verifiable web-based Git repositories.},
booktitle = {Proceedings of the 2018 on Asia Conference on Computer and Communications Security},
pages = {469–482},
numpages = {14},
keywords = {github, verification record, browser extension, commit signature},
location = {Incheon, Republic of Korea},
series = {ASIACCS '18}
}

@inproceedings{10.1145/3328778.3372646,
author = {Bentley, Carmen A. and Gehringer, Edward F.},
title = {Promoting Collaborative Skills with Github Project Boards},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372646},
doi = {10.1145/3328778.3372646},
abstract = {Teamwork skills are much in demand in the workplace, even more so with the growth of Agile methods. This calls for giving Computer Science students more practice in the kinds of team scenarios they will encounter on the job. Key for success are hands-on experience with planning methods, prioritization techniques, time management and organization. This poster shows how the cooperative tracking tool Github Project Boards helps teams strategize development, track progress, distribute work evenly, and facilitate collaboration. It also shows how instructors can use Github Project Boards to visualize and evaluate a team's development process.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1332},
numpages = {1},
keywords = {github project boards, tracking systems, collaboration, teaming skills, cooperative tools},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3239235.3240501,
author = {Coelho, Jailton and Valente, Marco Tulio and Silva, Luciana L. and Shihab, Emad},
title = {Identifying Unmaintained Projects in Github},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3240501},
doi = {10.1145/3239235.3240501},
abstract = {Background: Open source software has an increasing importance in modern software development. However, there is also a growing concern on the sustainability of such projects, which are usually managed by a small number of developers, frequently working as volunteers. Aims: In this paper, we propose an approach to identify GitHub projects that are not actively maintained. Our goal is to alert users about the risks of using these projects and possibly motivate other developers to assume the maintenance of the projects. Method: We train machine learning models to identify unmaintained or sparsely maintained projects, based on a set of features about project activity (commits, forks, issues, etc). We empirically validate the model with the best performance with the principal developers of 129 GitHub projects. Results: The proposed machine learning approach has a precision of 80%, based on the feedback of real open source developers; and a recall of 96%. We also show that our approach can be used to assess the risks of projects becoming unmaintained. Conclusions: The model proposed in this paper can be used by open source users and developers to identify GitHub projects that are not actively maintained anymore.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {15},
numpages = {10},
keywords = {github, open source software, unmaintained projects},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/2568225.2568315,
author = {Tsay, Jason and Dabbish, Laura and Herbsleb, James},
title = {Influence of Social and Technical Factors for Evaluating Contribution in GitHub},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568315},
doi = {10.1145/2568225.2568315},
abstract = { Open source software is commonly portrayed as a meritocracy, where decisions are based solely on their technical merit. However, literature on open source suggests a complex social structure underlying the meritocracy. Social work environments such as GitHub make the relationships between users and between users and work artifacts transparent. This transparency enables developers to better use information such as technical value and social connections when making work decisions. We present a study on open source software contribution in GitHub that focuses on the task of evaluating pull requests, which are one of the primary methods for contributing code in GitHub. We analyzed the association of various technical and social measures with the likelihood of contribution acceptance. We found that project managers made use of information signaling both good technical contribution practices for a pull request and the strength of the social connection between the submitter and project manager when evaluating pull requests. Pull requests with many comments were much less likely to be accepted, moderated by the submitter's prior interaction in the project. Well-established projects were more conservative in accepting pull requests. These findings provide evidence that developers use both technical and social information when evaluating potential contributions to open source software projects. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {356–366},
numpages = {11},
keywords = {contribution, open source, signaling theory, social media, social computing, GitHub, transparency},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1109/MSR.2017.15,
author = {Gharehyazie, Mohammad and Ray, Baishakhi and Filkov, Vladimir},
title = {Some from Here, Some from There: Cross-Project Code Reuse in GitHub},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.15},
doi = {10.1109/MSR.2017.15},
abstract = {Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts.To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {291–301},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3350546.3352519,
author = {Horawalavithana, Sameera and Bhattacharjee, Abhishek and Liu, Renhao and Choudhury, Nazim and O. Hall, Lawrence and Iamnitchi, Adriana},
title = {Mentions of Security Vulnerabilities on Reddit, Twitter and GitHub},
year = {2019},
isbn = {9781450369343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350546.3352519},
doi = {10.1145/3350546.3352519},
abstract = {Activity on social media is seen as a relevant sensor for different aspects of the society. In a heavily digitized society, security vulnerabilities pose a significant threat that is publicly discussed on social media. This study presents a comparison of user-generated content related to security vulnerabilities on three digital platforms: two social media conversation channels (Reddit and Twitter) and a collaborative software development platform (GitHub). Our data analysis shows that while more security vulnerabilities are discussed on Twitter, relevant conversations go viral earlier on Reddit. We show that the two social media platforms can be used to accurately predict activity on GitHub. },
booktitle = {IEEE/WIC/ACM International Conference on Web Intelligence},
pages = {200–207},
numpages = {8},
location = {Thessaloniki, Greece},
series = {WI '19}
}

@inproceedings{10.1109/CSMR.2013.41,
author = {Thung, Ferdian and Bissyande, Tegawende F. and Lo, David and Jiang, Lingxiao},
title = {Network Structure of Social Coding in GitHub},
year = {2013},
isbn = {9780769549484},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CSMR.2013.41},
doi = {10.1109/CSMR.2013.41},
abstract = {Social coding enables a different experience of software development as the activities and interests of one developer are easily advertised to other developers. Developers can thus track the activities relevant to various projects in one umbrella site. Such a major change in collaborative software development makes an investigation of networkings on social coding sites valuable. Furthermore, project hosting platforms promoting this development paradigm have been thriving, among which GitHub has arguably gained the most momentum. In this paper, we contribute to the body of knowledge on social coding by investigating the network structure of social coding in GitHub. We collect 100,000 projects and 30,000 developers from GitHub, construct developer-developer and project-project relationship graphs, and compute various characteristics of the graphs. We then identify influential developers and projects on this sub network of GitHub by using PageRank. Understanding how developers and projects are actually related to each other on a social coding site is the first step towards building tool supports to aid social programmers in performing their tasks more efficiently.},
booktitle = {Proceedings of the 2013 17th European Conference on Software Maintenance and Reengineering},
pages = {323–326},
numpages = {4},
keywords = {GitHub, Social coding, developer-developer network, project-project network},
series = {CSMR '13}
}

@inproceedings{10.1145/3183428.3183437,
author = {Wang, Zhendong and Wang, Yi and Redmiles, David},
title = {Competence-Confidence Gap: A Threat to Female Developers' Contribution on Github},
year = {2018},
isbn = {9781450356619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183428.3183437},
doi = {10.1145/3183428.3183437},
abstract = {On GitHub, contributing to a new project is crucial for a developer to gain personal growth and maximize impact in the community. It is known that female developers are often hesitant to explore the opportunities to contribute to new projects even when they possess the competence to make valuable contributions. Drawing from the literature of the competence-confidence gap, we develop a fresh explanation for this phenomenon. We validate the theoretical explanation through an empirical study using GitHub's historical data. In this study, we identify all female developers ranking in top 5,000 GitHub users. Using the Granger Causality Test, we find that, for the majority of identified female developers, initiating a pull request to a new repository is "Granger" caused by the quick increase of followers in the preceding couple of weeks. For most male developers, our observations show that their new pull requests have no relationship with the dynamics of follower numbers. The results indicate that the competence-confidence gap is a threat to female developers' contribution on GitHub. The research suggests that helping female developers to overcome the competence-confidence gap is critical for encouraging female's contribution open source development, as well as growing their reputations and impacts in the community.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Society},
pages = {81–90},
numpages = {10},
keywords = {competence-confidence gap, github, female developers, granger causality},
location = {Gothenburg, Sweden},
series = {ICSE-SEIS '18}
}

@inproceedings{10.1145/2950290.2950334,
author = {Gu, Xiaodong and Zhang, Hongyu and Zhang, Dongmei and Kim, Sunghun},
title = {Deep API Learning},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950334},
doi = {10.1145/2950290.2950334},
abstract = { Developers often wonder how to implement a certain functionality (e.g., how to parse XML files) using APIs. Obtaining an API usage sequence based on an API-related natural language query is very helpful in this regard. Given a query, existing approaches utilize information retrieval models to search for matching API sequences. These approaches treat queries and APIs as bags-of-words and lack a deep understanding of the semantics of the query. We propose DeepAPI, a deep learning based approach to generate API usage sequences for a given natural language query. Instead of a bag-of-words assumption, it learns the sequence of words in a query and the sequence of associated APIs. DeepAPI adapts a neural language model named RNN Encoder-Decoder. It encodes a word sequence (user query) into a fixed-length context vector, and generates an API sequence based on the context vector. We also augment the RNN Encoder-Decoder by considering the importance of individual APIs. We empirically evaluate our approach with more than 7 million annotated code snippets collected from GitHub. The results show that our approach generates largely accurate API sequences and outperforms the related approaches. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {631–642},
numpages = {12},
keywords = {deep learning, API usage, code search, API, RNN},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3274856.3274875,
author = {Steinberg, Boris and Baglij, Anton and Petrenko, Victor and Burkhovetskiy, Victor and Steinberg, Oleg and Metelica, Elena},
title = {An Analyzer for Program Parallelization and Optimization},
year = {2018},
isbn = {9781450365161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3274856.3274875},
doi = {10.1145/3274856.3274875},
abstract = {The article describes new facilities for program optimization and parallelization, work-in-progress modifications of previously implemented program transformations and compiler libraries, and future development of Optimizing parallelizing system (OPS) including opening its source code on GitHub. These new facilities, such as dialog-based optimization and parallelization, user-friendly program dependency visualization (which is needed for high-quality analyzers), parallel code generation for accelerators (GPUs, DSPs, FPGAs, or high performance clusters), are made possible by the fact, that OPS uses high-level intermediate representation as opposed to low-level intermediate representation used in popular compilers.},
booktitle = {Proceedings of the 3rd International Conference on Applications in Information Technology},
pages = {90–95},
numpages = {6},
keywords = {Parallelizing compiler, high-level intermediate representation, program transformations, interactive compiler, reconfigurable architectures, tiling, data locality},
location = {Aizu-Wakamatsu, Japan},
series = {ICAIT'2018}
}

@inproceedings{10.1109/MSR.2017.24,
author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
title = {TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.24},
doi = {10.1109/MSR.2017.24},
abstract = {Continuous Integration (CI) has become a best practice of modern software development. Thanks in part to its tight integration with GitHub, Travis CI has emerged as arguably the most widely used CI platform for Open-Source Software (OSS) development. However, despite its prominent role in Software Engineering in practice, the benefits, costs, and implications of doing CI are all but clear from an academic standpoint. Little research has been done, and even less was of quantitative nature. In order to lay the groundwork for data-driven research on CI, we built TravisTorrent, travistorrent.testroots.org, a freely available data set based on Travis CI and GitHub that provides easy access to hundreds of thousands of analyzed builds from more than 1,000 projects. Unique to TravisTorrent is that each of its 2,640,825 Travis builds is synthesized with meta data from Travis CI's API, the results of analyzing its textual build log, a link to the GitHub commit which triggered the build, and dynamically aggregated project data from the time of commit extracted through GHTorrent.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {447–450},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3236024.3236043,
author = {Basios, Michail and Li, Lingbo and Wu, Fan and Kanthan, Leslie and Barr, Earl T.},
title = {Darwinian Data Structure Selection},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236043},
doi = {10.1145/3236024.3236043},
abstract = {Data structure selection and tuning is laborious but can vastly improve an application’s performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86% (37/43) of the projects. The median improvement across the best solutions is 4.8%, 10.1%, 5.1% for runtime, memory and CPU usage. These aggregate results understate ARTEMIS’s potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache’s XML transformation tool. ARTEMIS improves gson by 16.5%, 1% and 2.2% for memory, runtime, and CPU; ARTEMIS improves xalan’s memory consumption by 23.5%. Every client of these projects will benefit from these performance improvements.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {118–128},
numpages = {11},
keywords = {Software Analysis and Optimisation, Search-based Software Engineering, Data Structure Optimisation, Genetic Improvement},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3385032.3385052,
author = {Dhasade, Akash Balasaheb and Venigalla, Akhila Sri Manasa and Chimalakonda, Sridhar},
title = {Towards Prioritizing GitHub Issues},
year = {2020},
isbn = {9781450375948},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385032.3385052},
doi = {10.1145/3385032.3385052},
abstract = {The vast growth in usage of GitHub by developers to host their projects has led to extensive forking and open source contributions. These contributions occur in the form of issues that report bugs or pull requests to either fix bugs or add new features to the project. On the other hand, massive increase in the number of issues reported by developers and users is a major challenge for integrators, as the number of concurrent issues to be handled is much higher than the number of core contributors. While there exists prior work on prioritizing pull requests, in this paper we make an attempt towards prioritizing issues using machine learning techniques. We present the Issue Prioritizer, a tool to prioritize issues based on three criteria: issue lifetime, issue hotness and category of the issue. We see this work as an initial step towards supporting developers to handle large volumes of issues in projects.},
booktitle = {Proceedings of the 13th Innovations in Software Engineering Conference on Formerly Known as India Software Engineering Conference},
articleno = {18},
numpages = {5},
keywords = {GitHub Issues, Multiple Concurrent Issues, Dynamic Tracking, Automatic Issue Prioritisation, Priority Ranking},
location = {Jabalpur, India},
series = {ISEC 2020}
}

@inproceedings{10.1109/BotSE.2019.00018,
author = {Wessel, Mairieli and Steinmacher, Igor and Wiese, Igor and Gerosa, Marco A.},
title = {Should I Stale or Should I Close? An Analysis of a Bot That Closes Abandoned Issues and Pull Requests},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/BotSE.2019.00018},
doi = {10.1109/BotSE.2019.00018},
abstract = {On GitHub, projects use bots to automate predefined and repetitive tasks related to issues and pull requests. Our research investigates the adoption of the stale bot, which helps maintainers triaging abandoned issues and pull requests. We analyzed the bots' configuration settings and their modifications over time. These settings define the time for tagging issues and pull request as stale and closing them. We collected data from 765 OSS projects hosted on GitHub. Our results indicate that most of the studied projects made no more than three modifications in the configurations file, issues tagged as bug reports are exempt from being considered stale, while the same occurs with pull requests that need some input to be processed.},
booktitle = {Proceedings of the 1st International Workshop on Bots in Software Engineering},
pages = {38–42},
numpages = {5},
keywords = {bots, abandoned issues, open source software},
location = {Montreal, Quebec, Canada},
series = {BotSE '19}
}

@inproceedings{10.1145/3366423.3380145,
author = {Shao, Huajie and Sun, Dachun and Wu, Jiahao and Zhang, Zecheng and Zhang, Aston and Yao, Shuochao and Liu, Shengzhong and Wang, Tianshi and Zhang, Chao and Abdelzaher, Tarek},
title = {Paper2repo: GitHub Repository Recommendation for Academic Papers},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380145},
doi = {10.1145/3366423.3380145},
abstract = {GitHub has become a popular social application platform, where a large number of users post their open source projects. In particular, an increasing number of researchers release repositories of source code related to their research papers in order to attract more people to follow their work. Motivated by this trend, we describe a novel item-item cross-platform recommender system, paper2repo, that recommends relevant repositories on GitHub that match a given paper in an academic search system such as Microsoft Academic. The key challenge is to identify the similarity between an input paper and its related repositories across the two platforms, without the benefit of human labeling. Towards that end, paper2repo integrates text encoding and constrained graph convolutional networks (GCN) to automatically learn and map the embeddings of papers and repositories into the same space, where proximity offers the basis for recommendation. To make our method more practical in real life systems, labels used for model training are computed automatically from features of user actions on GitHub. In machine learning, such automatic labeling is often called distant supervision. To the authors’ knowledge, this is the first distant-supervised cross-platform (paper to repository) matching system. We evaluate the performance of paper2repo on real-world data sets collected from GitHub and Microsoft Academic. Results demonstrate that it outperforms other state of the art recommendation methods.},
booktitle = {Proceedings of The Web Conference 2020},
pages = {629–639},
numpages = {11},
keywords = {text encoding, constrained graph convolutional networks, cross-platform recommendation, Recommender system},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3368089.3409735,
author = {Wang, Jiawei and Li, Li and Liu, Kui and Cai, Haipeng},
title = {Exploring How Deprecated Python Library APIs Are (Not) Handled},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409735},
doi = {10.1145/3368089.3409735},
abstract = {In this paper, we present the first exploratory study of deprecated Python library APIs to understand the status quo of API deprecation in the realm of Python libraries. Specifically, we aim to comprehend how deprecated library APIs are declared and documented in practice by their maintainers, and how library users react to them. By thoroughly looking into six reputed Python libraries and 1,200 GitHub projects, we experimentally observe that API deprecation is poorly handled by library contributors, which subsequently introduce difficulties for Python developers to resolve the usage of deprecated library APIs. This empirical evidence suggests that our community should take immediate actions to appropriately handle the deprecation of Python library APIs.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {233–244},
numpages = {12},
keywords = {Evolution, Python library, Deprecated API, Deprecation},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2901739.2901776,
author = {Cosentino, Valerio and Luis, Javier and Cabot, Jordi},
title = {Findings from GitHub: Methods, Datasets and Limitations},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901776},
doi = {10.1145/2901739.2901776},
abstract = {GitHub, one of the most popular social coding platforms, is the platform of reference when mining Open Source repositories to learn from past experiences. In the last years, a number of research papers have been published reporting findings based on data mined from GitHub. As the community continues to deepen in its understanding of software engineering thanks to the analysis performed on this platform, we believe it is worthwhile to reflect how research papers have addressed the task of mining GitHub repositories over the last years. In this regard, we present a meta-analysis of 93 research papers which addresses three main dimensions of those papers: i) the empirical methods employed, ii) the datasets they used and iii) the limitations reported. Results of our meta-analysis show some concerns regarding the dataset collection process and size, the low level of replicability, poor sampling techniques, lack of longitudinal studies and scarce variety of methodologies.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {137–141},
numpages = {5},
keywords = {GitHub, systematic review, meta-analysis},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1109/ICSME.2014.62,
author = {Vasilescu, Bogdan and Schuylenburg, Stef van and Wulms, Jules and Serebrenik, Alexander and Brand, Mark G. J. van den},
title = {Continuous Integration in a Social-Coding World: Empirical Evidence from GitHub},
year = {2014},
isbn = {9781479961467},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSME.2014.62},
doi = {10.1109/ICSME.2014.62},
abstract = {Continuous integration is a software engineering practice of frequently merging all developer working copies with a shared main branch, e.g., several times a day. With the advent of GitHub, a platform well known for its "social coding" features that aid collaboration and sharing, and currently the largest code host in the open source world, collaborative software development has never been more prominent. In GitHub development one can distinguish between two types of developer contributions to a project: direct ones, coming from a typically small group of developers with write access to the main project repository, and indirect ones, coming from developers who fork the main repository, update their copies locally, and submit pull requests for review and merger. In this paper we explore how GitHub developers use continuous integration as well as whether the contribution type (direct versus indirect) and different project characteristics (e.g., main programming language, or project age) are associated with the success of the automatic builds.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution},
pages = {401–405},
numpages = {5},
keywords = {automatic build, continuous integration, GitHub, collaborative software development},
series = {ICSME '14}
}

@inproceedings{10.1145/3338906.3342495,
author = {Vandenbogaerde, Bram},
title = {A Graph-Based Framework for Analysing the Design of Smart Contracts},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342495},
doi = {10.1145/3338906.3342495},
abstract = {Used as a platform for executing smart contracts, Blockchain technology has yielded new programming languages. We propose a graph-based framework for computing software design metrics for the Solidity programming language, and use this framework in a preliminary study on 505 smart contracts mined from GitHub. The results show that most of the smart contracts are rather straightforward from an objected-oriented point of view and that new design metrics specific to smart contracts should be developed.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1220–1222},
numpages = {3},
keywords = {Metrics, Smart Contracts, Mining Software Repositories},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/3432601.3432616,
author = {Podolskiy, Vladimir and Patrou, Maria and Patros, Panos and Gerndt, Michael and Kent, Kenneth B.},
title = {The Weakest Link: Revealing and Modeling the Architectural Patterns of Microservice Applications},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {Cloud microservice applications comprise interconnected services packed into containers. Such applications generate complex communication patterns among their microservices. Studying such patterns can support assuring various quality attributes, such as autoscaling for satisfying performance, availability and scalability, or targeted penetration testing for satisfying security and correctness. We study the structure of containerized microservice applications via providing the methodology and the results of a structural graph-based analysis of 103 Docker Compose deployment files from open-sourced Github repositories. Our findings indicate the dominance of a power-law distribution of microservice interconnections. Further analysis highlights the suitability of the Barab\'{a}si-Albert model for generating large random graphs that model the architecture of real microservice applications. The exhibited structures and their usage for engineering microservice applications are discussed.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {113–122},
numpages = {10},
keywords = {software vulnerability, application topology, microservice, cloud-native application},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1109/MSR.2017.2,
author = {Zampetti, Fiorella and Scalabrino, Simone and Oliveto, Rocco and Canfora, Gerardo and Di Penta, Massimiliano},
title = {How Open Source Projects Use Static Code Analysis Tools in Continuous Integration Pipelines},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.2},
doi = {10.1109/MSR.2017.2},
abstract = {Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a "softer" mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {334–344},
numpages = {11},
keywords = {open source projects, static analysis tools, continuous integration, empirical study},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3196398.3196436,
author = {Cohen, Eldan and Consens, Mariano P.},
title = {Large-Scale Analysis of the Co-Commit Patterns of the Active Developers in Github's Top Repositories},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196436},
doi = {10.1145/3196398.3196436},
abstract = {GitHub, the largest code hosting site (with 25 million public active repositories and contributions from 6 million active users), provides an unprecedented opportunity to observe the collaboration patterns of software developers. Understanding the patterns behind the social coding phenomena is an active research area where the insights gained can guide the design of better collaboration tools, and can also help to identify and select developer talent. In this paper, we present a large-scale analysis of the co-commit patterns in GitHub. We analyze 10 million commits made by 200 thousand developers to 16 thousand repositories, using 17 of the most popular programming languages over a period of 3 years. Although a large volume of data is included in our study, we pay close attention to the participation criteria for repositories and developers. We select repositories by reputation (based on star ranking), and we introduce the notion of active developer in GitHub (observing that a limited subset of developers is responsible for the vast majority of the commits). Using co-authorship networks, we analyze the co-commit patterns of the active developer network for each programming language. We observe that the active developer networks are less connected and more centralized than the general GitHub developer networks, and that the patterns vary significantly among languages. We compare our results to other collaborative environments (Wikipedia and scientific research networks), and we also describe the evolution of the co-commit patterns over time.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {426–436},
numpages = {11},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3379597.3387464,
author = {Wang, Peipei and Brown, Chris and Jennings, Jamie A. and Stolee, Kathryn T.},
title = {An Empirical Study on Regular Expression Bugs},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387464},
doi = {10.1145/3379597.3387464},
abstract = {Understanding the nature of regular expression (regex) issues is important to tackle practical issues developers face in regular expression usage. Knowledge about the nature and frequency of various types of regular expression issues, such as those related to performance, API misuse, and code smells, can guide testing, inform documentation writers, and motivate refactoring efforts. However, beyond ReDoS (Regular expression Denial of Service), little is known about to what extent regular expression issues affect software development and how these issues are addressed in practice.This paper presents a comprehensive empirical study of 350 merged regex-related pull requests from Apache, Mozilla, Facebook, and Google GitHub repositories. Through classifying the root causes and manifestations of those bugs, we show that incorrect regular expression behavior is the dominant root cause of regular expression bugs (165/356, 46.3%). The remaining root causes are incorrect API usage (9.3%) and other code issues that require regular expression changes in the fix (29.5%). By studying the code changes of regex-related pull requests, we observe that fixing regular expression bugs is nontrivial as it takes more time and more lines of code to fix them compared to the general pull requests. The results of this study contribute to a broader understanding of the practical problems faced by developers when using regular expressions.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {103–113},
numpages = {11},
keywords = {pull requests, bug fixes, Regular expression bug characteristics},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.5555/2486788.2486964,
author = {Pham, Raphael and Singer, Leif and Schneider, Kurt},
title = {Building Test Suites in Social Coding Sites by Leveraging Drive-by Commits},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { GitHub projects attract contributions from a community of users with varying coding and quality assurance skills. Developers on GitHub feel a need for automated tests and rely on test suites for regression testing and continuous integration. However, project owners report to often struggle with implementing an exhaustive test suite. Convincing contributors to provide automated test cases remains a challenge. The absence of an adequate test suite or using tests of low quality can degrade the quality of the software product. We present an approach for reducing the effort required by project owners for extending their test suites. We aim to utilize the phenomenon of drive-by commits: capable users quickly and easily solve problems in others' projects---even though they are not particularly involved in that project---and move on. By analyzing and directing the drive-by commit phenomenon, we hope to use crowdsourcing to improve projects' quality assurance efforts. Valuable test cases and maintenance tasks would be completed by capable users, giving core developers more resources to work on the more complicated issues. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1209–1212},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3092703.3098221,
author = {Walsh, Thomas A. and Kapfhammer, Gregory M. and McMinn, Phil},
title = {ReDeCheck: An Automatic Layout Failure Checking Tool for Responsively Designed Web Pages},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098221},
doi = {10.1145/3092703.3098221},
abstract = { Since people frequently access websites with a wide variety of devices (e.g., mobile phones, laptops, and desktops), developers need frameworks and tools for creating layouts that are useful at many viewport widths. While responsive web design (RWD) principles and frameworks facilitate the development of such sites, there is a lack of tools supporting the detection of failures in their layout. Since the quality assurance process for responsively designed websites is often manual, time-consuming, and error-prone, this paper presents ReDeCheck, an automated layout checking tool that alerts developers to both potential unintended regressions in responsive layout and common types of layout failure. In addition to summarizing ReDeCheck’s benefits, this paper explores two different usage scenarios for this tool that is publicly available on GitHub. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {360–363},
numpages = {4},
keywords = {layout failures, presentation failures, Responsive web design},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1109/ICSE.2019.00046,
author = {Zhang, Tianyi and Yang, Di and Lopes, Crista and Kim, Miryung},
title = {Analyzing and Supporting Adaptation of Online Code Examples},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00046},
doi = {10.1109/ICSE.2019.00046},
abstract = {Developers often resort to online Q&amp;A forums such as Stack Overflow (SO) for filling their programming needs. Although code examples on those forums are good starting points, they are often incomplete and inadequate for developers' local program contexts; adaptation of those examples is necessary to integrate them to production code. As a consequence, the process of adapting online code examples is done over and over again, by multiple developers independently. Our work extensively studies these adaptations and variations, serving as the basis for a tool that helps integrate these online code examples in a target context in an interactive manner.We perform a large-scale empirical study about the nature and extent of adaptations and variations of SO snippets. We construct a comprehensive dataset linking SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. We then qualitatively inspect 400 SO examples and their GitHub counterparts and develop a taxonomy of 24 adaptation types. Using this taxonomy, we build an automated adaptation analysis technique on top of GumTree to classify the entire dataset into these types. We build a Chrome extension called ExampleStack that automatically lifts an adaptation-aware template from each SO example and its GitHub counterparts to identify hot spots where most changes happen. A user study with sixteen programmers shows that seeing the commonalities and variations in similar GitHub counterparts increases their confidence about the given SO example, and helps them grasp a more comprehensive view about how to reuse the example differently and avoid common pitfalls.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {316–327},
numpages = {12},
keywords = {code adaptation, online code examples},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.5555/2878379.2878382,
author = {Matteis, Luca and Verborgh, Ruben},
title = {Hosting Queryable and Highly Available Linked Data for Free},
year = {2014},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {SPARQL endpoints suffer from low availability, and require to buy and configure complex servers to host them. With the advent of Linked Data Fragments, and more specifically Triple Pattern Fragments (TPFs), we can now perform complex queries on low-cost servers. Online file repositories and cloud hosting services, such as GitHub, Google Code, Google App Engine or Dropbox can be exploited to host this type of linked data for free. For this purpose we have developed two different proof-of-concept tools that can be used to publish TPFs on GitHub and Google App Engine. A generic TPF client can then be used to perform SPARQL queries on the freely hosted TPF servers.},
booktitle = {Proceedings of the 2014 International Conference on Developers - Volume 1268},
pages = {13–18},
numpages = {6},
keywords = {availability, querying, hosting, linked data},
location = {Riva del Garda, Italy},
series = {ISWC-DEV'14}
}

@inproceedings{10.1109/ICSE.2019.00079,
author = {Imtiaz, Nasif and Middleton, Justin and Chakraborty, Joymallya and Robson, Neill and Bai, Gina and Murphy-Hill, Emerson},
title = {Investigating the Effects of Gender Bias on GitHub},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00079},
doi = {10.1109/ICSE.2019.00079},
abstract = {Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature. We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub, then evaluate those hypotheses quantitatively. While our results show that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {700–711},
numpages = {12},
keywords = {open source, gender, GitHub},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3196398.3196455,
author = {Yu, Yue and Li, Zhixing and Yin, Gang and Wang, Tao and Wang, Huaimin},
title = {A Dataset of Duplicate Pull-Requests in Github},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196455},
doi = {10.1145/3196398.3196455},
abstract = {In GitHub, the pull-based development model enables community contributors to collaborate in a more efficient way. However, the distributed and parallel characteristics of this model pose a potential risk for developers to submit duplicate pull-requests (PRs), which increase the extra cost of project maintenance. To facilitate the further studies to better understand and solve the issues introduced by duplicate PRs, we construct a large dataset of historical duplicate PRs extracted from 26 popular open source projects in GitHub by using a semi-automatic approach. Furthermore, we present some preliminary applications to illustrate how further researches can be conducted based on this dataset.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {22–25},
numpages = {4},
keywords = {duplicate pull-request, github, distributed software development},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/SCAM.2014.15,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K.},
title = {On the Use of Context in Recommending Exception Handling Code Examples},
year = {2014},
isbn = {9781479961481},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SCAM.2014.15},
doi = {10.1109/SCAM.2014.15},
abstract = {Studies show that software developers often either misuse exception handling features or use them inefficiently, and such a practice may lead an undergoing software project to a fragile, insecure and non-robust application system. In this paper, we propose a context-aware code recommendation approach that recommends exception handling code examples from a number of popular open source code repositories hosted at GitHub. It collects the code examples exploiting GitHub code search API, and then analyzes, filters and ranks them against the code under development in the IDE by leveraging not only the structural (i.e., graph-based) and lexical features but also the heuristic quality measures of exception handlers in the examples. Experiments with 4,400 code examples and 65 exception handling scenarios as well as comparisons with four existing approaches show that the proposed approach is highly promising.},
booktitle = {Proceedings of the 2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation},
pages = {285–294},
numpages = {10},
keywords = {context-relevance, structural similarity, lexical similarity, Exception handler},
series = {SCAM '14}
}

@inproceedings{10.1145/3361242.3361244,
author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
title = {Exploring the Relationship Between Developer Activities and Profile Images on GitHub},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3361244},
doi = {10.1145/3361242.3361244},
abstract = {In the GitHub platform, social media profile images are one of many visual components of developers. Besides, developer activities such as reporting issues or following other developers are regarded as important development and self-expression behaviors. However, to the best of our knowledge, no study has yet been conducted to study the relationship between GitHub developer activities and profile images. In this paper, we aim to investigate the relationship between developer activities and profile images to gain some insights into the developers' internal properties. During our experiments, we manually classify profile images into seven categories. Next, we investigate the relationship between developer's demographic information and developer activity. Further, using logistic regression analysis, when controlled for various variables, we statistically identify and quantify the relationships between developer activities and profile image categories. We find that several profile image categories significantly correlate with developer's demographic information and activities. We also provide a rich resource of research ideas for further study. Our examination and analysis provide insights into the developers' internal properties when using different profile images. Moreover, this study is the first step in understanding the relationship between developer activities and profile images on GitHub.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {7},
numpages = {10},
keywords = {Developer activity, GitHub, Internal property, Profile image},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1109/ICPC.2019.00037,
author = {Chen, Di and Stolee, Kathryn T. and Menzies, Tim},
title = {Replication Can Improve Prior Results: A GitHub Study of Pull Request Acceptance},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00037},
doi = {10.1109/ICPC.2019.00037},
abstract = {Crowdsourcing and data mining can be used to effectively reduce the effort associated with the partial replication and enhancement of qualitative studies.For example, in a primary study, other researchers explored factors influencing the fate of GitHub pull requests using an extensive qualitative analysis of 20 pull requests. Guided by their findings, we mapped some of their qualitative insights onto quantitative questions. To determine how well their findings generalize, we collected much more data (170 additional pull requests from 142 GitHub projects). Using crowdsourcing, that data was augmented with subjective qualitative human opinions about how pull requests extended the original issue. The crowd's answers were then combined with quantitative features and, using data mining, used to build a predictor for whether code would be merged. That predictor was far more accurate than the one built from the primary study's qualitative factors (F1=90 vs 68%), illustrating the value of a mixed-methods approach and replication to improve prior results.To test the generality of this approach, the next step in future work is to conduct other studies that extend qualitative studies with crowdsourcing and data mining.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {179–190},
numpages = {12},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/3395027.3419581,
author = {Cuculovic, Milos and Fondement, Frederic and Devanne, Maxime and Weber, Jonathan and Hassenforder, Michel},
title = {Change Detection on JATS Academic Articles: An XML Diff Comparison Study},
year = {2020},
isbn = {9781450380003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395027.3419581},
doi = {10.1145/3395027.3419581},
abstract = {XML is currently a well established and widely used document format. It is used as a core data container in collaborative writing suites and other modern information architectures. The extraction and analysis of differences between two XML document versions is an attractive topic, and has already been tackled by several research groups. The goal of this study is to compare 12 existing state-of-the-art and commercial XML diff algorithms by applying them to JATS documents in order to extract and evaluate changes between two versions of the same academic article. Understanding changes between two article versions is important not only regarding data, but also semantics. Change information consumers in our case are editorial teams, and thus they are more generally interested in change semantics than in the exact data changes. The existing algorithms are evaluated on the following aspects: their edit detection suitability for both text and tree changes, execution speed, memory usage and delta file size. The evaluation process is supported by a Python tool available on Github.},
booktitle = {Proceedings of the ACM Symposium on Document Engineering 2020},
articleno = {5},
numpages = {10},
keywords = {semantic diff, JATS, change control, XML diff, document comparison, academic publishing},
location = {Virtual Event, CA, USA},
series = {DocEng '20}
}

@inproceedings{10.1145/3397537.3397551,
author = {Mattis, Toni and Rein, Patrick and Hirschfeld, Robert},
title = {Three Trillion Lines: Infrastructure for Mining GitHub in the Classroom},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397551},
doi = {10.1145/3397537.3397551},
abstract = {The increasing interest in collaborative software development on platforms like GitHub has led to the availability of large amounts of data about development activities. The GHTorrent project has recorded a significant proportion of GitHub’s public event stream and hosts the currently largest public dataset of meta-data about open-source development. We describe our infrastructure that makes this data locally available to researchers and students, examples for research activities carried out on this infrastructure, and what we learned from building the system. We identify a need for domain-specific tools, especially databases, that can deal with large-scale code repositories and associated meta-data and outline open challenges to use them more effectively for research and machine learning settings.},
booktitle = {Conference Companion of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {1–6},
numpages = {6},
keywords = {TravisCI, GitHub, Big Code, Teaching, Repository Mining},
location = {Porto, Portugal},
series = {20}
}

@inproceedings{10.1145/3338698.3338892,
author = {Lowe-Power, Jason and Nitta, Christopher},
title = {The Davis In-Order (DINO) CPU: A Teaching-Focused RISC-V CPU Design},
year = {2019},
isbn = {9781450368421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338698.3338892},
doi = {10.1145/3338698.3338892},
abstract = {The DINO CPU is an open source teaching-focused RISC-V CPU design available on GitHub (https://github.com/jlpteaching/dinocpu). We have used the DINO CPU in the computer architecture course at UC Davis for two quarters with two separate instructors. In this paper, we present details of the DINO CPU, the tools included with the DINO CPU, and our experiences using the DINO CPU.},
booktitle = {Proceedings of the Workshop on Computer Architecture Education},
articleno = {2},
numpages = {8},
location = {Phoenix, AZ, USA},
series = {WCAE'19}
}

@inproceedings{10.1145/3426422.3426981,
author = {Rak-amnouykit, Ingkarat and McCrevan, Daniel and Milanova, Ana and Hirzel, Martin and Dolby, Julian},
title = {Python 3 Types in the Wild: A Tale of Two Type Systems},
year = {2020},
isbn = {9781450381758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426422.3426981},
doi = {10.1145/3426422.3426981},
abstract = {Python 3 is a highly dynamic language, but it has introduced a syntax for expressing types with PEP484. This paper explores how developers use these type annotations, the type system semantics provided by type checking and inference tools, and the performance of these tools. We evaluate the types and tools on a corpus of public GitHub repositories. We review MyPy and PyType, two canonical static type checking and inference tools, and their distinct approaches to type analysis. We then address three research questions: (i) How often and in what ways do developers use Python 3 types? (ii) Which type errors do developers make? (iii) How do type errors from different tools compare?  Surprisingly, when developers use static types, the code rarely type-checks with either of the tools. MyPy and PyType exhibit false positives, due to their static nature, but also flag many useful errors in our corpus. Lastly, MyPy and PyType embody two distinct type systems, flagging different errors in many cases. Understanding the usage of Python types can help guide tool-builders and researchers. Understanding the performance of popular tools can help increase the adoption of static types and tools by practitioners, ultimately leading to more correct and more robust Python code.},
booktitle = {Proceedings of the 16th ACM SIGPLAN International Symposium on Dynamic Languages},
pages = {57–70},
numpages = {14},
keywords = {Python, type inference, type checking},
location = {Virtual, USA},
series = {DLS 2020}
}

@inproceedings{10.1109/MSR.2017.13,
author = {Yang, Di and Martins, Pedro and Saini, Vaibhav and Lopes, Cristina},
title = {Stack Overflow in Github: Any Snippets There?},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.13},
doi = {10.1109/MSR.2017.13},
abstract = {When programmers look for how to achieve certain programming tasks, Stack Overflow is a popular destination in search engine results. Over the years, Stack Overflow has accumulated an impressive knowledge base of snippets of code that are amply documented. We are interested in studying how programmers use these snippets of code in their projects. Can we find Stack Overflow snippets in real projects? When snippets are used, is this copy literal or does it suffer adaptations? And are these adaptations specializations required by the idiosyncrasies of the target artifact, or are they motivated by specific requirements of the programmer? The large-scale study presented on this paper analyzes 909k non-fork Python projects hosted on Github, which contain 290M function definitions, and 1.9M Python snippets captured in Stack Overflow. Results are presented as quantitative analysis of block-level code cloning intra and inter Stack Overflow and GitHub, and as an analysis of programming behaviors through the qualitative analysis of our findings.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {280–290},
numpages = {11},
keywords = {code clone, code reuse, large-scale analysis},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1109/SMC.2019.8914586,
author = {Liu, Renhao and Mubang, Frederick and Hall, Lawrence O. and Horawalavithana, Sameera and Iamnitchi, Adriana and Skvoretz, John},
title = {Predicting Longitudinal User Activity at Fine Time Granularity in Online Collaborative Platforms},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SMC.2019.8914586},
doi = {10.1109/SMC.2019.8914586},
abstract = {This paper introduces a decomposition approach to address the problem of predicting different user activities at hour granularity over a long period of time. Our approach involves two steps. First, we used a temporal neural network ensemble to predict the number of each type of activity that occurred in a day. Second, we used a set of neural networks to assign the events to a user-repository pair in a particular hour. We focused this work on a subset of the public GitHub dataset that records the activities of over 2 million users on over 400,000 software repositories. Our experiments show we were able to predict hourly user-repo activity with reasonably low error. Our simulations are accurate for 1–3 weeks (168–504 hours) after inception, with accuracy gradually falling off. It was shown that activity on Twitter and Reddit increases the accuracy of activity prediction on GitHub for most events.},
booktitle = {2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)},
pages = {2535–2542},
numpages = {8},
location = {Bari, Italy}
}

@inproceedings{10.1145/2950290.2983929,
author = {Oosterwaal, Sebastiaan and Deursen, Arie van and Coelho, Roberta and Sawant, Anand Ashok and Bacchelli, Alberto},
title = {Visualizing Code and Coverage Changes for Code Review},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983929},
doi = {10.1145/2950290.2983929},
abstract = { One of the tasks of reviewers is to verify that code modifications are well tested. However, current tools offer little support in understanding precisely how changes to the code relate to changes to the tests. In particular, it is hard to see whether (modified) test code covers the changed code. To mitigate this problem, we developed Operias, a tool that provides a combined visualization of fine-grained source code differences and coverage impact. Operias works both as a stand-alone tool on specific project versions and as a service hooked to GitHub. In the latter case, it provides automated reports for each new pull request, which reviewers can use to assess the code contribution. Operias works for any Java project that works with maven and its standard Cobertura coverage plugin. We present how Operias could be used to identify test-related problems in real-world pull requests. Operias is open source and available on GitHub with a demo video: https://github.com/SERG-Delft/operias },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {1038–1041},
numpages = {4},
keywords = {code review, software testing, software evolution},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.5555/2820518.2820570,
author = {Sinha, Vibha Singhal and Saha, Diptikalyan and Dhoolia, Pankaj and Padhye, Rohan and Mani, Senthil},
title = {Detecting and Mitigating Secret-Key Leaks in Source Code Repositories},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Several news articles in the past year highlighted incidents in which malicious users stole API keys embedded in files hosted on public source code repositories such as GitHub and BitBucket in order to drive their own work-loads for free. While some service providers such as Amazon have started taking steps to actively discover such developer carelessness by scouting public repositories and suspending leaked API keys, there is little support for tackling the problem from the code sharing platforms themselves.In this paper, we discuss practical solutions to detecting, preventing and fixing API key leaks. We first outline a handful of methods for detecting API keys embedded within source code, and evaluate their effectiveness using a sample set of projects from GitHub. Second, we enumerate the mechanisms which could be used by developers to prevent or fix key leaks in code repositories manually. Finally, we outline a possible solution that combines these techniques to provide tool support for protecting against key leaks in version control systems.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {396–400},
numpages = {5},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3377816.3381732,
author = {Raman, Naveen and Cao, Minxuan and Tsvetkov, Yulia and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {Stress and Burnout in Open Source: Toward Finding, Understanding, and Mitigating Unhealthy Interactions},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381732},
doi = {10.1145/3377816.3381732},
abstract = {Developers from open-source communities have reported high stress levels from frequent demands for features and bug fixes and from the sometimes aggressive tone of these demands. Toxic conversations may demotivate and burn out developers, creating challenges for sustaining open source. We outline a path toward finding, understanding, and possibly mitigating such unhealthy interactions. We take a first step toward finding them, by developing and demonstrating a measurement instrument (an SVM classifier tailored for software engineering) to detect toxic discussions in GitHub issues. We used our classifier to analyze trends over time and in different GitHub communities, finding that toxicity varies by community and that toxicity decreased between 2012 and 2018.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {57–60},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.1145/3338906.3338977,
author = {Chen, Zhenpeng and Cao, Yanbin and Lu, Xuan and Mei, Qiaozhu and Liu, Xuanzhe},
title = {SEntiMoji: An Emoji-Powered Learning Approach for Sentiment Analysis in Software Engineering},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338977},
doi = {10.1145/3338906.3338977},
abstract = {Sentiment analysis has various application scenarios in software engineering (SE), such as detecting developers' emotions in commit messages and identifying their opinions on Q&amp;A forums. However, commonly used out-of-the-box sentiment analysis tools cannot obtain reliable results on SE tasks and the misunderstanding of technical jargon is demonstrated to be the main reason. Then, researchers have to utilize labeled SE-related texts to customize sentiment analysis for SE tasks via a variety of algorithms. However, the scarce labeled data can cover only very limited expressions and thus cannot guarantee the analysis quality. To address such a problem, we turn to the easily available emoji usage data for help. More specifically, we employ emotional emojis as noisy labels of sentiments and propose a representation learning approach that uses both Tweets and GitHub posts containing emojis to learn sentiment-aware representations for SE-related texts. These emoji-labeled posts can not only supply the technical jargon, but also incorporate more general sentiment patterns shared across domains. They as well as labeled data are used to learn the final sentiment classifier. Compared to the existing sentiment analysis methods used in SE, the proposed approach can achieve significant improvement on representative benchmark datasets. By further contrast experiments, we find that the Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource, but try to transform knowledge from the open domain through ubiquitous signals such as emojis.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {841–852},
numpages = {12},
keywords = {Software engineering, Emoji, Sentiment analysis},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3368089.3409689,
author = {Huang, Kaifeng and Chen, Bihuan and Shi, Bowen and Wang, Ying and Xu, Congying and Peng, Xin},
title = {Interactive, Effort-Aware Library Version Harmonization},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409689},
doi = {10.1145/3368089.3409689},
abstract = {As a mixed result of intensive dependency on third-party libraries, flexible mechanisms to declare dependencies and increased number of modules in a project, different modules of a project directly depend on multiple versions of the same third-party library. Such library version inconsistencies could increase dependency maintenance cost, or even lead to dependency conflicts when modules are inter-dependent. Although automated build tools (e.g., Maven's enforcer plugin) provide partial support to detect library version inconsistencies, they do not provide any support to harmonize inconsistent library versions.  We first conduct a survey with 131 Java developers from GitHub to retrieve first-hand information about the root causes, detection methods, reasons for fixing or not fixing, fixing strategies, fixing efforts, and tool expectations on library version inconsistencies. Then, based on the insights from our survey, we propose LibHarmo, an interactive, effort-aware library version harmonization technique, to detect library version inconsistencies, interactively suggest a harmonized version with the least harmonization efforts based on library API usage analysis, and refactor build configuration files.  LibHarmo is currently developed for Java Maven projects. Our experimental study on 443 highly-starred Java Maven projects from GitHub shows that i) LibHarmo detected 621 library version inconsistencies in 152 (34.3%) projects with a false positive rate of 16.8%, while Maven's enforcer plugin only detected 219 of them; and ii) LibHarmo saved 87.5% of the harmonization efforts. Further, 31 library version inconsistencies have been confirmed, and 17 of them have been already harmonized by developers.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {518–529},
numpages = {12},
keywords = {Library Version Harmonization, Third-Party Libraries},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3350768.3350781,
author = {Fazzolino, Rafael and Rodrigues, Gena\'{\i}na Nunes},
title = {Feature-Trace: Generating Operational Profile and Supporting Testing Prioritization from BDD Features},
year = {2019},
isbn = {9781450376518},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3350768.3350781},
doi = {10.1145/3350768.3350781},
abstract = {Operational Profiles provide quantitative information about how the software will be used, which supports highlighting those software components more sensitive to reliability based on their profile usage. However, the generation of Operational Profiles usually requires a considerable team effort to liaise requirements specification until their reification into expected software artifacts. In this sense, it becomes paramount in the software life cycle the ability to seamlessly or efficiently perform traceability from requirement to code, embracing the testing process as a means to ensure that the requirements are satisfiably covered and addressed. In this work, we propose the Feature-Trace approach which merges the advantages of the Operational Profile and the benefits of the requirements-to-code traceability present in the BDD (Behavior-Driven Development) approach. The primary goal of our work is to use the BDD approach as an information source for the semi-automated generation of the Operational Profile. The proposed approach was evaluated on the Diaspora software, on a GitHub open source software. The case study revealed that the Feature-Trace approach is capable of extracting the operational profile seamlessly from the specified Diaspora's BDD features as well as obtaining and presenting vital information to guide the process of test cases prioritization.},
booktitle = {Proceedings of the XXXIII Brazilian Symposium on Software Engineering},
pages = {332–336},
numpages = {5},
keywords = {requirements traceability, behavior-driven development, testing, operational profile},
location = {Salvador, Brazil},
series = {SBES 2019}
}

@inproceedings{10.1145/3340631.3394884,
author = {Lee, Roy Ka-Wei and Hoang, Thong and Oentaryo, Richard J. and Lo, David},
title = {Keen2Act: Activity Recommendation in Online Social Collaborative Platforms},
year = {2020},
isbn = {9781450368612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340631.3394884},
doi = {10.1145/3340631.3394884},
abstract = {Social collaborative platforms such as GitHub and Stack Overflow have been increasingly used to improve work productivity via collaborative efforts. To improve user experiences in these platforms, it is desirable to have a recommender system that can suggest not only items (e.g., a GitHub repository) to a user, but also activities to be performed on the suggested items (e.g., forking a repository). To this end, we propose a new approach dubbed Keen2Act, which decomposes the recommendation problem into two stages: the Keen and Act steps. The Keen step identifies, for a given user, a (sub)set of items in which he/she is likely to be interested. The Act step then recommends to the user which activities to perform on the identified set of items. This decomposition provides a practical approach to tackling complex activity recommendation tasks while producing higher recommendation quality. We evaluate our proposed approach using two real-world datasets and obtain promising results whereby Keen2Act outperforms several baseline models.},
booktitle = {Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
pages = {308–312},
numpages = {5},
keywords = {activity recommendation, factorization machine, GitHub, social collaborative platform, stack overflow},
location = {Genoa, Italy},
series = {UMAP '20}
}

@inproceedings{10.1145/3238147.3240488,
author = {Janes, Andrea and Mairegger, Michael and Russo, Barbara},
title = {Code_call_lens: Raising the Developer Awareness of Critical Code},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240488},
doi = {10.1145/3238147.3240488},
abstract = {As a developer, it is often complex to foresee the impact of changes in source code on usage, e.g., it is time-consuming to find out all components that will be impacted by a change or estimate the impact on the usability of a failing piece of code. It is therefore hard to decide how much effort in quality assurance is justifiable to obtain the desired business goals. In this paper, to reduce the difficulty for developers to understand the importance of source code, we propose an automated way to provide this information to developers as they are working on a given piece of code. As a proof-of-concept, we developed a plug-in for Microsoft Visual Studio Code that informs about the importance of source code methods based on the frequency of usage by the end-users of the developed software. The plug-in aims to increase the awareness developers have about the importance of source code in an unobtrusive way, helping them to prioritize their effort to quality assurance, technical excellence, and usability. code_call_lens can be downloaded from GitHub at https://github.com/xxMUROxx/vscode.code_call_lens.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {876–879},
numpages = {4},
keywords = {User tracking, Empirical Software Engineering},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.5220/0005830604640471,
author = {Cruz, Guilherme A. Maldonado da and Huzita, Elisa Hatsue Moriya and Feltrim, Val\'{e}ria D.},
title = {Estimating Trust in Virtual Teams},
year = {2016},
isbn = {9789897581878},
publisher = {SCITEPRESS - Science and Technology Publications, Lda},
address = {Setubal, PRT},
url = {https://doi.org/10.5220/0005830604640471},
doi = {10.5220/0005830604640471},
abstract = {The advance in technology has enabled the emergence of virtual teams. In these teams, people are in different places and possibly over different time zones, making use of computer mediated communication. At the same time distribution brings benefits, there are some challenges as the difficulty to develop trust, which is essential for efficiency in these teams. In this scenario, trust information could be used to allocate members in a new team and/or, to monitor them during the project execution. In this paper we present an automatic framework for detecting trust between members of global software development teams using sentiment analysis from comments and profile data available in versioning systems. Besides the framework description, we also present its implementation for the GitHub versioning system.},
booktitle = {Proceedings of the 18th International Conference on Enterprise Information Systems},
pages = {464–471},
numpages = {8},
keywords = {Global Software Development., Sentiment Analysis, Trust, Versioning System, Virtual Teams},
location = {Rome, Italy},
series = {ICEIS 2016}
}

@inproceedings{10.1145/2889160.2891035,
author = {Rastogi, Ayushi},
title = {Do Biases Related to Geographical Location Influence Work-Related Decisions in GitHub?},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2891035},
doi = {10.1145/2889160.2891035},
abstract = {Visible demographic characteristics are seen as elements of bias in offline work environments. In this study, we investigate the influence of the geographical location on the evaluation of pull requests in GitHub -- the most popular online collaborative code development environment. We use a mixed-methods approach and present analyses of 70,000+ pull requests and 2,500+ survey responses. Quantitative analysis of GitHub projects' data suggests that the geographical location significantly explains the pull request acceptance decisions. These observations are in agreement with the perceptions of submitters based on their experiences with bias. Integrators feel that it is easy to work with contributors from the same geographical location and that they encourage contributors from the same geographical location. However, integrators do not feel that contributors from some countries are better at writing pull requests compared to others.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {665–667},
numpages = {3},
keywords = {software process, empirical studies, empirical software engineering},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3021460.3021477,
author = {Agrawall, Akash and Chaitanya, Krishna and Agrawal, Arnav Kumar and Choppella, Venkatesh},
title = {Mitigating Browser-Based DDoS Attacks Using CORP},
year = {2017},
isbn = {9781450348560},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3021460.3021477},
doi = {10.1145/3021460.3021477},
abstract = {On March 27, 2015, Github witnessed a massive DDoS attack, the largest in Github's history till date. In this incident, browsers and users were used as vectors to launch the attack. In this paper, we analyse such browser-based DDoS attacks and simulate them in a lab environment. Existing browser security policies like Same Origin Policy (SOP), Content Security Policy (CSP) do not mitigate these attacks by design. In this paper we observe that CORP (Cross Origin Request Policy), a browser security policy, can be used to mitigate these attacks. CORP enables a server to control cross-origin interactions initiated by a browser. The browser intercepts the cross-origin requests and blocks unwanted requests by the server. This takes the load off the server to mitigate the attack.},
booktitle = {Proceedings of the 10th Innovations in Software Engineering Conference},
pages = {137–146},
numpages = {10},
keywords = {DDoS, Javascript, MITM (Man in the middle), Browser, Cross-origin requests, Browser-based DDoS},
location = {Jaipur, India},
series = {ISEC '17}
}

@inproceedings{10.1145/3377811.3380406,
author = {Henkel, Jordan and Bird, Christian and Lahiri, Shuvendu K. and Reps, Thomas},
title = {Learning from, Understanding, and Supporting DevOps Artifacts for Docker},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380406},
doi = {10.1145/3377811.3380406},
abstract = {With the growing use of DevOps tools and frameworks, there is an increased need for tools and techniques that support more than code. The current state-of-the-art in static developer assistance for tools like Docker is limited to shallow syntactic validation. We identify three core challenges in the realm of learning from, understanding, and supporting developers writing DevOps artifacts: (i) nested languages in DevOps artifacts, (ii) rule mining, and (iii) the lack of semantic rule-based analysis. To address these challenges we introduce a toolset, binnacle, that enabled us to ingest 900,000 GitHub repositories.Focusing on Docker, we extracted approximately 178,000 unique Dockerfiles, and also identified a Gold Set of Dockerfiles written by Docker experts. We addressed challenge (i) by reducing the number of effectively uninterpretable nodes in our ASTs by over 80% via a technique we call phased parsing. To address challenge (ii), we introduced a novel rule-mining technique capable of recovering two-thirds of the rules in a benchmark we curated. Through this automated mining, we were able to recover 16 new rules that were not found during manual rule collection. To address challenge (iii), we manually collected a set of rules for Dockerfiles from commits to the files in the Gold Set. These rules encapsulate best practices, avoid docker build failures, and improve image size and build latency. We created an analyzer that used these rules, and found that, on average, Dockerfiles on GitHub violated the rules five times more frequently than the Dockerfiles in our Gold Set. We also found that industrial Dockerfiles fared no better than those sourced from GitHub.The learned rules and analyzer in binnacle can be used to aid developers in the IDE when creating Dockerfiles, and in a post-hoc fashion to identify issues in, and to improve, existing Dockerfiles.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {38–49},
numpages = {12},
keywords = {docker, mining, static checking, DevOps},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.5555/3400397.3400677,
author = {Eckman, David J. and Henderson, Shane G. and Pasupathy, Raghu},
title = {Redesigning a Testbed of Simulation-Optimization Problems and Solvers for Experimental Comparisons},
year = {2019},
isbn = {9781728132839},
publisher = {IEEE Press},
abstract = {We describe major improvements to the testing capabilities of SimOpt, a library of simulation-optimization problems and solvers. Foremost among these improvements is a transition to GitHub that makes SimOpt easier to use and maintain. We also design two new wrapper functions that facilitate empirical comparisons of solvers. The wrapper functions make extensive use of common random numbers (CRN) both within and across solvers for various purposes; e.g., identifying random initial solutions and running simulation replications. We examine some of the intricacies of using CRN to compare simulation-optimization solvers.},
booktitle = {Proceedings of the Winter Simulation Conference},
pages = {3457–3467},
numpages = {11},
location = {National Harbor, Maryland},
series = {WSC '19}
}

@inproceedings{10.5555/3016387.3016512,
author = {Amir, Ofra and Grosz, Barbara J. and Gajos, Krzysztof Z.},
title = {MIP-Nets: Enabling Information Sharing in Loosely-Coupled Teamwork},
year = {2016},
publisher = {AAAI Press},
abstract = {People collaborate in carrying out such complex activities as treating patients, co-authoring documents and developing software. While technologies such as Dropbox and Github enable groups to work in a distributed manner, coordinating team members' individual activities poses significant challenges. In this paper, we formalize the problem of "information sharing in loosely-coupled extended-duration teamwork". We develop a new representation, Mutual Influence Potential Networks (MIP-Nets), to model collaboration patterns and dependencies among activities, and an algorithm, MIP-DOI, that uses this representation to reason about information sharing.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {4192–4193},
numpages = {2},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@inproceedings{10.1145/3379597.3387512,
author = {Bhattacharjee, Avijit and Nath, Sristy Sumana and Zhou, Shurui and Chakroborti, Debasish and Roy, Banani and Roy, Chanchal K. and Schneider, Kevin},
title = {An Exploratory Study to Find Motives Behind Cross-Platform Forks from Software Heritage Dataset},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387512},
doi = {10.1145/3379597.3387512},
abstract = {The fork-based development mechanism provides the flexibility and the unified processes for software teams to collaborate easily in a distributed setting without too much coordination overhead. Currently, multiple social coding platforms support fork-based development, such as GitHub, GitLab, and Bitbucket. Although these different platforms virtually share the same features, they have different emphasis. As GitHub is the most popular platform and the corresponding data is publicly available, most of the current studies are focusing on GitHub hosted projects. However, we observed anecdote evidences that people are confused about choosing among these platforms, and some projects are migrating from one platform to another, and the reasons behind these activities remain unknown. With the advances of Software Heritage Graph Dataset (SWHGD), we have the opportunity to investigate the forking activities across platforms. In this paper, we conduct an exploratory study on 10 popular open-source projects to identify cross-platform forks and investigate the motivation behind. Preliminary result shows that cross-platform forks do exist. For the 10 subject systems used in this study, we found 81,357 forks in total among which 179 forks are on GitLab. Based on our qualitative analysis, we found that most of the cross-platform forks that we identified are mirrors of the repositories on another platform, but we still find cases that were created due to preference of using certain functionalities (e.g. Continuous Integration (CI)) supported by different platforms. This study lays the foundation of future research directions, such as understanding the differences between platforms and supporting cross-platform collaboration.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {11–15},
numpages = {5},
keywords = {Social coding, Cross-platform forks, OSS, Collaboration},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/2991041.2991044,
author = {Niephaus, Fabio and Henrichs, Dale and Taeumel, Marcel and Pape, Tobias and Felgentreff, Tim and Hirschfeld, Robert},
title = {SmalltalkCI: A Continuous Integration Framework for Smalltalk Projects},
year = {2016},
isbn = {9781450345248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991041.2991044},
doi = {10.1145/2991041.2991044},
abstract = {Continuous integration (CI) is a programming practice that reduces the risk of project failure by integrating code changes multiple times a day. This has always been important to the Smalltalk community, so custom integration infrastructures are operated that allow CI testing for Smalltalk projects shared in Monticello repositories or traditional changesets.In the last few years, the open hosting platform GitHub has become more and more popular for Smalltalk projects. Unfortunately, there was no convenient way to enable CI testing for those projects.We present smalltalkCI, a continuous integration framework for Smalltalk. It aims to provide a uniform way to load and test Smalltalk projects written in different Smalltalk dialects. smalltalkCI runs on Linux, macOS, and on Windows and can be used locally as well as on a remote server. In addition, it is compatible with Travis CI and AppVeyor, which allows developers to easily set up free CI testing for their GitHub projects without having to run a custom integration infrastructure.},
booktitle = {Proceedings of the 11th Edition of the International Workshop on Smalltalk Technologies},
articleno = {3},
numpages = {9},
keywords = {Travis CI, AppVeyor, Continuous Integration, Smalltalk, Coverage Testing},
location = {Prague, Czech Republic},
series = {IWST'16}
}

@inproceedings{10.1109/COMM48946.2020.9142009,
author = {Avram, Andrei-Marius and Morogan, Luciana and Toma, Stefan-Adrian},
title = {OpenNIG - Open Neural Image Generator},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/COMM48946.2020.9142009},
doi = {10.1109/COMM48946.2020.9142009},
abstract = {Generative models are statistical models that learn a true underlying data distribution from samples using unsupervised learning, aiming to generate new data points with some variation. In this paper, we introduce OpenNIG (Open Neural Image Generator), an open-source neural networks toolkit for image generation. It offers the possibility to easily train, validate and test state of the art models. The framework also contains a module that enables the user to directly download and process some of the most common databases used in deep learning. OpenNIG is freely available via GitHub.},
booktitle = {2020 13th International Conference on Communications (COMM)},
pages = {177–181},
numpages = {5},
location = {Bucharest, Romania}
}

@inproceedings{10.1145/3384217.3384219,
author = {Shakya, Raunak and Rahman, Akond},
title = {A Preliminary Taxonomy of Techniques Used in Software Fuzzing},
year = {2020},
isbn = {9781450375610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384217.3384219},
doi = {10.1145/3384217.3384219},
abstract = {Software fuzzing is a testing technique, which generates erroneous and random input to a software so that the software of interest can be monitored for exceptions such as crashes [1]. Both in the open source software (OSS) and proprietary domain, fuzzing has been widely used to explore software vulnerabilities. For example, information technology (IT) organizations such as Google1 and Microsoft2 use software fuzzing as part of the software development process. As of Jan 2019, GitHub hosts 2,915 OSS repositories related to fuzzing3.},
booktitle = {Proceedings of the 7th Symposium on Hot Topics in the Science of Security},
articleno = {14},
numpages = {2},
keywords = {scoping review, fuzzing, taxonomy, software security},
location = {Lawrence, Kansas},
series = {HotSoS '20}
}

@inproceedings{10.1109/ICSE.2019.00047,
author = {Horton, Eric and Parnin, Chris},
title = {DockerizeMe: Automatic Inference of Environment Dependencies for Python Code Snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00047},
doi = {10.1109/ICSE.2019.00047},
abstract = {Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50% of public Python gists fail due to an import error for a missing library.We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {328–338},
numpages = {11},
keywords = {python, configuration management, dependencies, docker, environment inference},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1109/MSR.2019.00057,
author = {Alqaimi, Anwar and Thongtanunam, Patanamon and Treude, Christoph},
title = {Automatically Generating Documentation for Lambda Expressions in Java},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00057},
doi = {10.1109/MSR.2019.00057},
abstract = {When lambda expressions were introduced to the Java programming language as part of the release of Java 8 in 2014, they were the language's first step into functional programming. Since lambda expressions are still relatively new, not all developers use or understand them. In this paper, we first present the results of an empirical study to determine how frequently developers of GitHub repositories make use of lambda expressions and how they are documented. We find that 11% of Java GitHub repositories use lambda expressions, and that only 6% of the lambda expressions are accompanied by source code comments. We then present a tool called LambdaDoc which can automatically detect lambda expressions in a Java repository and generate natural language documentation for them. Our evaluation of LambdaDoc with 23 professional developers shows that they perceive the generated documentation to be complete, concise, and expressive, while the majority of the documentation produced by our participants without tool support was inadequate. Our contribution builds an important step towards automatically generating documentation for functional programming constructs in an object-oriented language.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {310–320},
numpages = {11},
keywords = {lambda expressions, documentation generation},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/2702123.2702549,
author = {Vasilescu, Bogdan and Posnett, Daryl and Ray, Baishakhi and van den Brand, Mark G.J. and Serebrenik, Alexander and Devanbu, Premkumar and Filkov, Vladimir},
title = {Gender and Tenure Diversity in GitHub Teams},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702549},
doi = {10.1145/2702123.2702549},
abstract = {Software development is usually a collaborative venture. Open Source Software (OSS) projects are no exception; indeed, by design, the OSS approach can accommodate teams that are more open, geographically distributed, and dynamic than commercial teams. This, we find, leads to OSS teams that are quite diverse. Team diversity, predominantly in offline groups, is known to correlate with team output, mostly with positive effects. How about in OSS? Using GitHub, the largest publicly available collection of OSS projects, we studied how gender and tenure diversity relate to team productivity and turnover. Using regression modeling of GitHub data and the results of a survey, we show that both gender and tenure diversity are positive and significant predictors of productivity, together explaining a sizable fraction of the data variability. These results can inform decision making on all levels, leading to better outcomes in recruiting and performance.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {3789–3798},
numpages = {10},
keywords = {gender, productivity, diversity, github, open source},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{10.1145/1985441.1985476,
author = {Heller, Brandon and Marschner, Eli and Rosenfeld, Evan and Heer, Jeffrey},
title = {Visualizing Collaboration and Influence in the Open-Source Software Community},
year = {2011},
isbn = {9781450305747},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985441.1985476},
doi = {10.1145/1985441.1985476},
abstract = {We apply visualization techniques to user profiles and repository metadata from the GitHub source code hosting service. Our motivation is to identify patterns within this development community that might otherwise remain obscured. Such patterns include the effect of geographic distance on developer relationships, social connectivity and influence among cities, and variation in projectspecific contribution styles (e.g., centralized vs. distributed). Our analysis examines directed graphs in which nodes represent users' geographic locations and edges represent (a) follower relationships, (b) successive commits, or (c) contributions to the same project. We inspect this data using a set of visualization techniques: geo-scatter maps, small multiple displays, and matrix diagrams. Using these representations, and tools based on them, we develop hypotheses about the larger GitHub community that would be difficult to discern using traditional lists, tables, or descriptive statistics. These methods are not intended to provide conclusive answers; instead, they provide a way for researchers to explore the question space and communicate initial insights.},
booktitle = {Proceedings of the 8th Working Conference on Mining Software Repositories},
pages = {223–226},
numpages = {4},
keywords = {social graph, github, open source, collaboration, visualization, data exploration, mapping, geoscatter},
location = {Waikiki, Honolulu, HI, USA},
series = {MSR '11}
}

@inproceedings{10.1145/2884781.2884826,
author = {Gousios, Georgios and Storey, Margaret-Anne and Bacchelli, Alberto},
title = {Work Practices and Challenges in Pull-Based Development: The Contributor's Perspective},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884826},
doi = {10.1145/2884781.2884826},
abstract = {The pull-based development model is an emerging way of contributing to distributed software projects that is gaining enormous popularity within the open source software (OSS) world. Previous work has examined this model by focusing on projects and their owners---we complement it by examining the work practices of project contributors and the challenges they face.We conducted a survey with 645 top contributors to active OSS projects using the pull-based model on GitHub, the prevalent social coding site. We also analyzed traces extracted from corresponding GitHub repositories. Our research shows that: contributors have a strong interest in maintaining awareness of project status to get inspiration and avoid duplicating work, but they do not actively propagate information; communication within pull requests is reportedly limited to low-level concerns and contributors often use communication channels external to pull requests; challenges are mostly social in nature, with most reporting poor responsiveness from integrators; and the increased transparency of this setting is a confirmed motivation to contribute. Based on these findings, we present recommendations for practitioners to streamline the contribution process and discuss potential future research directions.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {285–296},
numpages = {12},
keywords = {distributed software development, pull-based development, pull request, GitHub, open source contribution},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2993283.2993285,
author = {North, Kevin J. and Sarma, Anita and Cohen, Myra B.},
title = {Understanding Git History: A Multi-Sense View},
year = {2016},
isbn = {9781450343978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993283.2993285},
doi = {10.1145/2993283.2993285},
abstract = { Version control systems archive data about the development history of a project, which can be used to analyze and understand different facets of a software project. The project history can be used to evaluate the development process of a team, as an aid in bug fixing, or to help new members get on track with development. However, state of the art techniques for analyzing version control data provide only partial views into this information, and lack an easy way to present all the dimensions of the data. In this paper we present GitVS, a hybrid view that incorporates visualization and sonification to represent the multiple dimensions of version control data - development time line, conflicts, etc. In a formative user study comparing the GitHub Network Graph, GitVS, and a version of GitVS without sound, we show GitVS improves over the GitHub Network Graph and that while sound makes it easier to correctly understand version history for some tasks, it is more difficult for others. },
booktitle = {Proceedings of the 8th International Workshop on Social Software Engineering},
pages = {1–7},
numpages = {7},
keywords = {Sonification, Conflicts, Version Control History},
location = {Seattle, WA, USA},
series = {SSE 2016}
}

@inproceedings{10.1145/3368089.3409722,
author = {Brown, Chris and Parnin, Chris},
title = {Understanding the Impact of GitHub Suggested Changes on Recommendations between Developers},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409722},
doi = {10.1145/3368089.3409722},
abstract = {Recommendations between colleagues are effective for encouraging developers to adopt better practices. Research shows these peer interactions are useful for improving developer behaviors, or the adoption of activities to help software engineers complete programming tasks. However, in-person recommendations between developers in the workplace are declining. One form of online recommendations between developers are pull requests, which allow users to propose code changes and provide feedback on contributions. GitHub, a popular code hosting platform, recently introduced the suggested changes feature, which allows users to recommend improvements for pull requests. To better understand this feature and its impact on recommendations between developers, we report an empirical study of this system, measuring usage, effectiveness, and perception. Our results show that suggested changes support code review activities and significantly impact the timing and communication between developers on pull requests. This work provides insight into the suggested changes feature and implications for improving future systems for automated developer recommendations, such as providing situated, concise, and actionable feedback.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1065–1076},
numpages = {12},
keywords = {developer recommendations, online programming communities, Empirical software engineering, GitHub, suggested changes},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1109/BIBM.2015.7359904,
author = {Cui, Xiaodong and Zhen Wei and Zhang, Lin and Liu, Hui and Lei Sun and Zhang, Shao-Wu and Huang, Yufei and Meng, Jia},
title = {Sketching the Distribution of Transcriptomic Features on RNA Transcripts with Travis Coordinates},
year = {2015},
isbn = {9781467367998},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/BIBM.2015.7359904},
doi = {10.1109/BIBM.2015.7359904},
abstract = {Biological features, such as, genes, transcription factor binding sites, SNPs, etc., are usually denoted with genome-based coordinates as the genomic features. While genome-based representation is usually very effective, it can be tedious to examine the distribution of RNA-related genomic features on RNA transcripts with existing tools due to the conversion and comparison between genome-based coordinates to RNA-based coordinates. We developed here an open source R package Travis for sketching the transcriptomic view of genomic features so as to facilitate the analysis of RNA-related but genome-based coordinates. Internally, Travis package extracts the coordinates relative to the landmarks of transcripts, with which the distribution of RNA-related genomic features can then be conveniently analyzed. We demonstrated the usage of Travis package in analyzing post-transcriptional RNA modifications (5-MethylCytosine and N6-MethylAdenosine) derived from high-throughput sequencing approaches (MeRIP-Seq and RNA BS-Seq). The Travis R package is now publicly available from GitHub: https://github.com/lzcyzm/Travis.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
pages = {1536–1542},
numpages = {7},
series = {BIBM '15}
}

@inproceedings{10.1145/2970276.2970283,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K. and Redl, Jesse and Collins, Jason A.},
title = {CORRECT: Code Reviewer Recommendation at GitHub for Vendasta Technologies},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970283},
doi = {10.1145/2970276.2970283},
abstract = { Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (e.g., external library experience) of a developer but also her experience in certain specialized technologies (e.g., Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the request, mines two relevant histories, and then returns a ranked list of appropriate code reviewers for the request within the browser's context. Demo: https://www.youtube.com/watch?v=rXU1wTD6QQ0 },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {792–797},
numpages = {6},
keywords = {specialized technology experience, pull request, GitHub, cross-project experience, Code reviewer recommendation},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3364641.3364650,
author = {Favato, Danilo and Ishitani, Daniel and Oliveira, Johnatan and Figueiredo, Eduardo},
title = {Linus's Law: More Eyes Fewer Flaws in Open Source Projects},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364650},
doi = {10.1145/3364641.3364650},
abstract = {Linus's Law states that "given enough eyeballs, all bugs are shallow". In other words, given a large enough number of developers, almost every programming flaw is characterized and fixed quickly. Although there is much debate about this subject, we still lack empirical evidence to support this law. Given that this theme has, and still is, motivating business decisions in software development, we investigate the implications of Linus's Law in two empirical studies on open source projects mined from GitHub. In the first pilot study, we mined seven popular Java projects from GitHub and investigated the correlation between committers and programming flaws in source code files. Results of this pilot study suggest a positive correlation between the number of developers and programming flaws. We cross-validate these results in a second study with almost one hundred Python projects from GitHub. In this second study, we analyzed the correlation between the number of forks - i.e., a proxy for number of developers - and programming flaws identified in projects. In both studies, programming flaws were detected by using static code analysis tools. As a result of the second study, we could not observe a correlation between the number of developers and the number of programming flaws in Python projects. From both studies we conclude that we were unable to find evidence to support the Linus's Law.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {69–78},
numpages = {10},
keywords = {Linus's law, python, software quality, java},
location = {Fortaleza, Brazil},
series = {SBQS'19}
}

@inproceedings{10.1109/ASE.2019.00032,
author = {Saifullah, C M Khaled and Asaduzzaman, Muhammad and Roy, Chanchal K.},
title = {Learning from Examples to Find Fully Qualified Names of API Elements in Code Snippets},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00032},
doi = {10.1109/ASE.2019.00032},
abstract = {Developers often reuse code snippets from online forums, such as Stack Overflow, to learn API usages of software frameworks or libraries. These code snippets often contain ambiguous undeclared external references. Such external references make it difficult to learn and use those APIs correctly. In particular, reusing code snippets containing such ambiguous undeclared external references requires significant manual efforts and expertise to resolve them. Manually resolving fully qualified names (FQN) of API elements is a non-trivial task. In this paper, we propose a novel context-sensitive technique, called COSTER, to resolve FQNs of API elements in such code snippets. The proposed technique collects locally specific source code elements as well as globally related tokens as the context of FQNs, calculates likelihood scores, and builds an occurrence likelihood dictionary (OLD). Given an API element as a query, COSTER captures the context of the query API element, matches that with the FQNs of API elements stored in the OLD, and rank those matched FQNs leveraging three different scores: likelihood, context similarity, and name similarity scores. Evaluation with more than 600K code examples collected from GitHub and two different Stack Overflow datasets shows that our proposed technique improves precision by 4-6% and recall by 3-22% compared to state-of-the-art techniques. The proposed technique significantly reduces the training time compared to the StatType, a state-of-the-art technique, without sacrificing accuracy. Extensive analyses on results demonstrate the robustness of the proposed technique.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {243–254},
numpages = {12},
keywords = {fully qualified name, context sensitive technique, recommendation system, API usages},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1109/ICSE.2017.16,
author = {Khatchadourian, Raffi and Masuhara, Hidehiko},
title = {Automated Refactoring of Legacy Java Software to Default Methods},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.16},
doi = {10.1109/ICSE.2017.16},
abstract = {Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {82–93},
numpages = {12},
keywords = {refactoring, default methods, interfaces, Java},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/2597073.2597119,
author = {Matragkas, Nicholas and Williams, James R. and Kolovos, Dimitris S. and Paige, Richard F.},
title = {Analysing the 'biodiversity' of Open Source Ecosystems: The GitHub Case},
year = {2014},
isbn = {9781450328630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2597073.2597119},
doi = {10.1145/2597073.2597119},
abstract = { In nature the diversity of species and genes in ecological communities affects the functioning of these communities. Biologists have found out that more diverse communities appear to be more productive than less diverse communities. Moreover such communities appear to be more stable in the face of perturbations. In this paper, we draw the analogy between ecological communities and Open Source Software (OSS) ecosystems, and we investigate the diversity and structure of OSS communities. To address this question we use the MSR 2014 challenge dataset, which includes data from the top-10 software projects for the top programming languages on GitHub. Our findings show that OSS communities on GitHub consist of 3 types of users (core developers, active users, passive users). Moreover, we show that the percentage of core developers and active users does not change as the project grows and that the majority of members of large projects are passive users. },
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories},
pages = {356–359},
numpages = {4},
keywords = {Data mining, Data and knowledge visualization},
location = {Hyderabad, India},
series = {MSR 2014}
}

@inproceedings{10.1145/3092703.3098230,
author = {Casalnuovo, Casey and Suchak, Yagnik and Ray, Baishakhi and Rubio-Gonz\'{a}lez, Cindy},
title = {GitcProc: A Tool for Processing and Classifying GitHub Commits},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098230},
doi = {10.1145/3092703.3098230},
abstract = { Sites such as GitHub have created a vast collection of software artifacts that researchers interested in understanding and improving software systems can use. Current tools for processing such GitHub data tend to target project metadata and avoid source code processing, or process source code in a manner that requires significant effort for each language supported. This paper presents GitcProc, a lightweight tool based on regular expressions and source code blocks, which downloads projects and extracts their project history, including fine-grained source code information and development time bug fixes. GitcProc can track changes to both single-line and block source code structures and associate these changes to the surrounding function context with minimal set up required from users. We demonstrate GitcProc's ability to capture changes in multiple languages by evaluating it on C, C++, Java, and Python projects, and show it finds bug fixes and the context of source code changes effectively with few false positives. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {396–399},
numpages = {4},
keywords = {Git Mining Tool, Information Extraction, Language Independence},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/3097983.3106683,
author = {Pafka, Szil\'{a}rd},
title = {Machine Learning Software in Practice: Quo Vadis?},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3106683},
doi = {10.1145/3097983.3106683},
abstract = {Due to the hype in our industry in the last couple of years, there is a growing mismatch between software tools machine learning practitioners wish for, what they would truly need for their work, what's available (either commercially or open source) and what tool developers and researchers focus on. In this talk we will give a couple of examples of this mismatch. Several surveys and anecdotal evidence show that most practitioners work most of the time (at least in the modeling phase) with datasets that t in the RAM of a single server, therefore distributed computing tools are very of- ten overkill. Our benchmarks (available on github [1]) of the most widely used open source tools for binary classification (various implementations of algorithms such as linear methods, random forests, gradient boosted trees and neural networks) on such data show over 10x speed and over 10x RAM usage difference between various tools, with "big data" tools being the most inefficient. Significant performance gains have been obtained by those tools that incorporate various low-level (close to CPU and memory architecture) optimizations. Nevertheless, we will show that even the best tools show degrading performance on the multi-socket servers featuring a high number of cores, systems that have become widely accessible more recently. Finally, while most of this talk is about performance, we will also argue that machine learning tools that feature high-level easy-to-use APIs provide increasing productivity for practitioners and therefore are preferable.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {25},
numpages = {1},
keywords = {binary classification, accuracy, training speed, memory footprint, software implementations},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/2786805.2786850,
author = {Vasilescu, Bogdan and Yu, Yue and Wang, Huaimin and Devanbu, Premkumar and Filkov, Vladimir},
title = {Quality and Productivity Outcomes Relating to Continuous Integration in GitHub},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786850},
doi = {10.1145/2786805.2786850},
abstract = { Software processes comprise many steps; coding is followed by building, integration testing, system testing, deployment, operations, among others. Software process integration and automation have been areas of key concern in software engineering, ever since the pioneering work of Osterweil; market pressures for Agility, and open, decentralized, software development have provided additional pressures for progress in this area. But do these innovations actually help projects? Given the numerous confounding factors that can influence project performance, it can be a challenge to discern the effects of process integration and automation. Software project ecosystems such as GitHub provide a new opportunity in this regard: one can readily find large numbers of projects in various stages of process integration and automation, and gather data on various influencing factors as well as productivity and quality outcomes. In this paper we use large, historical data on process metrics and outcomes in GitHub projects to discern the effects of one specific innovation in process automation: continuous integration. Our main finding is that continuous integration improves the productivity of project teams, who can integrate more outside contributions, without an observable diminishment in code quality. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {805–816},
numpages = {12},
keywords = {Continuous integration, pull requests, GitHub},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3357384.3357971,
author = {Gong, Qingyuan and Zhang, Jiayun and Chen, Yang and Li, Qi and Xiao, Yu and Wang, Xin and Hui, Pan},
title = {Detecting Malicious Accounts in Online Developer Communities Using Deep Learning},
year = {2019},
isbn = {9781450369763},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357384.3357971},
doi = {10.1145/3357384.3357971},
abstract = {Online developer communities like GitHub provide services such as distributed version control and task management, which allow a massive number of developers to collaborate online. However, the openness of the communities makes themselves vulnerable to different types of malicious attacks, since the attackers can easily join and interact with legitimate users. In this work, we formulate the malicious account detection problem in online developer communities, and propose GitSec, a deep learning-based solution to detect malicious accounts. GitSec distinguishes malicious accounts from legitimate ones based on the account profiles as well as dynamic activity characteristics. On one hand, GitSec makes use of users' descriptive features from the profiles. On the other hand, GitSec processes users' dynamic behavioral data by constructing two user activity sequences and applying a parallel neural network design to deal with each of them, respectively. An attention mechanism is used to integrate the information generated by the parallel neural networks. The final judgement is made by a decision maker implemented by a supervised machine learning-based classifier. Based on the real-world data of GitHub users, our extensive evaluations show that GitSec is an accurate detection system, with an F1-score of 0.922 and an AUC value of 0.940.},
booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
pages = {1251–1260},
numpages = {10},
keywords = {social networks, online developer community, malicious account detection, deep learning},
location = {Beijing, China},
series = {CIKM '19}
}

@inproceedings{10.1145/3377811.3380926,
author = {Nguyen, Son and Phan, Hung and Le, Trinh and Nguyen, Tien N.},
title = {Suggesting Natural Method Names to Check Name Consistencies},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380926},
doi = {10.1145/3377811.3380926},
abstract = {Misleading names of the methods in a project or the APIs in a software library confuse developers about program functionality and API usages, leading to API misuses and defects. In this paper, we introduce MNire, a machine learning approach to check the consistency between the name of a given method and its implementation. MNire first generates a candidate name and compares the current name against it. If the two names are sufficiently similar, we consider the method as consistent. To generate the method name, we draw our ideas and intuition from an empirical study on the nature of method names in a large dataset. Our key finding is that high proportions of the tokens of method names can be found in the three contexts of a given method including its body, the interface (the method's parameter types and return type), and the enclosing class' name. Even when such tokens are not there, MNire uses the contexts to predict the tokens due to the high likelihoods of their co-occurrences. Our unique idea is to treat the name generation as an abstract summarization on the tokens collected from the names of the program entities in the three above contexts.We conducted several experiments to evaluate MNire in method name consistency checking and in method name recommending on large datasets with +14M methods. In detecting inconsistency method names, MNire improves the state-of-the-art approach by 10.4% and 11% relatively in recall and precision, respectively. In method name recommendation, MNire improves relatively over the state-of-the-art technique, code2vec, in both recall (18.2% higher) and precision (11.1% higher). To assess MNire's usefulness, we used it to detect inconsistent methods and suggest new names in several active, GitHub projects. We made 50 pull requests (PRs) and received 42 responses. Among them, five PRs were merged into the main branch, and 13 were approved for later merging. In total, in 31/42 cases, the developer teams agree that our suggested names are more meaningful than the current names, showing MNire's usefulness.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1372–1384},
numpages = {13},
keywords = {naturalness of source code, program entity name suggestion, deep learning},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2487788.2487832,
author = {Venkataramani, Rahul and Gupta, Atul and Asadullah, Allahbaksh and Muddu, Basavaraju and Bhat, Vasudev},
title = {Discovery of Technical Expertise from Open Source Code Repositories},
year = {2013},
isbn = {9781450320382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2487788.2487832},
doi = {10.1145/2487788.2487832},
abstract = {Online Question and Answer websites for developers have emerged as the main forums for interaction during the software development process. The veracity of an answer in such websites is typically verified by the number of 'upvotes' that the answer garners from peer programmers using the same forum. Although this mechanism has proved to be extremely successful in rating the usefulness of the answers, it does not lend itself very elegantly to model the expertise of a user in a particular domain. In this paper, we propose a model to rank the expertise of the developers in a target domain by mining their activity in different opensource projects. To demonstrate the validity of the model, we built a recommendation system for StackOverflow which uses the data mined from GitHub.},
booktitle = {Proceedings of the 22nd International Conference on World Wide Web},
pages = {97–98},
numpages = {2},
keywords = {source code repository, recommendations, technical expertise, knowledge discovery, github, stackoverflow},
location = {Rio de Janeiro, Brazil},
series = {WWW '13 Companion}
}

@inproceedings{10.1145/3305366.3328067,
author = {Koch, Sebastian and Schneider, Teseo and Williams, Francis and Panozzo, Daniele},
title = {Geometric Computing with Python},
year = {2019},
isbn = {9781450363075},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3305366.3328067},
doi = {10.1145/3305366.3328067},
abstract = {This course is a group endeavor by Sebastian Koeh, Teseo Sehneider, Francis Williams, and Daniele Panozzo. Please contact us if you have questions or comments. For troubleshooting, please post an issue on github. We are grateful to the authors of all open souree C++ libraries we are using. In particular, libigl, tetwild, polyfem, pybind11, and Jupyter.The course will mainly use• igl (Section 2)• polyfem (Section 3)• ABC Dataset CAD Processing (Section 4)• TetWild• 3D ViewerWe provide documentation for the first 3 libraries in these course notes and we refer to https://geometryprocessing.github.io/geometric-computing-python/ for a complete and live version.},
booktitle = {ACM SIGGRAPH 2019 Courses},
articleno = {11},
numpages = {45},
location = {Los Angeles, California},
series = {SIGGRAPH '19}
}

@inproceedings{10.1109/TechDebt.2019.00031,
author = {Ericsson, Morgan and Wingkvist, Anna},
title = {TDMentions: A Dataset of Technical Debt Mentions in Online Posts},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/TechDebt.2019.00031},
doi = {10.1109/TechDebt.2019.00031},
abstract = {The term technical debt is easy to understand as a metaphor, but can quickly grow complex in practice. We contribute with a dataset, TDMentions, that enables researchers to study how developers and end users use the term technical debt in online posts and discussions. The dataset consists of posts from news aggregators and Q&amp;A-sites, blog posts, and issues and commits on GitHub.},
booktitle = {Proceedings of the Second International Conference on Technical Debt},
pages = {123–124},
numpages = {2},
keywords = {technical debt, data mining, social networks},
location = {Montreal, Quebec, Canada},
series = {TechDebt '19}
}

@inproceedings{10.1145/3159450.3159595,
author = {Heckman, Sarah and King, Jason},
title = {Developing Software Engineering Skills Using Real Tools for Automated Grading},
year = {2018},
isbn = {9781450351034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3159450.3159595},
doi = {10.1145/3159450.3159595},
abstract = {Situated learning theory supports engaging students with materials and resources that reflect professional standards and best practices. Starting with our introductory courses, we incorporate situated learning to support student engagement in software engineering practices and processes through the use of industrial strength open-source tools in several classes throughout the undergraduate computer science curriculum at NC State University. Additionally, these tools support several logistical and educational needs in computer science classrooms, including assignment submission systems and automated grading. In this tools paper, we present our Canary Framework for supporting software engineering practices through the use of Eclipse for development; GitHub for submission and collaboration; and Jenkins for continuous integration and automated grading. These tools are used in five of ten core courses by more than 3000 students over ten semesters. While the use of these tools in education is not unique, we want to share our model of using professional tools in a classroom setting and our experiences on how this framework can support multiple courses throughout the curriculum and at scale.},
booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
pages = {794–799},
numpages = {6},
keywords = {version control, automated grading, software engineering best practice, continuous integration},
location = {Baltimore, Maryland, USA},
series = {SIGCSE '18}
}

@inproceedings{10.1109/MSR.2019.00038,
author = {Baltes, Sebastian and Treude, Christoph and Diehl, Stephan},
title = {SOTorrent: Studying the Origin, Evolution, and Usage of Stack Overflow Code Snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00038},
doi = {10.1109/MSR.2019.00038},
abstract = {Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of copyable code snippets. Like other software artifacts, code on SO evolves over time, for example when bugs are fixed or APIs are updated to the most recent version. To be able to analyze how code and the surrounding text on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text and code blocks. It connects code snippets from SO posts to other platforms by aggregating URLs from surrounding text blocks and comments, and by collecting references from GitHub files to SO posts. Our vision is that researchers will use SOTorrent to investigate and understand the evolution and maintenance of code on SO and its relation to other platforms such as GitHub.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {191–194},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3379597.3387473,
author = {Gonzalez, Danielle and Zimmermann, Thomas and Nagappan, Nachiappan},
title = {The State of the ML-Universe: 10 Years of Artificial Intelligence &amp; Machine Learning Software Development on GitHub},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387473},
doi = {10.1145/3379597.3387473},
abstract = {In the last few years, artificial intelligence (AI) and machine learning (ML) have become ubiquitous terms. These powerful techniques have escaped obscurity in academic communities with the recent onslaught of AI &amp; ML tools, frameworks, and libraries that make these techniques accessible to a wider audience of developers. As a result, applying AI &amp; ML to solve existing and emergent problems is an increasingly popular practice. However, little is known about this domain from the software engineering perspective. Many AI &amp; ML tools and applications are open source, hosted on platforms such as GitHub that provide rich tools for large-scale distributed software development. Despite widespread use and popularity, these repositories have never been examined as a community to identify unique properties, development patterns, and trends.In this paper, we conducted a large-scale empirical study of AI &amp; ML Tool (700) and Application (4,524) repositories hosted on GitHub to develop such a characterization. While not the only platform hosting AI &amp; ML development, GitHub facilitates collecting a rich data set for each repository with high traceability between issues, commits, pull requests and users. To compare the AI &amp; ML community to the wider population of repositories, we also analyzed a set of 4,101 unrelated repositories. We enhance this characterization with an elaborate study of developer workflow that measures collaboration and autonomy within a repository. We've captured key insights of this community's 10 year history such as it's primary language (Python) and most popular repositories (Tensorflow, Tesseract). Our findings show the AI &amp; ML community has unique characteristics that should be accounted for in future research.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {431–442},
numpages = {12},
keywords = {GitHub, software engineering, artificial intelligence, machine learning, mining software repositories, Open Source},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3098279.3122150,
author = {Henze, Niels and Mayer, Sven and Le, Huy Viet and Schwind, Valentin},
title = {Improving Software-Reduced Touchscreen Latency},
year = {2017},
isbn = {9781450350754},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3098279.3122150},
doi = {10.1145/3098279.3122150},
abstract = {The latency of current mobile devices' touchscreens is around 100ms and has widely been explored. Latency down to 2ms is noticeable, and latency as low as 25ms reduces users' performance. Previous work reduced touch latency by extrapolating a finger's movement using an ensemble of shallow neural networks and showed that predicting 33ms into the future increases users' performance. Unfortunately, this prediction has a high error. Predicting beyond 33ms did not increase participants' performance, and the error affected the subjective assessment. We use more recent machine learning techniques to reduce the prediction error. We train LSTM networks and multilayer perceptrons using a large data set and regularization. We show that linear extrapolation causes an 116.7% higher error and the previously proposed ensembles of shallow networks cause a 26.7% higher error compared to the LSTM networks. The trained models, the data used for testing, and the source code is available on GitHub.},
booktitle = {Proceedings of the 19th International Conference on Human-Computer Interaction with Mobile Devices and Services},
articleno = {107},
numpages = {8},
keywords = {latency, touchscreen, LSTM, prediction, lag, touch input, machine learning, multilayer perceptron},
location = {Vienna, Austria},
series = {MobileHCI '17}
}

@inproceedings{10.5555/3306127.3331884,
author = {Bhattacharya, Parantapa and Ekanayake, Saliya and Kuhlman, Chris J. and Lebiere, Christian and Morrison, Don and Swarup, Samarth and Wilson, Mandy L. and Orr, Mark G.},
title = {The Matrix: An Agent-Based Modeling Framework for Data Intensive Simulations},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Human decision-making is influenced by social, psychological, neurological, emotional, normative, and learning factors, as well as individual traits like age and education level. Social/cognitive computational models that incorporate these factors are increasingly used to study how humans make decisions. A result is that agent models, within agent-based modeling (ABM), are becoming more heavyweight, i.e., are more computationally demanding, making scalability and at-scale simulations all the more difficult to achieve. To address these challenges, we have developed an ABM simulation framework that addresses data-intensive simulation at-scale. We describe system requirements and design, and demonstrate at-scale simulation by modeling 3 million users (each as an individual agent), 13 million repositories, and 239 million user-repository interactions on GitHub. Simulations predict user interactions with GitHub repositories, which, to our knowledge, are the first simulations of this kind. Our simulations demonstrate a three-order of magnitude increase in the number of cognitive agents simultaneously interacting.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1635–1643},
numpages = {9},
keywords = {distributed simulation, agent-based simulation, simulation framework},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/3030207.3030213,
author = {Leitner, Philipp and Bezemer, Cor-Paul},
title = {An Exploratory Study of the State of Practice of Performance Testing in Java-Based Open Source Projects},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030213},
doi = {10.1145/3030207.3030213},
abstract = {The usage of open source (OS) software is wide-spread across many industries. While the functional quality of OS projects is considered to be similar to closed-source software, much is unknown about the quality in terms of performance. One challenge for OS developers is that, unlike for functional testing, there is a lack of accepted best practices for performance testing. To reveal the state of practice of performance testing in OS projects, we conduct an exploratory study on 111 Java-based OS projects from GitHub. We study the performance tests of these projects from five perspectives: (1) developers, (2) size, (3) test organization, (4) types of performance tests and (5) used tooling. We show that writing performance tests is not a popular task in OS projects: performance tests form only a small portion of the test suite, are rarely updated, and are usually maintained by a small group of core project developers. Further, even though many projects are aware that they need performance tests, developers appear to struggle implementing them. We argue that future performance testing frameworks should provider better support for low-friction testing, for instance via non-parameterized methods or performance test generation, as well as focus on a tight integration with standard continuous integration tooling.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {373–384},
numpages = {12},
keywords = {open source, performance testing, mining software repositories, performance engineering, empirical software engineering},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1145/2970276.2970285,
author = {Greene, Gillian J. and Fischer, Bernd},
title = {CVExplorer: Identifying Candidate Developers by Mining and Exploring Their Open Source Contributions},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970285},
doi = {10.1145/2970276.2970285},
abstract = { Open source code contributions contain a large amount of technical skill information about developers, which can help to identify suitable candidates for a particular development job and therefore impact the success of a development team. We develop CVExplorer as a tool to extract, visualize, and explore relevant technical skills data from GitHub, such as languages and libraries used. It allows non-technical users to filter and identify developers according to technical skills demonstrated across all of their open source contributions, in order to support more accurate candidate identification. We demonstrate the usefulness of CVExplorer by using it to recommend candidates for open positions in two companies. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {804–809},
numpages = {6},
keywords = {Mining software repositories, Identifying candidate developers, Developer skills identification},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.5555/3408207.3408257,
author = {Vendome, Christopher and Rao, Dhananjai M. and Giabbanelli, Philippe J.},
title = {How Do Modelers Code Artificial Societies? Investigating Practices and Quality of Netlogo Codes from Large Repositories},
year = {2020},
isbn = {9781713812883},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {Many guidelines have been developed for simulations in general or for agent-based models which support artificial societies. When applying such guidelines to examine existing practices, assessment studies are limited by the artifacts released by modelers. Although code is the final product defining an artificial society, 90% of the code produced is not released hence previous assessments necessarily focused on higher-level items such as conceptual design or validation. We address this gap by collecting 338 artificial societies from two hosting platforms, CoMSES/OpenABM and GitHub. An innovation of our approach is the use of software engineering techniques to automatically examine the models with respect to items such as commenting the code, using libraries, or dividing code into functions. We found that developers of artificial societies code the decision-making of their agents from scratch in every model, despite the existence of several libraries that could be used as building blocks.},
booktitle = {Proceedings of the 2020 Spring Simulation Conference},
articleno = {42},
numpages = {12},
keywords = {NetLogo, model quality, software repository mining, GitHub, agent-based model},
location = {Fairfax, Virginia},
series = {SpringSim '20}
}

@inproceedings{10.1145/3292006.3300032,
author = {Matyukhina, Alina and Stakhanova, Natalia and Dalla Preda, Mila and Perley, Celine},
title = {Adversarial Authorship Attribution in Open-Source Projects},
year = {2019},
isbn = {9781450360999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292006.3300032},
doi = {10.1145/3292006.3300032},
abstract = {Open-source software is open to anyone by design, whether it is a community of developers, hackers or malicious users. Authors of open-source software typically hide their identity through nicknames and avatars. However, they have no protection against authorship attribution techniques that are able to create software author profiles just by analyzing software characteristics. In this paper we present an author imitation attack that allows to deceive current authorship attribution systems and mimic a coding style of a target developer. Withing this context we explore the potential of the existing attribution techniques to be deceived. Our results show that we are able to imitate the coding style of the developers based on the data collected from the popular source code repository, GitHub. To subvert author imitation attack, we propose a novel author obfuscation approach that allows us to hide the coding style of the author. Unlike existing obfuscation tools, this new obfuscation technique uses transformations that preserve code readability. We assess the effectiveness of our attacks on several datasets produced by actual developers from GitHub, and participants of the GoogleCodeJam competition. Throughout our experiments we show that the author hiding can be achieved by making sensible transformations which significantly reduce the likelihood of identifying the author's style to 0% by current authorship attribution systems.},
booktitle = {Proceedings of the Ninth ACM Conference on Data and Application Security and Privacy},
pages = {291–302},
numpages = {12},
keywords = {imitation, attacks, obfuscation, open-source software, authorship attribution, adversarial},
location = {Richardson, Texas, USA},
series = {CODASPY '19}
}

@inproceedings{10.1109/ICPC.2017.18,
author = {Beniamini, Gal and Gingichashvili, Sarah and Orbach, Alon Klein and Feitelson, Dror G.},
title = {Meaningful Identifier Names: The Case of Single-Letter Variables},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.18},
doi = {10.1109/ICPC.2017.18},
abstract = {It is widely accepted that variable names in computer programs should be meaningful, and that this aids program comprehension. "Meaningful" is commonly interpreted as favoring long descriptive names. However, there is at least some use of short and even single-letter names: using i in loops is very common, and we show (by extracting variable names from 1000 popular github projects in 5 languages) that some other letters are also widely used. In addition, controlled experiments with different versions of the same functions (specifically, different variable names) failed to show significant differences in ability to modify the code. Finally, an online survey showed that certain letters are strongly associated with certain types and meanings. This implies that a single letter can in fact convey meaning. The conclusion from all this is that single letter variables can indeed be used beneficially in certain cases, leading to more concise code.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {45–54},
numpages = {10},
keywords = {program comprehension, meaningful identifier names, single-letter names},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3194932.3194941,
author = {Werder, Karl and Brinkkemper, Sjaak},
title = {MEME: Toward a Method for Emotions Extraction from Github},
year = {2018},
isbn = {9781450357517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194932.3194941},
doi = {10.1145/3194932.3194941},
abstract = {Software engineering researchers are increasingly interested in the role of emotion during software development. While general tools are available to extract emotions from textual data, these perform poorly in the domain of software engineering. Hence, this paper develops MEME - a Method for EMotion Extraction. Using GHtorrent and GitHub as data sources, the paper presents an implementation of the method. The evaluation results suggest a better performance of MEME in contrast to Syuzhet R package emotion analysis.},
booktitle = {Proceedings of the 3rd International Workshop on Emotion Awareness in Software Engineering},
pages = {20–24},
numpages = {5},
location = {Gothenburg, Sweden},
series = {SEmotion '18}
}

@inproceedings{10.1145/3123266.3129391,
author = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},
title = {TensorLayer: A Versatile Library for Efficient Deep Learning Development},
year = {2017},
isbn = {9781450349062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3123266.3129391},
doi = {10.1145/3123266.3129391},
abstract = {Recently we have observed emerging uses of deep learning techniques in multimedia systems. Developing a practical deep learning system is arduous and complex. It involves labor-intensive tasks for constructing sophisticated neural networks, coordinating multiple network models, and managing a large amount of training-related data. To facilitate such a development process, we propose TensorLayer which is a Python-based versatile deep learning library. TensorLayer provides high-level modules that abstract sophisticated operations towards neuron layers, network models, training data and dependent training jobs. In spite of offering simplicity, it has transparent module interfaces that allows developers to flexibly embed low-level controls within a backend engine, with the aim of supporting fine-grain tuning towards training. Real-world cluster experiment results show that TensorLayeris able to achieve competitive performance and scalability in critical deep learning tasks. TensorLayer was released in September 2016 on GitHub. Since after, it soon become one of the most popular open-sourced deep learning library used by researchers and practitioners.},
booktitle = {Proceedings of the 25th ACM International Conference on Multimedia},
pages = {1201–1204},
numpages = {4},
keywords = {reinforcement learning, computer vision, deep learning, parallel computation, data management, natural language processing},
location = {Mountain View, California, USA},
series = {MM '17}
}

@inproceedings{10.1109/ASE.2019.00064,
author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward J. and Allamanis, Miltiadis and Goues, Claire Le and Neubig, Graham and Vasilescu, Bogdan},
title = {DIRE: A Neural Approach to Decompiled Identifier Naming},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00064},
doi = {10.1109/ASE.2019.00064},
abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub.1 Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {628–639},
numpages = {12},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2666539.2666570,
author = {Zhang, Lingxiao and Zou, Yanzhen and Xie, Bing and Zhu, Zixiao},
title = {Recommending Relevant Projects via User Behaviour: An Exploratory Study on Github},
year = {2014},
isbn = {9781450332248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666539.2666570},
doi = {10.1145/2666539.2666570},
abstract = { Social coding sites (e.g., Github) provide various features like Forking and Sending Pull-requests to support crowd-based software engineering. When using these features, a large amount of user behavior data is recorded. User behavior data can reflect developers preferences and interests in software development activities. Online service providers in many fields have been using user behavior data to discover user preferences and interests to achieve various purposes. In the field of software engineering however, there has been few studies in mining large amount of user behavior data. Our goal is to design an approach based on user behavior data, to recommend relevant open source projects to developers, which can be helpful in activities like searching for the right open source solutions to quickly build prototypes. In this paper, we explore the possibilities of such a method by conducting a set of experiments on selected data sets from Github. We find it a promising direction in mining projects' relevance from user behavior data. Our study also obtain some important issues that is worth considering in this method. },
booktitle = {Proceedings of the 1st International Workshop on Crowd-Based Software Development Methods and Technologies},
pages = {25–30},
numpages = {6},
keywords = {Recommendation system, Social coding, Github, Crowd-base software engineering},
location = {Hong Kong, China},
series = {CrowdSoft 2014}
}

@inproceedings{10.5555/3306127.3331935,
author = {Blythe, James and Ferrara, Emilio and Huang, Di and Lerman, Kristina and Muric, Goran and Sapienza, Anna and Tregubov, Alexey and Pacheco, Diogo and Bollenbacher, John and Flammini, Alessandro and Hui, Pik-Mai and Menczer, Filippo},
title = {The DARPA SocialSim Challenge: Massive Multi-Agent Simulations of the Github Ecosystem},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We model the evolution of GitHub, a large collaborative software-development ecosystem, using massive multi-agent simulations as a part of DARPA's SocialSim program. Our best performing models and our agent-based simulation framework are described here. Six different agent models were tested based on a variety of machine learning and statistical methods. The most successful models are based on sampling from a stationary probability distribution of actions and repositories for each agent.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {1835–1837},
numpages = {3},
keywords = {collaborative platforms, massive scale simulations, GitHub},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/2601248.2601269,
author = {Tomassetti, Federico and Torchiano, Marco},
title = {An Empirical Assessment of Polyglot-Ism in GitHub},
year = {2014},
isbn = {9781450324762},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2601248.2601269},
doi = {10.1145/2601248.2601269},
abstract = {In this paper we study how the language cocktails are composed. How many languages are used in each software projects, which language types are used and which languages are typically used together. Our study was done on a sample of over 15,000 projects from the largest software forge, GitHub. The results show that many languages are used in each project: 96% projects employ at least 2 languages, over 50% employ at least two programming languages. Finally, there are strong relations between different languages: hence sets of languages tend to be adopted together.},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {17},
numpages = {4},
keywords = {polyglot development},
location = {London, England, United Kingdom},
series = {EASE '14}
}

@inproceedings{10.1145/2901739.2901748,
author = {Bao, Lingfeng and Lo, David and Xia, Xin and Wang, Xinyu and Tian, Cong},
title = {How Android App Developers Manage Power Consumption? An Empirical Study by Mining Power Management Commits},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901748},
doi = {10.1145/2901739.2901748},
abstract = {As Android platform becomes more and more popular, a large amount of Android applications have been developed. When developers design and implement Android applications, power consumption management is an important factor to consider since it affects the usability of the applications. Thus, it is important to help developers adopt proper strategies to manage power consumption. Interestingly, today, there is a large number of Android application repositories made publicly available in sites such as GitHub. These repositories can be mined to help crystalize common power management activities that developers do. These in turn can be used to help other developers to perform similar tasks to improve their own Android applications.In this paper, we present an empirical study of power management commits in Android applications. Our study extends that of Moura et al. who perform an empirical study on energy aware commits; however they do not focus on Android applications and only a few of the commits that they study come from Android applications. Android applications are often different from other applications (e.g., those running on a server) due to the issue of limited battery life and the use of specialized APIs. As subjects of our empirical study, we obtain a list of open source Android applications from F-Droid and crawl their commits from Github. We get 468 power management commits after we filter the commits using a set of keywords and by performing manual analysis. These 468 power management commits are from 154 different Android applications and belong to 15 different application categories. Furthermore, we use open card sort to categorize these power management commits and we obtain 6 groups which correspond to different power management activities. Our study also reveals that for different kinds of Android application (e.g., Games, Connectivity, Navigation, Internet, Phone &amp; SMS, Time, etc.), the dominant power management activities differ. For example, the percentage of power management commits belonging to Power Adaptation activity is larger for Navigation applications than those belonging to other categories.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {37–48},
numpages = {12},
keywords = {power management, power consumption, mining software repository, empirical study},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1007/978-3-319-26844-6_22,
author = {Kikas, Riivo and Dumas, Marlon and Pfahl, Dietmar},
title = {Issue Dynamics in Github Projects},
year = {2015},
isbn = {9783319268439},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-26844-6_22},
doi = {10.1007/978-3-319-26844-6_22},
abstract = {Issue repositories are used to keep of track of bugs, development tasks and feature requests in software development projects. In the case of open source projects, everyone can submit a new issue in the tracker. This practice can lead to situations where more issues are created than what can be effectively handled by the project members, raising the question of how issues are treated as the capacity of the project members is exceeded. In this paper, we study the temporal dynamics of issues in a popular open source development platform, namely Github, based on a sample of 4000 projects. We specifically analyze how the rate of issue creation, the amount of pending issues, and their average lifetime evolve over the course of time. The results show that more issues are opened shortly after the creation of a project repository and that the amount of pending issues increases inexorably due to forgotten unclosed issues. Yet, the average issue lifetime for issues that do get closed is relatively stable over time. These observations suggest that Github projects have implicit mechanisms for handling issues perceived to be important to the project, while neglecting those that exceed the project's capacity.},
booktitle = {Proceedings of the 16th International Conference on Product-Focused Software Process Improvement - Volume 9459},
pages = {295–310},
numpages = {16},
location = {Bolzano, Italy},
series = {PROFES 2015}
}

@inproceedings{10.1145/3340482.3342742,
author = {Borg, Markus and Svensson, Oscar and Berg, Kristian and Hansson, Daniel},
title = {SZZ Unleashed: An Open Implementation of the SZZ Algorithm - Featuring Example Usage in a Study of Just-in-Time Bug Prediction for the Jenkins Project},
year = {2019},
isbn = {9781450368551},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340482.3342742},
doi = {10.1145/3340482.3342742},
abstract = {Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation},
pages = {7–12},
numpages = {6},
keywords = {SZZ, issue tracking, defect prediction, mining software repositories},
location = {Tallinn, Estonia},
series = {MaLTeSQuE 2019}
}

@inproceedings{10.1145/3340555.3353744,
author = {Aneja, Deepali and McDuff, Daniel and Shah, Shital},
title = {A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression Capabilities},
year = {2019},
isbn = {9781450368605},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340555.3353744},
doi = {10.1145/3340555.3353744},
abstract = {Embodied avatars as virtual agents have many applications and provide benefits over disembodied agents, allowing nonverbal social and interactional cues to be leveraged, in a similar manner to how humans interact with each other. We present an open embodied avatar built upon the Unreal Engine that can be controlled via a simple python programming interface. The avatar has lip syncing (phoneme control), head gesture and facial expression (using either facial action units or cardinal emotion categories) capabilities. We release code and models to illustrate how the avatar can be controlled like a puppet or used to create a simple conversational agent using public application programming interfaces (APIs). GITHUB link: https://github.com/danmcduff/AvatarSim },
booktitle = {2019 International Conference on Multimodal Interaction},
pages = {69–73},
numpages = {5},
keywords = {embodied agents, conversational systems, multimodality, avatars, expression retargeting.},
location = {Suzhou, China},
series = {ICMI '19}
}

@inproceedings{10.1145/3387940.3391489,
author = {Schumacher, Max Eric Henry and Le, Kim Tuyen and Andrzejak, Artur},
title = {Improving Code Recommendations by Combining Neural and Classical Machine Learning Approaches},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391489},
doi = {10.1145/3387940.3391489},
abstract = {Code recommendation systems for software engineering are designed to accelerate the development of large software projects. A classical example is code completion or next token prediction offered by modern integrated development environments. A particular challenging case for such systems are dynamic languages like Python due to limited type information at editing time. Recently, researchers proposed machine learning approaches to address this challenge. In particular, the Probabilistic Higher Order Grammar technique (Bielik et al., ICML 2016) uses a grammar-based approach with a classical machine learning schema to exploit local context. A method by Li et al., (IJCAI 2018) uses deep learning methods, in detail a Recurrent Neural Network coupled with a Pointer Network. We compare these two approaches quantitatively on a large corpus of Python files from GitHub. We also propose a combination of both approaches, where a neural network decides which schema to use for each prediction. The proposed method achieves a slightly better accuracy than either of the systems alone. This demonstrates the potential of ensemble-like methods for code completion and recommendation tasks in dynamically typed languages.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {476–482},
numpages = {7},
keywords = {code recommendations, neural networks, machine learning},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@inproceedings{10.1145/2538862.2539011,
author = {Wilson, Greg and Perez, Fernando and Norvig, Peter},
title = {Teaching Computing with the IPython Notebook (Abstract Only)},
year = {2014},
isbn = {9781450326056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2538862.2539011},
doi = {10.1145/2538862.2539011},
abstract = {The IPython Notebook is an interactive browser-based environment where you can combine code execution, text, mathematics, plots, and rich media into a single document. Originally designed for use as an electronic lab notebook for computational science, it is increasingly being used in teaching as well, and a rich ecosystem of open source plugins and extensions for teaching is growing around it. The first half of this hands-on workshop will introduce the Notebook and present examples of lessons and instructional materials built around it. In the second half, attendees will explore future directions for the Notebook as a teaching platform. For more information, please view our GitHub repository online at https://github.com/gvwilson/sigcse2014-ipython-workshop.},
booktitle = {Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
pages = {740},
numpages = {1},
keywords = {electronic lab notebook, python, pedagogy},
location = {Atlanta, Georgia, USA},
series = {SIGCSE '14}
}

@inproceedings{10.1145/3338906.3338918,
author = {Zhou, Shurui and Vasilescu, Bogdan and K\"{a}stner, Christian},
title = {What the Fork: A Study of Inefficient and Efficient Forking Practices in Social Coding},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338918},
doi = {10.1145/3338906.3338918},
abstract = {Forking and pull requests have been widely used in open-source communities as a uniform development and contribution mechanism, giving developers the flexibility to modify their own fork without affecting others before attempting to contribute back. However, not all projects use forks efficiently; many experience lost and duplicate contributions and fragmented communities. In this paper, we explore how open-source projects on GitHub differ with regard to forking inefficiencies. First, we observed that different communities experience these inefficiencies to widely different degrees and interviewed practitioners to understand why. Then, using multiple regression modeling, we analyzed which context factors correlate with fewer inefficiencies.We found that better modularity and centralized management are associated with more contributions and a higher fraction of accepted pull requests, suggesting specific best practices that project maintainers can adopt to reduce forking-related inefficiencies in their communities.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {350–361},
numpages = {12},
keywords = {Fork-based development, Collaboration efficiency, Centralized Management, Modularity},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3338906.3338952,
author = {Bavishi, Rohan and Yoshida, Hiroaki and Prasad, Mukul R.},
title = {Phoenix: Automated Data-Driven Synthesis of Repairs for Static Analysis Violations},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338952},
doi = {10.1145/3338906.3338952},
abstract = {Traditional automatic program repair (APR) tools rely on a test-suite as a repair specification. But test suites even when available are not of specification quality, limiting the performance and hence viability of test-suite based repair. On the other hand, static analysis-based bug finding tools are seeing increasing adoption in industry but still face challenges since the reported violations are viewed as not easily actionable. We propose a novel solution that solves both these challenges through a technique for automatically generating high-quality patches for static analysis violations by learning from examples. Our approach uses the static analyzer as an oracle and does not require a test suite. We realize our solution in a system, Phoenix, that implements a fully-automated pipeline that mines and cleans patches for static analysis violations from the wild, learns generalized executable repair strategies as programs in a novel Domain Specific Language (DSL), and then instantiates concrete repairs from them on new unseen violations. Using Phoenix we mine a corpus of 5,389 unique violations and patches from 517 Github projects. In a cross-validation study on this corpus Phoenix successfully produced 4,596 bug-fixes, with a recall of 85% and a precision of 54%. When applied to the latest revisions of a further5 Github projects, Phoenix produced 94 correct patches to previously unknown bugs, 19 of which have already been accepted and merged by the development teams. To the best of our knowledge this constitutes, by far the largest application of any automatic patch generation technology to large-scale real-world systems},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {613–624},
numpages = {12},
keywords = {program synthesis, program repair, static analysis, programming-by-example},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2884781.2884836,
author = {Near, Joseph P. and Jackson, Daniel},
title = {Finding Security Bugs in Web Applications Using a Catalog of Access Control Patterns},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884836},
doi = {10.1145/2884781.2884836},
abstract = {We propose a specification-free technique for finding missing security checks in web applications using a catalog of access control patterns in which each pattern models a common access control use case. Our implementation, Space, checks that every data exposure allowed by an application's code matches an allowed exposure from a security pattern in our catalog. The only user-provided input is a mapping from application types to the types of the catalog; the rest of the process is entirely automatic. In an evaluation on the 50 most watched Ruby on Rails applications on Github, Space reported 33 possible bugs---23 previously unknown security bugs, and 10 false positives.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {947–958},
numpages = {12},
keywords = {bug finding, web application security, access control},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3194124.3194129,
author = {Katsuragawa, Daiki and Ihara, Akinori and Kula, Raula Gaikovina and Matsumoto, Kenichi},
title = {Maintaining Third-Party Libraries through Domain-Specific Category Recommendations},
year = {2018},
isbn = {9781450357302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194124.3194129},
doi = {10.1145/3194124.3194129},
abstract = {Proper maintenance of third-party libraries contributes toward sustaining a healthy project, mitigating the risk it becoming outdated and obsolete. In this paper, we propose domain-specific categories (i.e., grouping of libraries that perform similar functionality) in library recommendations that aids in library maintenance. Our empirical study covers 2,511 GitHub projects and 150 domain-specific categories of Java libraries. Our results show that a system uses up to six different categories in their dependencies. Furthermore, recommending domain-specific categories is practical (i.e., with an accuracy between 66% to 81% for multiple categories) and its suggestion of libraries within that domain is comparable to existing techniques.},
booktitle = {Proceedings of the 1st International Workshop on Software Health},
pages = {2–9},
numpages = {8},
location = {Gothenburg, Sweden},
series = {SoHeal '18}
}

@inproceedings{10.1145/3301326.3301380,
author = {Pooput, Panthip and Muenchaisri, Pornsiri},
title = {Finding Impact Factors for Rejection of Pull Requests on GitHub},
year = {2018},
isbn = {9781450365536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301326.3301380},
doi = {10.1145/3301326.3301380},
abstract = {A pull request is an important method for code contributions in GitHub that will be submitted when the developers would like to merge their code changes from their local machine to the main repository on which all source code in the project are stored. Before merging the code changes into the main repository, the developers have to request for a permission. If their source code is allowed to merge, the pull request status is accepted. On the other hand, if their source code is not allowed to merge, the pull request status is rejected. The pull request status may be rejected due to several factors, such as code complexity, code quality, the number of changed files, etc. Fixing the rejected pull requests will take some extra effort and time which may affect the project cost and timeline. This paper aims at finding the impact factors that are associated with the rejection of pull requests on GitHub and also discovering the relationships among impact factors by using the association rules in data mining.},
booktitle = {Proceedings of the 2018 VII International Conference on Network, Communication and Computing},
pages = {70–76},
numpages = {7},
keywords = {Data mining, Association rules, Pull Request, Ansible, GitHub},
location = {Taipei City, Taiwan},
series = {ICNCC 2018}
}

@inproceedings{10.1109/ICSM.2015.7332478,
author = {Hora, Andre and Valente, Marco Tulio},
title = {Apiwave: Keeping Track of API Popularity and Migration},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332478},
doi = {10.1109/ICSM.2015.7332478},
abstract = {Every day new frameworks and libraries are created and existing ones evolve. To benefit from such newer or improved APIs, client developers should update their applications. In practice, this process presents some challenges: APIs are commonly backward-incompatible (causing client applications to fail when updating) and multiple APIs are available (making it difficult to decide which one to use). To address these challenges, we propose apiwave, a tool that keeps track of API popularity and migration of major frameworks/libraries. The current version includes data about the evolution of top 650 GitHub Java projects, from which 320K APIs were extracted. We also report an experience using apiwave on real-world scenarios.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {321–323},
numpages = {3},
series = {ICSME '15}
}

@inproceedings{10.1109/ICPC.2017.41,
author = {Malaquias, Romero and Ribeiro, M\'{a}rcio and Bonif\'{a}cio, Rodrigo and Monteiro, Eduardo and Medeiros, Fl\'{a}vio and Garcia, Alessandro and Gheyi, Rohit},
title = {The Discipline of Preprocessor-Based Annotations Does #ifdef TAG n't #endif Matter},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.41},
doi = {10.1109/ICPC.2017.41},
abstract = {The C preprocessor is a simple, effective, and language-independent tool. Developers use the preprocessor in practice to deal with portability and variability issues. Despite the widespread usage, the C preprocessor suffers from severe criticism, such as negative effects on code understandability and maintainability. In particular, these problems may get worse when using undisciplined annotations, i.e., when a preprocessor directive encompasses only parts of C syntactical units. Nevertheless, despite the criticism and guidelines found in systems like Linux to avoid undisciplined annotations, the results of a previous controlled experiment indicated that the discipline of annotations has no influence on program comprehension and maintenance. To better understand whether developers care about the discipline of preprocessor-based annotations and whether they can really influence on maintenance tasks, in this paper we conduct a mixed-method research involving two studies. In the first one, we identify undisciplined annotations in 110 open-source C/C++ systems of different domains, sizes, and popularity GitHub metrics. We then refactor the identified undisciplined annotations to make them disciplined. Right away, we submit pull requests with our code changes. Our results show that almost two thirds of our pull requests have been accepted and are now merged. In the second study, we conduct a controlled experiment. We have several differences with respect to the aforementioned one, such as blocking of cofounding effects and more replicas. We have evidences that maintaining undisciplined annotations is more time consuming and error prone, representing a different result when compared to the previous experiment. Overall, we conclude that undisciplined annotations should not be neglected.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {297–307},
numpages = {11},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/2568225.2568313,
author = {Subramanian, Siddharth and Inozemtseva, Laura and Holmes, Reid},
title = {Live API Documentation},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568313},
doi = {10.1145/2568225.2568313},
abstract = { Application Programming Interfaces (APIs) provide powerful abstraction mechanisms that enable complex functionality to be used by client programs. However, this abstraction does not come for free: understanding how to use an API can be difficult. While API documentation can help, it is often insufficient on its own. Online sites like Stack Overflow and Github Gists have grown to fill the gap between traditional API documentation and more example-based resources. Unfortunately, these two important classes of documentation are independent.  In this paper we describe an iterative, deductive method of linking source code examples to API documentation. We also present an implementation of this method, called Baker, that is highly precise (0.97) and supports both Java and JavaScript. Baker can be used to enhance traditional API documentation with up-to-date source code examples; it can also be used to incorporate links to the API documentation into the code snippets that use the API. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {643–652},
numpages = {10},
keywords = {source code search, documentation, Source code examples},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3197091.3205819,
author = {Glassey, Richard},
title = {Managing Assignment Feedback via Issue Tracking},
year = {2018},
isbn = {9781450357074},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3197091.3205819},
doi = {10.1145/3197091.3205819},
abstract = {This poster provides insight into the use of an issue tracker for the management of assignment feedback within an introductory course in computer science (CS). Students have made use of Github for three successive years, and the issue tracker has become one of the key mechanisms for managing formative feedback. This approach has yielded three key benefits: increased student engagement in their own feedback; provided an early experience of an authentic and industry desirable communication skill; and created a means to oversee and learn from feedback discussions for the teaching team.},
booktitle = {Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
pages = {382},
numpages = {1},
keywords = {Issue Tracker, Introductory Programming, Formative Feedback},
location = {Larnaca, Cyprus},
series = {ITiCSE 2018}
}

@inproceedings{10.1109/ICSME.2014.102,
author = {Reiss, Steven P.},
title = {Tool Demo: Browsing Software Repositories},
year = {2014},
isbn = {9781479961467},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSME.2014.102},
doi = {10.1109/ICSME.2014.102},
abstract = {We demonstrate a tool for browsing large software repositories such as GitHub or Source Forge using all the facilities one normally associates with an integrated development environment. The tool integrates code search engines with the Code Bubbles development environment. It lets the user perform and compare multiple searches, investigate and explore the results that are returned, expand searches as necessary, and eventually export appropriate results.},
booktitle = {Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution},
pages = {589–592},
numpages = {4},
keywords = {software repositories, Code search, integrated development environments},
series = {ICSME '14}
}

@inproceedings{10.1145/3278142.3278143,
author = {Baltes, Sebastian and Knack, Jascha and Anastasiou, Daniel and Tymann, Ralf and Diehl, Stephan},
title = {(No) Influence of Continuous Integration on the Commit Activity in GitHub Projects},
year = {2018},
isbn = {9781450360562},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278142.3278143},
doi = {10.1145/3278142.3278143},
abstract = {A core goal of Continuous Integration (CI) is to make small incremental changes to software projects, which are integrated frequently into a mainline repository or branch. This paper presents an empirical study that investigates if developers adjust their commit activity towards the above-mentioned goal after projects start using CI. We analyzed the commit and merge activity in 93 GitHub projects that introduced the hosted CI system Travis CI, but have previously been developed for at least one year before introducing CI. In our analysis, we only found one non-negligible effect, an increased merge ratio, meaning that there were more merging commits in relation to all commits after the projects started using Travis CI. This effect has also been reported in related work. However, we observed the same effect in a random sample of 60 GitHub projects not using CI. Thus, it is unlikely that the effect is caused by the introduction of CI alone. We conclude that: (1) in our sample of projects, the introduction of CI did not lead to major changes in developers' commit activity, and (2) it is important to compare the commit activity to a baseline before attributing an effect to a treatment that may not be the cause for the observed effect.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Software Analytics},
pages = {1–7},
numpages = {7},
keywords = {mining software repositories, open source software, continuous integration, commit activity},
location = {Lake Buena Vista, FL, USA},
series = {SWAN 2018}
}

@inproceedings{10.1109/MSR.2019.00022,
author = {Treude, Christoph and Wagner, Markus},
title = {Predicting Good Configurations for GitHub and Stack Overflow Topic Models},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00022},
doi = {10.1109/MSR.2019.00022},
abstract = {Software repositories contain large amounts of textual data, ranging from source code comments and issue descriptions to questions, answers, and comments on Stack Overflow. To make sense of this textual data, topic modelling is frequently used as a text-mining tool for the discovery of hidden semantic structures in text bodies. Latent Dirichlet allocation (LDA) is a commonly used topic model that aims to explain the structure of a corpus by grouping texts. LDA requires multiple parameters to work well, and there are only rough and sometimes conflicting guidelines available on how these parameters should be set. In this paper, we contribute (i) a broad study of parameters to arrive at good local optima for GitHub and Stack Overflow text corpora, (ii) an a-posteriori characterisation of text corpora related to eight programming languages, and (iii) an analysis of corpus feature importance via per-corpus LDA configuration. We find that (1) popular rules of thumb for topic modelling parameter configuration are not applicable to the corpora used in our experiments, (2) corpora sampled from GitHub and Stack Overflow have different characteristics and require different configurations to achieve good model fit, and (3) we can predict good configurations for unseen corpora reliably. These findings support researchers and practitioners in efficiently determining suitable configurations for topic modelling when analysing textual data contained in software repositories.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {84–95},
numpages = {12},
keywords = {corpus features, topic modelling, algorithm portfolio},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3077286.3077320,
author = {Darwish, Ali and Nakhmani, Arie},
title = {Internal Covariate Shift Reduction in Encoder-Decoder Convolutional Neural Networks},
year = {2017},
isbn = {9781450350242},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3077286.3077320},
doi = {10.1145/3077286.3077320},
abstract = {Internal covariant shift in deep neural networks affects the learning and convergence speed in ConvNets. Batch normalization was recently proposed to reduce the distribution of each layer's input to accelerate the training process. It also reduces overfitting and eliminates the need for using dropout in the fully connected layers, or RELU activation. Batch normalization, in its essence, seeks stable distribution of activation values throughout training, and normalizes the inputs of nonlinear data. In order to determine the usefulness of batch normalization in neural networks that don't use fully connected layers we evaluated the performance of an encoder-decoder ConvNet with and without using batch normalization. We found that batch normalization increased the learning performance by 18% but also increased the training time in each epoch (iteration) by 26%. The code for this work and the datasets are provided in a github repository.},
booktitle = {Proceedings of the SouthEast Conference},
pages = {179–182},
numpages = {4},
location = {Kennesaw, GA, USA},
series = {ACM SE '17}
}

@inproceedings{10.1145/3132847.3133065,
author = {Sathanur, Arun V. and Choudhury, Sutanay and Joslyn, Cliff and Purohit, Sumit},
title = {When Labels Fall Short: Property Graph Simulation via Blending of Network Structure and Vertex Attributes},
year = {2017},
isbn = {9781450349185},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3132847.3133065},
doi = {10.1145/3132847.3133065},
abstract = {Property graphs can be used to represent heterogeneous networks with labeled (attributed) vertices and edges. Given a property graph, simulating another graph with same or greater size with the same statistical properties with respect to the labels and connectivity is critical for privacy preservation and benchmarking purposes. In this work we tackle the problem of capturing the statistical dependence of the edge connectivity on the vertex labels and using the same distribution to regenerate property graphs of the same or expanded size in a scalable manner. However, accurate simulation becomes a challenge when the attributes do not completely explain the network structure. We propose the Property Graph Model (PGM) approach that uses a label augmentation strategy to mitigate the problem and preserve the vertex label and the edge connectivity distributions as well as their correlation, while also replicating the degree distribution. Our proposed algorithm is scalable with a linear complexity in the number of edges in the target graph. We illustrate the efficacy of the PGM approach in regenerating and expanding the datasets by leveraging two distinct illustrations. Our open-source implementation is available on GitHub.},
booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
pages = {2287–2290},
numpages = {4},
keywords = {label augmentation, attributed graphs, property graphs, joint distribution, label-topology correlation, graph generation},
location = {Singapore, Singapore},
series = {CIKM '17}
}

@inproceedings{10.1145/2645710.2645717,
author = {Loni, Babak and Said, Alan},
title = {WrapRec: An Easy Extension of Recommender System Libraries},
year = {2014},
isbn = {9781450326681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645710.2645717},
doi = {10.1145/2645710.2645717},
abstract = {WrapRec is an easy-to-use Recommender Systems toolkit, which allows users to easily implement or wrap recommendation algorithms from other frameworks. The main goals of WrapRec are to provide a flexible I/O, evaluation mechanism and code reusability. WrapRec provides a rich data model which makes it easy to implement algorithms for different recommender system problems, such as context-aware and cross-domain recommendation. The toolkit is written in C# and the source code is publicly available on Github under the GPL license.},
booktitle = {Proceedings of the 8th ACM Conference on Recommender Systems},
pages = {377–378},
numpages = {2},
keywords = {open source library, recommender systems},
location = {Foster City, Silicon Valley, California, USA},
series = {RecSys '14}
}

@inproceedings{10.1145/3308560.3316541,
author = {Mei, Qiaozhu},
title = {Decoding the New World Language: Analyzing the Popularity, Roles, and Utility of Emojis},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3316541},
doi = {10.1145/3308560.3316541},
abstract = {Emojis have quickly become a universal language that is used by worldwide users, for everyday tasks, across language barriers, and in different apps and platforms. The prevalence of emojis has quickly attracted great attentions from various research communities such as natural language processing, Web mining, ubiquitous computing, and human-computer interaction, as well as other disciplines including social science, arts, psychology, and linguistics.This talk summarizes the recent efforts made by my research group and our collaborators on analyzing large-scale emoji data. The usage of emojis by worldwide users presents interesting commonality as well as divergence. In our analysis of emoji usage by millions of smartphone users in 212 countries, we show that the different preferences and usage of emojis provide rich signals for understanding the cultural differences of Internet users, which correlate with the Hofstede’s cultural dimensions [4].Emojis play different roles when used alongside text. Through jointly learning the embeddings and topological structures of words and emojis, we reveal that emojis present both complementary and supplementary relations to words. Based on the structural properties of emojis in the semantic spaces, we are able to untangle several factors behind the popularity of emojis [1].This talk also highlights the utility of emojis. In general, emojis have been used by Internet users as text supplements to describe objects and situations, express sentiments, or express humor and sarcasm; they are also used as communication tools to attract attention, adjust tones, or establish personal relationships. The benefit of using emojis goes beyond these intentions. In particular, we show that including emojis in the description of an issue report on GitHub results in the issue being responded to by more users and resolved sooner.Large-scale emoji data can also be utilized by AI systems to improve the quality of Web mining services. In particular, a smart machine learning system can infer the latent topics, sentiments, and even demographic information of users based on how they use emojis online. Our analysis reveals a considerable difference between female and male users of emojis, which is big enough for a machine learning algorithm to accurately predict the gender of a user. In Web services that are customized for gender groups, gender inference models built upon emojis can complement those based on text or behavioral traces with fewer privacy concerns [2].Emojis can be also used as an instrument to bridge Web mining tasks across language barriers, especially to transfer sentiment knowledge from a language with rich training labels (e.g., English) to languages that have been difficult for advanced natural language processing tasks [3]. Through this bridge, developers of AI systems and Web services are able to reduce the inequality in the quality of services received by the international users that has been caused by the imbalance of available human annotations in different languages.In general, emojis have evolved from visual ideograms to a brand-new world language in the era of AI and a new Web. The popularity, roles, and utility of emojis have all gone beyond people’s original intentions, which have created a huge opportunity for future research that calls for joint efforts from multiple disciplines.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {417–418},
numpages = {2},
keywords = {emojis, natural language processing, text mining},
location = {San Francisco, USA},
series = {WWW '19}
}

@inproceedings{10.5555/3192424.3192679,
author = {Leibzon, William},
title = {Social Network of Software Development at GitHub},
year = {2016},
isbn = {9781509028467},
publisher = {IEEE Press},
abstract = {This paper looks at organization of software development teams and project communities at GitHub. Using social network analysis several open-source projects are analyzed and social networks of users with ties to a project are shown to have some scale-free properties. We further show how to find core development group and a network metric is introduced to measure collaboration among core members, corresponding to if a project is healthy and more likely to be successful.},
booktitle = {Proceedings of the 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
pages = {1374–1376},
numpages = {3},
keywords = {collaboration, github, open-source, social network analysis, scale-free network},
location = {Davis, California},
series = {ASONAM '16}
}

@inproceedings{10.1109/ASE.2015.51,
author = {Martie, Lee and LaToza, Thomas D. and van der Hoek, Andr\'{e}},
title = {CodeExchange: Supporting Reformulation of Internet-Scale Code Queries in Context},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.51},
doi = {10.1109/ASE.2015.51},
abstract = {Programming today regularly involves searching for source code online, whether through a general search engine such as Google or a specialized code search engine such as SearchCode, Ohloh, or GitHub. Searching typically is an iterative process, with developers adjusting the keywords they use based on the results of the previous query. However, searching in this manner is not ideal, because just using keywords places limits on what developers can express as well as the overall interaction that is required. Based on the observation that the results from one query create a context in which a next is formulated, we present CodeExchange, a new code search engine that we developed to explicitly leverage this context to support fluid, expressive reformulation of queries. We motivate the need for CodeExchange, highlight its key design decisions and overall architecture, and evaluate its use in both a field deployment and a laboratory study.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {24–35},
numpages = {12},
keywords = {query reformulation, context, interface, code search, internet-scale},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3183440.3183491,
author = {Li, Yi and Zhu, Chenguang and Rubin, Julia and Chechik, Marsha},
title = {CSlicerCloud: A Web-Based Semantic History Slicing Framework},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183491},
doi = {10.1145/3183440.3183491},
abstract = {Traditional commit-based sequential organization of software version histories is insufficient for many development tasks which require high-level, semantic understanding of program functionality, such as porting features or cutting new releases. Semantic history slicing is a technique which uses well-organized unit tests as identifiers for corresponding software functionalities and extracts a set of commits that correspond to a specific high-level functionality. In this paper, we present CSlicerCloud, a Web-based semantic history slicing service tailored for Java projects hosted on GitHub. It is accessible through Web browsers and powered in the backend by a collection of history slicing techniques underneath. We evaluated CSlicerCloud on a dataset containing developer-annotated change histories collected from 10 open source software projects. A video demonstration which showcases the main features of CSlicerCloud can be found at https://youtu.be/7kcswA0bQzo.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {57–60},
numpages = {4},
keywords = {software evolution, program semantics, version histories},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3379597.3387495,
author = {Spinellis, Diomidis and Kotti, Zoe and Kravvaritis, Konstantinos and Theodorou, Georgios and Louridas, Panos},
title = {A Dataset of Enterprise-Driven Open Source Software},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387495},
doi = {10.1145/3379597.3387495},
abstract = {We present a dataset of open source software developed mainly by enterprises rather than volunteers. This can be used to address known generalizability concerns, and, also, to perform research on open source business software development. Based on the premise that an enterprise's employees are likely to contribute to a project developed by their organization using the email account provided by it, we mine domain names associated with enterprises from open data sources as well as through white- and blacklisting, and use them through three heuristics to identify 17 264 enterprise GitHub projects. We provide these as a dataset detailing their provenance and properties. A manual evaluation of a dataset sample shows an identification accuracy of 89%. Through an exploratory data analysis we found that projects are staffed by a plurality of enterprise insiders, who appear to be pulling more than their weight, and that in a small percentage of relatively large projects development happens exclusively through enterprise insiders.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {533–537},
numpages = {5},
keywords = {software ecosystems, SEC 10-K, Fortune Global 500, Software engineering economics, SEC 20-F, open source software in business, dataset, EDGAR},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3334480.3382998,
author = {Liu, Dongyu and Smith, Micah J. and Veeramachaneni, Kalyan},
title = {Understanding User-Bot Interactions for Small-Scale Automation in Open-Source Development},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382998},
doi = {10.1145/3334480.3382998},
abstract = {Small-scale automation tools, or "bots," have been widely deployed in open-source software development to support manual project maintenance tasks. Though interactions between these bots and human developers can have significant effects on user experience, previous research has instead mostly focused on project outcomes. We reviewed existing small-scale bots in wide use on GitHub. After an in-depth qualitative and quantitative evaluation, we compiled several important design principles for human-bot interaction in this context. Following the requirements, we further propose a workflow to support bot developers.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–8},
numpages = {8},
keywords = {software and its engineering, software creation and management, human-centered computing, HCI design and evaluation methods},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}

@inproceedings{10.1145/3168836.3168839,
author = {Freire, Juliana},
title = {Spatio-Temporal Analytics, Urban Analytics},
year = {2017},
isbn = {9781450373159},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168836.3168839},
doi = {10.1145/3168836.3168839},
abstract = {Part 1Lecture 1:Urban Data: Challenges and OpportunitiesData Quality IssuesExploring Urban Data: Usability and InteractivityLecture 2:Finding Interesting FeaturesUsing Data to Discover and Explain DataTransparency and ReproducibilityHands-on 1:Data cleaningHands-on 2:Exploring shadows and trees in NYCCourse Material (lab and data) at https://github.com/julianafreire/ACMSummerSchool2017Course Notes at https://www.dropbox.com/s/4xsq0nubnbfmwaq/big-urban-data-lecture.pdf?dl=0The students will have to install OpenRefine and Jupyter+a few libraries.The instructions are in the lab descriptions in github.},
booktitle = {1st Europe Summer School: Data Science},
articleno = {3},
numpages = {1},
location = {Athens, Greece},
series = {SummerSchool '17}
}

@inproceedings{10.1145/3183440.3183471,
author = {Wang, Cong and Jiang, Yu and Zhao, Xibin and Song, Xiaoyu and Gu, Ming and Sun, Jiaguang},
title = {Weak-Assert: A Weakness-Oriented Assertion Recommendation Toolkit for Program Analysis},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3183471},
doi = {10.1145/3183440.3183471},
abstract = {Assertions are helpful in program analysis, such as software testing and verification. The most challenging part of automatically recommending assertions is to design the assertion patterns and to insert assertions in proper locations. In this paper, we develop Weak-Assert1, a weakness-oriented assertion recommendation toolkit for program analysis of C code. A weakness-oriented assertion is an assertion which can help to find potential program weaknesses. Weak-Assert uses well-designed patterns to match the abstract syntax trees of source code automatically. It collects significant messages from trees and inserts assertions into proper locations of programs. These assertions can be checked by using program analysis techniques. The experiments are set up on Juliet test suite and several actual projects in Github. Experimental results show that Weak-Assert helps to find 125 program weaknesses in 26 actual projects. These weaknesses are confirmed manually to be triggered by some test cases.The address of the abstract demo video is: https://youtu.be/_RWC4GJvRWc},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {69–72},
numpages = {4},
keywords = {assertion recommendation, formal program verification, program testing, program weakness},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3194932.3194938,
author = {Imtiaz, Nasif and Middleton, Justin and Girouard, Peter and Murphy-Hill, Emerson},
title = {Sentiment and Politeness Analysis Tools on Developer Discussions Are Unreliable, but so Are People},
year = {2018},
isbn = {9781450357517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194932.3194938},
doi = {10.1145/3194932.3194938},
abstract = {Many software engineering researchers use sentiment and politeness analysis tools to study the emotional environment within collaborative software development. However, papers that use these tools rarely establish their reliability. In this paper, we evaluate popular existing tools for sentiment and politeness detection over a dataset of 589 manually rated GitHub comments that represent developer discussions. We also develop a coding scheme on how to quantify politeness for conversational texts found on collaborative platforms. We find that not only do the tools have a low agreement with human ratings on sentiment and politeness, human raters also have a low agreement among themselves.},
booktitle = {Proceedings of the 3rd International Workshop on Emotion Awareness in Software Engineering},
pages = {55–61},
numpages = {7},
keywords = {affect analysis, developer discussion, politeness, github, sentiment},
location = {Gothenburg, Sweden},
series = {SEmotion '18}
}

@inproceedings{10.1145/3030207.3030226,
author = {Stefan, Petr and Horky, Vojtech and Bulej, Lubomir and Tuma, Petr},
title = {Unit Testing Performance in Java Projects: Are We There Yet?},
year = {2017},
isbn = {9781450344043},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3030207.3030226},
doi = {10.1145/3030207.3030226},
abstract = {Although methods and tools for unit testing of performance exist for over a decade, anecdotal evidence suggests unit testing of performance is not nearly as common as unit testing of functionality. We examine this situation in a study of GitHub projects written in Java, looking for occurrences of performance evaluation code in common performance testing frameworks. We quantify the use of such frameworks, identifying the most relevant performance testing approaches, and describe how we adjust the design of our SPL performance testing framework to follow these conclusions.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
pages = {401–412},
numpages = {12},
keywords = {open source, performance unit testing, jmh, spl, survey},
location = {L'Aquila, Italy},
series = {ICPE '17}
}

@inproceedings{10.1007/978-3-319-49004-5_20,
author = {Halilaj, Lavdim and Petersen, Niklas and Grangel-Gonz\'{a}lez, Irl\'{a}n and Lange, Christoph and Auer, S\"{o}ren and Coskun, G\"{o}khan and Lohmann, Steffen},
title = {VoCol: An Integrated Environment to Support Version-Controlled Vocabulary Development},
year = {2016},
isbn = {9783319490038},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-49004-5_20},
doi = {10.1007/978-3-319-49004-5_20},
abstract = {Vocabularies are increasingly being developed on platforms for hosting version-controlled repositories, such as GitHub. However, these platforms lack important features that have proven useful in vocabulary development. We present VoCol, an integrated environment that supports the development of vocabularies using Version Control Systems. VoCol is based on a fundamental model of vocabulary development, consisting of the three core activities modeling, population, and testing. We implemented VoCol using a loose coupling of validation, querying, analytics, visualization, and documentation generation components on top of a standard Git repository. All components, including the version-controlled repository, can be configured and replaced with little effort to cater for various use cases. We demonstrate the applicability of VoCol with a real-world example and report on a user study that confirms its usability and usefulness.},
booktitle = {20th International Conference on Knowledge Engineering and Knowledge Management - Volume 10024},
pages = {303–319},
numpages = {17},
keywords = {Git, Integrated development environment, GitHub, Version control system, Vocabulary development, Ontology engineering, Webhook, IDE},
location = {Bologna, Italy},
series = {EKAW 2016}
}

@inproceedings{10.1145/3345629.3345631,
author = {Amit, Idan and Feitelson, Dror G.},
title = {Which Refactoring Reduces Bug Rate?},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345631},
doi = {10.1145/3345629.3345631},
abstract = {We present a methodology to identify refactoring operations that reduce the bug rate in the code. The methodology is based on comparing the bug fixing rate in certain time windows before and after the refactoring. We analyzed 61,331 refactor commits from 1,531 large active GitHub projects. When comparing three-month windows, the bug rate is substantially reduced in 17% of the files of analyzed refactors, compared to 12% of the files in random commits. Within this group, implementing 'todo's provides the most benefits. Certain operations like reuse, upgrade, and using enum and namespaces are also especially beneficial.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {12–15},
numpages = {4},
keywords = {Code quality, refactoring, machine learning},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1145/2642803.2642811,
author = {Aarnoutse, Floor and Renes, Cassandra and Snijders, Remco and Jansen, Slinger},
title = {The Reality of an Associate Model: Comparing Partner Activity in the Eclipse Ecosystem},
year = {2014},
isbn = {9781450327787},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642803.2642811},
doi = {10.1145/2642803.2642811},
abstract = {Two determinants of software ecosystem health are productivity of and value creation by the actors in the ecosystem. While keystone players use partnership models to orchestrate actors, the relationship between the type of partnership and activity has not been studied. To address this gap, we have researched the partnership model of the Eclipse Ecosystem and the activity of different types of partners. We have used Eclipse Dash and GitHub to gather data about the activity of Eclipse partners. The results show that a higher level of membership is related to more activity. However, it is also observed that non-member companies are more active than associate members, which suggests that Eclipse can and should improve their partnership model by motivating associate members and incorporating active non-member companies. In addition, other software ecosystems could use these results and implications to improve their own partnership models.},
booktitle = {Proceedings of the 2014 European Conference on Software Architecture Workshops},
articleno = {8},
numpages = {6},
keywords = {Partner activity, Membership, Software Ecosystem, Eclipse, Associate model},
location = {Vienna, Austria},
series = {ECSAW '14}
}

@inproceedings{10.1145/3196398.3196430,
author = {Baltes, Sebastian and Dumani, Lorik and Treude, Christoph and Diehl, Stephan},
title = {SOTorrent: Reconstructing and Analyzing the Evolution of Stack Overflow Posts},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196430},
doi = {10.1145/3196398.3196430},
abstract = {Stack Overflow (SO) is the most popular question-and-answer website for software developers, providing a large amount of code snippets and free-form text on a wide variety of topics. Like other software artifacts, questions and answers on SO evolve over time, for example when bugs in code snippets are fixed, code is updated to work with a more recent library version, or text surrounding a code snippet is edited for clarity. To be able to analyze how content on SO evolves, we built SOTorrent, an open dataset based on the official SO data dump. SOTorrent provides access to the version history of SO content at the level of whole posts and individual text or code blocks. It connects SO posts to other platforms by aggregating URLs from text blocks and by collecting references from GitHub files to SO posts. In this paper, we describe how we built SOTorrent, and in particular how we evaluated 134 different string similarity metrics regarding their applicability for reconstructing the version history of text and code blocks. Based on a first analysis using the dataset, we present insights into the evolution of SO posts, e.g., that post edits are usually small, happen soon after the initial creation of the post, and that code is rarely changed without also updating the surrounding text. Further, our analysis revealed a close relationship between post edits and comments. Our vision is that researchers will use SOTorrent to investigate and understand the evolution of SO posts and their relation to other platforms such as GitHub.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {319–330},
numpages = {12},
keywords = {stack overflow, software evolution, open dataset, code snippets},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2464464.2464498,
author = {Heymann, S\'{e}bastien and Le Grand, B\'{e}n\'{e}dicte},
title = {Towards a Redefinition of Time in Information Networks?},
year = {2013},
isbn = {9781450318891},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464464.2464498},
doi = {10.1145/2464464.2464498},
abstract = {How should we characterize the dynamics of the Web? Whereas network maps have contributed to a redefinition of distances and space in information networks, current studies still use a traditional time unit -the second- to understand the temporality of the Web. This unit leads to the observation of exogenous phenomena like day-night patterns. In order to capture the intrinsic dynamics of the network, we introduce an innovative -yet simple- concept of time which relies on the measure of changes in the network space. We demonstrate its practical interest on the evolution of the Github social network.},
booktitle = {Proceedings of the 5th Annual ACM Web Science Conference},
pages = {158–161},
numpages = {4},
keywords = {time, measurement, dynamics, complex networks, social network, sliding window},
location = {Paris, France},
series = {WebSci '13}
}

@inproceedings{10.1145/3275219.3275222,
author = {Zheng, Zhiwen and Wang, Liang and Xu, Jingwei and Wu, Tianheng and Wu, Simeng and Tao, Xianping},
title = {Measuring and Predicting the Relevance Ratings between FLOSS Projects Using Topic Features},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275222},
doi = {10.1145/3275219.3275222},
abstract = {Understanding the relevance between the Free/Libra Open Source Software projects is important for developers to perform code and design reuse, discover and develop new features, keep their projects up-to-date, and etc. However, it is challenging to perform relevance ratings between the FLOSS projects mainly because: 1) beyond simple code similarity, there are complex aspects considered when measuring the relevance; and 2) the prohibitive large amount of FLOSS projects available. To address the problem, in this paper, we propose a method to measure and further predict the relevance ratings between FLOSS projects. Our method uses topic features extracted by the LDA topic model to describe the characteristics of a project. By using the topic features, multiple aspects of FLOSS projects such as the application domain, technology used, and programming language are extracted and further used to measure and predict their relevance ratings. Based on the topic features, our method uses matrix factorization to leverage the partially known relevance ratings between the projects to learn the mapping between different topic features to the relevance ratings. Finally, our method combines the topic modeling and matrix factorization technologies to predict the relevance ratings between software projects without human intervention, which is scalable to a large amount of projects. We evaluate the performance of the proposed method by applying our topic extraction and relevance modeling methods using 300 projects from GitHub. The result of topic extraction experiment shows that, for topic modeling, our LDA-based approach achieves the highest hit rate of 98.3% and the highest average accuracy of 29.8%. And the relevance modeling experiment shows that our relevance modeling approach achieves the minimum average predict error of 0.093, suggesting the effectiveness of applying the proposed method on real-world data sets.},
booktitle = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
articleno = {12},
numpages = {10},
keywords = {Topic Modeling, FLOSS Projects, Relevance Rating, Matrix Factorization},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/2910674.2935858,
author = {Giannakopoulos, Theodoros and Siantikos, Georgios},
title = {A ROS Framework for Audio-Based Activity Recognition},
year = {2016},
isbn = {9781450343374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2910674.2935858},
doi = {10.1145/2910674.2935858},
abstract = {Research on robot perception mostly focuses on visual information analytics. Audio-based perception is mostly based on speech-related information. However, non-verbal information of the audio channel can be equally important in the perception procedure, or at least play a complementary role. This paper presents a framework for audio signal analysis that utilizes the ROS architectural principles. Details on the design and implementation issues of this workflow are described, while classification results are also presented in the context of two use-cases motivated by the task of medical monitoring. The proposed audio analysis framework is provided as an open-source library at github (https://github.com/tyiannak/AUROS).},
booktitle = {Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
articleno = {41},
numpages = {4},
keywords = {open-source, feature extraction, classification, audio segmentation, audio analysis, ROS},
location = {Corfu, Island, Greece},
series = {PETRA '16}
}

@inproceedings{10.1145/3107411.3107455,
author = {Trieu, Tuan and Cheng, Jianlin},
title = {3D Genome Structure Modeling by Lorentzian Objective Function},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107455},
doi = {10.1145/3107411.3107455},
abstract = {Reconstructing 3D structure of a genome from chromosomal conformation capturing data such as Hi-C data has emerged as an important problem in bioinformatics and computational biology in the recent years. In this talk, I will present our latest method that uses Lorentzian function to describe distance restraints between chromosomal regions, which will be used to guide the reconstruction of 3D structures of individual chromosomes and an entire genome. The method is more robust against noisy distance restraints derived from Hi-C data than traditional objective functions such as squared error function and Gaussian probabilistic function. The method can handle both intra- and inter-chromosomal contacts effectively to build 3D structures of a big genome such as the human genome consisting of a number of chromosomes, which are not possible with most existing methods. We have released the Java source code that implements the method (called LorDG) at GitHub (https://github.com/BDM-Lab/LorDG), which is being used by the community to model 3D genome structures. We are currently further improving the method to build very high-resolution (e.g. 1KB base pair) 3D genome and chromosome models.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {641},
numpages = {1},
keywords = {chromosomal conformation capturing, 3d genome, lorentzian function, hi-c, modeling, optimization},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1109/ICSM.2015.7332512,
author = {Goeminne, Mathieu and Mens, Tom},
title = {Towards a Survival Analysis of Database Framework Usage in Java Projects},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332512},
doi = {10.1109/ICSM.2015.7332512},
abstract = {Many software projects rely on a relational database in order to realize part of their functionality. Various database frameworks and object-relational mappings have been developed and used to facilitate data manipulation. Little is known about whether and how such frameworks co-occur, how they complement or compete with each other, and how this changes over time. We empirically studied these aspects for 5 Java database frameworks, based on a corpus of 3,707 GitHub Java projects. In particular, we analysed whether certain database frameworks co-occur frequently, and whether some database frameworks get replaced over time by others. Using the statistical technique of survival analysis, we explored the survival of the database frameworks in the considered projects. This provides useful evidence to software developers about which frameworks can be used successfully in combination and which combinations should be avoided.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {551–555},
numpages = {5},
series = {ICSME '15}
}

@inproceedings{10.1109/MSR.2019.00050,
author = {Ahmad, Mashal and Cinn\'{e}ide, Mel \'{O}},
title = {Impact of Stack Overflow Code Snippets on Software Cohesion: A Preliminary Study},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00050},
doi = {10.1109/MSR.2019.00050},
abstract = {Developers frequently copy code snippets from publicly-available resources such as Stack Overflow (SO). While this may lead to a 'quick fix' for a development problem, little is known about how these copied code snippets affect the code quality of the recipient application, or how the quality of the recipient classes subsequently evolves over the time of the project. This has an impact on whether such code copying should be encouraged, and how classes that receive such code snippets should be monitored during evolution. To investigate this issue, we used instances from the SOTorrent database where Java snippets had been copied from Stack Overflow into GitHub projects. In each case, we measured the quality of the recipient class just prior to the addition of the snippet, immediately after the addition of the snippet, and at a later stage in the project. Our goal was to determine if the addition of the snippet caused quality to improve or deteriorate, and what the long-term implications were for the quality of the recipient class. Code quality was measured using the cohesion metrics Low-level Similarity-based Class Cohesion (LSCC) and Class Cohesion (CC). Over a random sample of 378 classes that received code snippets copied from Stack Overflow to GitHub, we found that in almost 70% of the cases where the copied snippet affected cohesion, the effect was to reduce the cohesion of the recipient class. Furthermore, this deterioration in cohesion tends to persist in the subsequent evolution of recipient class. In over 70% of cases the recipient class never fully regained the cohesion it lost in receiving the snippet. These results suggest that when copying code snippets from external repositories, more attention should be paid to integrating the code with the recipient class.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {250–254},
numpages = {5},
keywords = {cohesion, quality, metrics, evolution},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3328778.3367027,
author = {Malan, David J. and Sharp, Chad and van Assema, Jelle and Yu, Brian and Zidane, Kareem},
title = {CS50's GitHub-Based Tools for Teaching and Learning},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3367027},
doi = {10.1145/3328778.3367027},
abstract = {For CS50 at Harvard, we have developed a suite of free, open-source tools to help students with writing, testing, and submitting programming assignments; and to help teachers grade those assignments and check them for plagiarism. help50, a program that parses error messages and provides beginner-friendly advice to interpreting them, helps students understand and resolve often-cryptic compiler errors. check50 runs a set of automated tests on students' code, providing feedback and hints about where students have made errors. style50 lints students' code, highlighting places where it doesn't meet the course's style guide. submit50 allows students to submit assignments to a GitHub repository, without students needing to have knowledge of git or version control themselves. And compare50, an open-source and customizable alternative to Moss, allows teachers to analyze submissions for similarity, looking for pairs or clusters of submissions that might be the result of improper collaboration. The grading and submission tools require only a GitHub account to use, and can serve as free, extensible alternatives to tools like Codio, Gradescope, and Vocareum. In this workshop, we'll introduce each of the tools, and discuss how to use them for your own classroom. To date, each tool has been deployed to hundreds of students on campus and thousands online. Along the way, we'll discuss how to use the tools effectively, compare and contrast them with other options, identify how the tools have changed students' behavior for the better and for worse, and highlight pedagogical and technological changes we've made to redress the latter. Laptop (with Wi-Fi) required. Linux, macOS, or Windows. Latest version of Chrome.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1393},
numpages = {1},
keywords = {correctness testing, linting, style, open-source, error messages, homework submission},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/2884781.2884790,
author = {Nadi, Sarah and Kr\"{u}ger, Stefan and Mezini, Mira and Bodden, Eric},
title = {Jumping through Hoops: Why Do Java Developers Struggle with Cryptography APIs?},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884790},
doi = {10.1145/2884781.2884790},
abstract = {To protect sensitive data processed by current applications, developers, whether security experts or not, have to rely on cryptography. While cryptography algorithms have become increasingly advanced, many data breaches occur because developers do not correctly use the corresponding APIs. To guide future research into practical solutions to this problem, we perform an empirical investigation into the obstacles developers face while using the Java cryptography APIs, the tasks they use the APIs for, and the kind of (tool) support they desire. We triangulate data from four separate studies that include the analysis of 100 StackOverflow posts, 100 GitHub repositories, and survey input from 48 developers. We find that while developers find it difficult to use certain cryptographic algorithms correctly, they feel surprisingly confident in selecting the right cryptography concepts (e.g., encryption vs. signatures). We also find that the APIs are generally perceived to be too low-level and that developers prefer more task-based solutions.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {935–946},
numpages = {12},
keywords = {API misuse, cryptography, empirical software engineering},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3297280.3297501,
author = {Cesarini, Mirko and Mercorio, Fabio and Mezzanzanica, Mario and Moscato, Vincenzo and Picariello, Antonio},
title = {A Tool for Exploring Networks of Computer Scientists as a Graph},
year = {2019},
isbn = {9781450359337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297280.3297501},
doi = {10.1145/3297280.3297501},
abstract = {In this paper we present GraphDBLP, a tool to query the DBLP bibliography as a graph. The DBLP source data were enriched with semantic similarity relationships computed using word-embeddings. A user can interact with the system either writing queries on the graph-db visual console or using a shell-interface provided with 4 parametric and pre-defined queries.GraphDBLP would represent a first graph-database instance of the computer scientist network, that can be improved through new relationships and properties on nodes at any time, and this is the main purpose of the tool, we have made freely available on Github.},
booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
pages = {2240–2242},
numpages = {3},
keywords = {graph database, semantic knowledge discovery, application, semantic social network},
location = {Limassol, Cyprus},
series = {SAC '19}
}

@inproceedings{10.1145/3133956.3134048,
author = {Duan, Ruian and Bijlani, Ashish and Xu, Meng and Kim, Taesoo and Lee, Wenke},
title = {Identifying Open-Source License Violation and 1-Day Security Risk at Large Scale},
year = {2017},
isbn = {9781450349468},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133956.3134048},
doi = {10.1145/3133956.3134048},
abstract = {With millions of apps available to users, the mobile app market is rapidly becoming very crowded. Given the intense competition, the time to market is a critical factor for the success and profitability of an app. In order to shorten the development cycle, developers often focus their efforts on the unique features and workflows of their apps and rely on third-party Open Source Software (OSS) for the common features. Unfortunately, despite their benefits, careless use of OSS can introduce significant legal and security risks, which if ignored can not only jeopardize security and privacy of end users, but can also cause app developers high financial loss. However, tracking OSS components, their versions, and interdependencies can be very tedious and error-prone, particularly if an OSS is imported with little to no knowledge of its provenance.We therefore propose OSSPolice, a scalable and fully-automated tool for mobile app developers to quickly analyze their apps and identify free software license violations as well as usage of known vulnerable versions of OSS. OSSPolice introduces a novel hierarchical indexing scheme to achieve both high scalability and accuracy, and is capable of efficiently comparing similarities of app binaries against a database of hundreds of thousands of OSS sources (billions of lines of code). We populated OSSPolice with 60K C/C++ and 77K Java OSS sources and analyzed 1.6M free Google Play Store apps. Our results show that 1) over 40K apps potentially violate GPL/AGPL licensing terms, and 2) over 100K of apps use known vulnerable versions of OSS. Further analysis shows that developers violate GPL/AGPL licensing terms due to lack of alternatives, and use vulnerable versions of OSS despite efforts from companies like Google to improve app security. OSSPolice is available on GitHub.},
booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2169–2185},
numpages = {17},
keywords = {code clone detection, application security, license violation},
location = {Dallas, Texas, USA},
series = {CCS '17}
}

@inproceedings{10.1145/2786805.2786834,
author = {Nagappan, Meiyappan and Robbes, Romain and Kamei, Yasutaka and Tanter, \'{E}ric and McIntosh, Shane and Mockus, Audris and Hassan, Ahmed E.},
title = {An Empirical Study of Goto in C Code from GitHub Repositories},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786834},
doi = {10.1145/2786805.2786834},
abstract = { It is nearly 50 years since Dijkstra argued that goto obscures the flow of control in program execution and urged programmers to abandon the goto statement. While past research has shown that goto is still in use, little is known about whether goto is used in the unrestricted manner that Dijkstra feared, and if it is ‘harmful’ enough to be a part of a post-release bug. We, therefore, conduct a two part empirical study - (1) qualitatively analyze a statistically rep- resentative sample of 384 files from a population of almost 250K C programming language files collected from over 11K GitHub repositories and find that developers use goto in C files for error handling (80.21±5%) and cleaning up resources at the end of a procedure (40.36 ± 5%); and (2) quantitatively analyze the commit history from the release branches of six OSS projects and find that no goto statement was re- moved/modified in the post-release phase of four of the six projects. We conclude that developers limit themselves to using goto appropriately in most cases, and not in an unrestricted manner like Dijkstra feared, thus suggesting that goto does not appear to be harmful in practice. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {404–414},
numpages = {11},
keywords = {Dijkstra, Empirical SE, Github, Use of goto statements},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3238147.3240732,
author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
title = {An Empirical Investigation into Learning Bug-Fixing Patches in the Wild via Neural Machine Translation},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240732},
doi = {10.1145/3238147.3240732},
abstract = {Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. We mine millions of bug-fixes from the change histories of GitHub repositories to extract meaningful examples of such bug-fixes. Then, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. Our model is able to fix hundreds of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9% of the cases.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {832–837},
numpages = {6},
keywords = {bug-fixes, neural machine translation},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3382494.3422171,
author = {Scoccia, Gian Luca and Autili, Marco},
title = {Web Frameworks for Desktop Apps: An Exploratory Study},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422171},
doi = {10.1145/3382494.3422171},
abstract = {Background: Novel frameworks for the development of desktop applications with web technologies have become popular. These desktop web app frameworks allow developers to reuse existing code and knowledge of web applications for the creation of cross-platform apps integrated with native APIs. Aims: To date, desktop web app frameworks have not been studied empirically. In this paper, we aim to fill this gap by characterizing the usage of web frameworks and providing evidence on how beneficial are their pros and how impactful are their cons. Method: We conducted an empirical study, collecting and analyzing 453 desktop web apps publicly available on GitHub. We performed qualitative and quantitative analyses to uncover the traits and issues of desktop web apps. Results: We found that desktop web app frameworks enable the development of cross-platform applications even for teams of limited dimensions, taking advantage of the abundant number of available web libraries. However, at the same time, bugs deriving from platform compatibility issues are common. Conclusions: Our study provides concrete evidence on some disadvantages associated with desktop web app frameworks. Future work is required to assess their impact on the required development and maintenance effort, and to investigate other aspects not considered in this first research.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {35},
numpages = {6},
keywords = {Web technologies, cross-platform, desktop apps},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3196398.3196462,
author = {Joonbakhsh, Alireza and Sami, Ashkan},
title = {Mining and Extraction of Personal Software Process Measures through IDE Interaction Logs},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196462},
doi = {10.1145/3196398.3196462},
abstract = {The Personal Software Process (PSP) is an effective software process improvement method that heavily relies on manual collection of software development data. This paper describes a semi-automated method that reduces the burden of PSP data collection by extracting the required time and size of PSP measurements from IDE interaction logs. The tool mines enriched event data streams so can be easily generalized to other developing environment also. In addition, the proposed method is adaptable to phase definition changes and creates activity visualizations and summarizations that are helpful for software project management. Tools and processed data used for this paper are available on GitHub at: https://github.com/unknowngithubuser1/data.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {78–81},
numpages = {4},
keywords = {IDE, personal software process},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.5555/3320516.3320674,
author = {Yao, Shuochao and Hao, Yifan and Liu, Dongxin and Liu, Shengzhong and Shao, Huajie and Wu, Jiahao and Bamba, Mouna and Abdelzaher, Tarek and Flamino, James and Szymanski, Boleslaw},
title = {A Predictive Self-Configuring Simulator for Online Media},
year = {2018},
isbn = {978153866570},
publisher = {IEEE Press},
abstract = {This paper describes the design, implementation, and early experiences with a novel agent-based simulator of online media streams, developed under DARPA's SocialSim Program to extract and predict trends in information dissemination on online media. A hallmark of the simulator is its self-configuring property. Instead of requiring initial set-up, the input to the simulator constitutes data traces collected from the medium to be simulated. The simulator automatically learns from the data such elements as the number of agents involved, the number of objects involved, and the rate of introduction of new agents and objects. It also develops behavior models of simulated agents and objects, and their dependencies. These models are then used to run simulations allowing for future extrapolations and "what if" analyses. Results are presented on using this system to simulate GitHub transactions. They show good performance in terms of both simulation accuracy and overhead.},
booktitle = {Proceedings of the 2018 Winter Simulation Conference},
pages = {1262–1273},
numpages = {12},
location = {Gothenburg, Sweden},
series = {WSC '18}
}

@inproceedings{10.1145/3369255.3369300,
author = {Raibulet, Claudia and Fontana, Francesca Arcelli and Pigazzini, Ilaria},
title = {Teaching Software Engineering Tools to Undergraduate Students},
year = {2019},
isbn = {9781450372541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3369255.3369300},
doi = {10.1145/3369255.3369300},
abstract = {Today, software development is characterized by keywords such as collaborative, teamwork, distributed, agile, dynamic, qualitative and tool-supported among many others. In this paper, we present our experience in teaching three software development tools often used in industry in a software engineering course for undergraduate students: GitHub, SonarQube, and Microsoft Project. The main reasons behind the use of these tools during the development of a software project were: (1) students become familiar with examples of tools adopted in industry and academia, (2) students are enabled to collaborate in teams for the achievement of a common goal, and (3) students become aware of the management tasks needed by a project developed in teams. We exploited these tools in the software engineering course in the last three academic years. The students feedback on using these tools gathered through a questionnaire was positive. Students were enthusiastic in learning about new tools and their support for software development and management. In this paper we summarize the students feedback during three academic years and the lessons we have learned from their feedback.},
booktitle = {Proceedings of the 2019 11th International Conference on Education Technology and Computers},
pages = {262–267},
numpages = {6},
keywords = {Microsoft Project, Software engineering, SonarQube, GitHub, education, tool},
location = {Amsterdam, Netherlands},
series = {ICETC 2019}
}

@inproceedings{10.1007/978-3-319-43659-3_9,
author = {S\^{\i}rbu, Alina and Babaoglu, Ozalp},
title = {Power Consumption Modeling and Prediction in a Hybrid CPU-GPU-MIC Supercomputer},
year = {2016},
isbn = {9783319436586},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-43659-3_9},
doi = {10.1007/978-3-319-43659-3_9},
abstract = {Power consumption is a major obstacle for High Performance Computing HPC systems in their quest towards the holy grail of ExaFLOP performance. Significant advances in power efficiency have to be made before this goal can be attained and accurate modeling is an essential step towards power efficiency by optimizing system operating parameters to match dynamic energy needs. In this paper we present a study of power consumption by jobs in Eurora, a hybrid CPU-GPU-MIC system installed at the largest Italian data center. Using data from a dedicated monitoring framework, we build a data-driven model of power consumption for each user in the system and use it to predict the power requirements of future jobs. We are able to achieve good prediction results for over 80\"{\i} undefined% of the users in the system. For the remaining users, we identify possible reasons why prediction performance is not as good. Possible applications for our predictive modeling results include scheduling optimization, power-aware billing and system-scale power modeling. All the scripts used for the study have been made available on GitHub.},
booktitle = {Proceedings of the 22nd International Conference on Euro-Par 2016: Parallel Processing - Volume 9833},
pages = {117–130},
numpages = {14},
keywords = {Hybrid system, Job power modeling, High performance computing, Job power prediction, Support vector regression}
}

@inproceedings{10.1109/ICSM.2013.50,
author = {Venkataramani, Rahul and Asadullah, Allahbaksh and Bhat, Vasudev and Muddu, Basavaraju},
title = {Latent Co-Development Analysis Based Semantic Search for Large Code Repositories},
year = {2013},
isbn = {9780769549811},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2013.50},
doi = {10.1109/ICSM.2013.50},
abstract = {Distributed and collaborative software development has increased the popularity of source code repositories like GitHub. With the number of projects in such code repositories exceeding millions, it is important to identify the domains to which the projects belong. A domain is a concept or a hierarchy of concepts used to categorize a project. We have proposed a model to cluster projects in a code repository by mining the latent co-development network. These identified clusters are mapped to domains with the help of a taxonomy which we constructed using the metadata from an online Question and Answer (Q&amp;A) website. To demonstrate the validity of the model, we built a prototype for semantic search on source code repositories. In this paper, we outline the proposed model and present the early results.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Software Maintenance},
pages = {372–375},
numpages = {4},
keywords = {source code repositories, semantic search, Software repository analysis and mining, Human aspects of software evolution},
series = {ICSM '13}
}

@inproceedings{10.1109/ASE.2019.00158,
author = {Neupane, Krishna Prasad},
title = {An Approach for Investigating Emotion Dynamics in Software Development},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00158},
doi = {10.1109/ASE.2019.00158},
abstract = {Emotion awareness is critical to interpersonal communication, including that in software development. The SE community has studied emotion in software development using isolated emotion states but it has not considered the dynamic nature of emotion. To investigate the emotion dynamics, SE community needs an effective approach. In this paper, we propose such an approach which can automatically collect project teams' communication records, identify the emotions and their intensities in them, model the emotion dynamics into time series, and provide efficient data management. We demonstrate that this approach can provide end-to-end support for various emotion awareness research and practices through automated data collection, modeling, storage, analysis, and presentation using the IPython's project data on GitHub.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1268–1270},
numpages = {3},
keywords = {software development, emotion awareness, time-series database, emotion dynamics, emotion intensity},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1109/ICSE-NIER.2019.00022,
author = {Beller, Moritz and Hejderup, Joseph},
title = {Blockchain-Based Software Engineering},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2019.00022},
doi = {10.1109/ICSE-NIER.2019.00022},
abstract = {Blockchain technology has found a great number of applications, from banking to the Internet of Things (IoT). However, it has not yet been envisioned whether and which problems in Software Engineering (SE) Blockchain technology could solve. In this paper, we coin this field "Blockchain-based Software Engineering" and exemplify how Blockchain technology could solve two core SE problems: Continuous Integration (CI) Services such as Travis CI and Package Managers such as apt-get. We believe that Blockchain technology could help (1) democratize and professionalize Software Engineering infrastructure that currently relies on free work done by few volunteers, (2) improve the quality of artifacts and services, and (3) increase trust in ubiquitously used systems like GitHub or Travis CI.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {53–56},
numpages = {4},
keywords = {distributed system, blockchain},
location = {Montreal, Quebec, Canada},
series = {ICSE-NIER '19}
}

@inproceedings{10.1145/2804381.2804387,
author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo},
title = {The Challenges of Sentiment Detection in the Social Programmer Ecosystem},
year = {2015},
isbn = {9781450338189},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2804381.2804387},
doi = {10.1145/2804381.2804387},
abstract = { A recent research trend has emerged to study the role of affect in in the social programmer ecosystem, by applying sentiment analysis to the content available in sites such as GitHub and Stack Overflow. In this paper, we aim at assessing the suitability of a state-of-the-art sentiment analysis tool, already applied in social computing, for detecting affective expressions in Stack Overflow. We also aim at verifying the construct validity of choosing sentiment polarity and strength as an appropriate way to operationalize affective states in empirical studies on Stack Overflow. Finally, we underline the need to overcome the limitations induced by domain-dependent use of lexicon that may produce unreliable results. },
booktitle = {Proceedings of the 7th International Workshop on Social Software Engineering},
pages = {33–40},
numpages = {8},
keywords = {Sentiment Analysis, Technical Forum, Stack Overflow, Online Q&amp;A, Social Software Engineering, Social Programmer},
location = {Bergamo, Italy},
series = {SSE 2015}
}

@inproceedings{10.5555/2859032.2867575,
author = {Nakazawa, Shun and Tanaka, Tetsuo},
title = {Prototype of Kanban Tool and Preliminary Evaluation of Visualizing Method for Task Assignment},
year = {2015},
isbn = {9781467382113},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Kanban is a method used in agile software development. It is a most important tool as it acts as a central communication hub among the members of an agile development team. In this research, the authors develop a prototype of a Kanban tool. The tool displays each developer's tasks across multiple horizontal rows. Therefore, users can assess the task assignment and workloads of team members in one glance. The board also links up with GitHub and has a feature of real time synchronization among clients for distributed development. An experiment showed that the proposed approach was effective.},
booktitle = {Proceedings of the 2015 International Conference on Computer Application Technologies},
pages = {7},
numpages = {1},
keywords = {Kanban, GitHub coordination, Task board, Real-time synchronization, Agile software development},
series = {CCATS '15}
}

@inproceedings{10.1109/CCATS.2015.21,
author = {Nakazawa, Shun and Tanaka, Tetsuo},
title = {Prototype of Kanban Tool and Preliminary Evaluation of Visualizing Method for Task Assignment},
year = {2015},
isbn = {9781467382113},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CCATS.2015.21},
doi = {10.1109/CCATS.2015.21},
abstract = {Kanban is a method used in agile software development. It is a most important tool as it acts as a central communication hub among the members of an agile development team. In this research, the authors develop a prototype of a Kanban tool. The tool displays each developer's tasks across multiple horizontal rows. Therefore, users can assess the task assignment and workloads of team members in one glance. The board also links up with GitHub and has a feature of real time synchronization among clients for distributed development. An experiment showed that the proposed approach was effective.},
booktitle = {Proceedings of the 2015 International Conference on Computer Application Technologies},
pages = {48–49},
numpages = {2},
keywords = {Agile software development, GitHub coordination, Task board, Kanban, Real-time synchronization},
series = {CCATS '15}
}

@inproceedings{10.1145/3127005.3127008,
author = {Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco},
title = {Scripted GUI Testing of Android Apps: A Study on Diffusion, Evolution and Fragility},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127008},
doi = {10.1145/3127005.3127008},
abstract = {Background. Evidence suggests that mobile applications are not thoroughly tested as their desktop counterparts. In particular GUI testing is generally limited. Like web-based applications, mobile apps suffer from GUI test fragility, i.e. GUI test classes failing due to minor modifications in the GUI, without the application functionalities being altered.Aims. The objective of our study is to examine the diffusion of GUI testing on Android, and the amount of changes required to keep test classes up to date, and in particular the changes due to GUI test fragility. We define metrics to characterize the modifications and evolution of test classes and test methods, and proxies to estimate fragility-induced changes.Method. To perform our experiments, we selected six widely used open-source tools for scripted GUI testing of mobile applications previously described in the literature. We have mined the repositories on GitHub that used those tools, and computed our set of metrics.Results. We found that none of the considered GUI testing frameworks achieved a major diffusion among the open-source Android projects available on GitHub. For projects with GUI tests, we found that test suites have to be modified often, specifically 5%--10% of developers' modified LOCs belong to tests, and that a relevant portion (60% on average) of such modifications are induced by fragility.Conclusions. Fragility of GUI test classes constitute a relevant concern, possibly being an obstacle for developers to adopt automated scripted GUI tests. This first evaluation and measure of fragility of Android scripted GUI testing can constitute a benchmark for developers, and the basis for the definition of a taxonomy of fragility causes, and actionable guidelines to mitigate the issue.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {22–32},
numpages = {11},
keywords = {Automated Software Testing, Mobile Development, Software Evolution, Software Maintenance, GUI Testing},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.5555/2457524.2457690,
author = {Loyola, Pablo and Ko, In-Young},
title = {Biological Mutualistic Models Applied to Study Open Source Software Development},
year = {2012},
isbn = {9780769548807},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {The evolution of the Web has allowed the generation of several platforms for collaborative work. One of the main contributors to these advances is the Open Source initiative, in which projects are boosted to a new level of interaction and cooperation that improves their software quality and reliability. In order to understand how the group of contributors interacts with the software under development, we propose a novel methodology that adapts Lotka-Volterra-based biological models used for host-parasite interaction. In that sense, we used the concept mutualism from social parasites. Preliminary results based on experiments on the Github collaborative platform showed that Open Source phenomena can be modeled as a mutualistic system, in terms of the evolution of the population of developers and repositories.},
booktitle = {Proceedings of the The 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology - Volume 01},
pages = {248–253},
numpages = {6},
keywords = {open source software development, population models, web-based collaborative work},
series = {WI-IAT '12}
}

@inproceedings{10.1145/3377812.3382151,
author = {Martinez, Matias and Etien, Anne and Ducasse, St\'{e}phane and Fuhrman, Christopher},
title = {RTj: A Java Framework for Detecting and Refactoring Rotten Green Test Cases},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382151},
doi = {10.1145/3377812.3382151},
abstract = {Rotten green tests are passing tests which have at least one assertion that is not executed. They give developers a false sense of trust in the code. In this paper, we present RTj, a framework that analyzes test cases from Java projects with the goal of detecting and refactoring rotten test cases. RTj automatically discovered 418 rotten tests from 26 open-source Java projects hosted on GitHub. Using RTj, developers have an automated recommendation of the tests that need to be modified for improving the quality of the applications under test. A video is available at: https://youtu.be/Uqxf-Wzp3Mg},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {69–72},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/MSR.2017.46,
author = {Madeyski, Lech and Kawalerowicz, Marcin},
title = {Continuous Defect Prediction: The Idea and a Related Dataset},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.46},
doi = {10.1109/MSR.2017.46},
abstract = {We would like to present the idea of our Continuous Defect Prediction (CDP) research and a related dataset that we created and share. Our dataset is currently a set of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data we mine from software repositories. Our dataset embraces 1265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset we use TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories. We extend this data to a file change level and calculate the software process metrics that may be used, for example, as features to predict risky software changes that could break the build if committed to a repository with CI enabled.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {515–518},
numpages = {4},
keywords = {continuous defect prediction, open science, defect prediction, software repository, mining software repositories},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3293882.3330578,
author = {Lou, Yiling and Chen, Junjie and Zhang, Lingming and Hao, Dan and Zhang, Lu},
title = {History-Driven Build Failure Fixing: How Far Are We?},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330578},
doi = {10.1145/3293882.3330578},
abstract = {Build systems are essential for modern software development and maintenance since they are widely used to transform source code artifacts into executable software. Previous work shows that build systems break frequently during software evolution. Therefore, automated build-fixing techniques are in huge demand. In this paper we target a mainstream build system, Gradle, which has become the most widely used build system for Java projects in the open-source community (e.g., GitHub). HireBuild, state-of-the-art build-fixing tool for Gradle, has been recently proposed to fix Gradle build failures via mining the history of prior fixes. Although HireBuild has been shown to be effective for fixing real-world Gradle build failures, it was evaluated on only a limited set of build failures, and largely depends on the quality/availability of historical fix information. To investigate the efficacy and limitations of the history-driven build fix, we first construct a new and large build failure dataset from Top-1000 GitHub projects. Then, we evaluate HireBuild on the extended dataset both quantitatively and qualitatively. Inspired by the findings of the study, we propose a simplistic new technique that generates potential patches via searching from the present project under test and external resources rather than the historical fix information. According to our experimental results, the simplistic approach based on present information successfully fixes 2X more reproducible build failures than the state-of-art HireBuild based on historical fix information. Furthermore, our results also reveal various findings/guidelines for future advanced build failure fixing.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {43–54},
numpages = {12},
keywords = {Build System, Build Failure Fixing, {Automated Program Repair},
location = {Beijing, China},
series = {ISSTA 2019}
}

@inproceedings{10.1145/3377813.3381347,
author = {Chong, Nathan and Cook, Byron and Kallas, Konstantinos and Khazem, Kareem and Monteiro, Felipe R. and Schwartz-Narbonne, Daniel and Tasiran, Serdar and Tautschnig, Michael and Tuttle, Mark R.},
title = {Code-Level Model Checking in the Software Development Workflow},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381347},
doi = {10.1145/3377813.3381347},
abstract = {This experience report describes a style of applying symbolic model checking developed over the course of four years at Amazon Web Services (AWS). Lessons learned are drawn from proving properties of numerous C-based systems, e.g., custom hypervisors, encryption code, boot loaders, and an IoT operating system. Using our methodology, we find that we can prove the correctness of industrial low-level C-based systems with reasonable effort and predictability. Furthermore, AWS developers are increasingly writing their own formal specifications. All proofs discussed in this paper are publicly available on GitHub.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {11–20},
numpages = {10},
keywords = {model checking, memory safety, continuous integration},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/3388142.3388146,
author = {Brady, James F.},
title = {Computing System Congestion Management Using Exponential Smoothing Forecasting},
year = {2020},
isbn = {9781450376440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388142.3388146},
doi = {10.1145/3388142.3388146},
abstract = {An overloaded computer must finish what it starts and not start what will fail or hang. A congestion management algorithm, the author developed, effectively manages traffic overload with its unique formulation of Exponential Smoothing forecasting. This set of equations resolve forecasting startup issues that have limited the model's adoption as a discrete time series predictor. These expressions also satisfy implementation requirements to perform calculations using integer math and be able to reset the forecast seamlessly. A computer program, written in C language, which exercises the methodology, is downloadable from GitHub.},
booktitle = {Proceedings of the 2020 the 4th International Conference on Compute and Data Analysis},
pages = {164–169},
numpages = {6},
keywords = {overload control, congestion management, exponential smoothing},
location = {Silicon Valley, CA, USA},
series = {ICCDA 2020}
}

@inproceedings{10.1145/3416921.3416939,
author = {Peksa, Janis},
title = {Autonomous Data-Driven Integration Algorithm},
year = {2020},
isbn = {9781450375382},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416921.3416939},
doi = {10.1145/3416921.3416939},
abstract = {In this paper, an autonomous data-driven integration algorithm is the main focus of the Autonomous Open Data Prediction Framework [1]. This paper proposes the autonomous Web Service integration into ERP systems. The paper highlights the Autonomous Open Data Prediction Framework algorithm using flowchart and UML diagrams, also pseudocode of the Kalman filter method, as well as, selection of the appropriate programming language. Described the Autonomous Open Data Prediction Framework API that all programming code is published to the GitHub repository. It is highlighting the lack of forecasting methods offered in ERP systems that have not been able to ensure all the necessary business process opportunities to increase business value.},
booktitle = {Proceedings of the 2020 4th International Conference on Cloud and Big Data Computing},
pages = {63–67},
numpages = {5},
keywords = {autonomous algorithm, Data-driven integration, integration framework, ERP systems integration},
location = {Virtual, United Kingdom},
series = {ICCBDC '20}
}

@inproceedings{10.1109/ICSM.2015.7332461,
author = {Stanciulescu, Stefan and Schulze, Sandro and Wasowski, Andrzej},
title = {Forked and Integrated Variants in an Open-Source Firmware Project},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332461},
doi = {10.1109/ICSM.2015.7332461},
abstract = {Code cloning has been reported both on small (code fragments) and large (entire projects) scale. Cloning-in-the-large, or forking, is gaining ground as a reuse mechanism thanks to availability of better tools for maintaining forked project variants, hereunder distributed version control systems and interactive source management platforms such as Github. We study advantages and disadvantages of forking using the case of Marlin, an open source firmware for 3D printers. We find that many problems and advantages of cloning do translate to forking. Interestingly, the Marlin community uses both forking and integrated variability management (conditional compilation) to create variants and features. Thus, studying it increases our understanding of the choice between integrated and clone-based variant management. It also allows us to observe mechanisms governing source code maturation, in particular when, why and how feature implementations are migrated from forks to the main integrated platform. We believe that this understanding will ultimately help development of tools mixing clone-based and integrated variant management, combining the advantages of both.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {151–160},
numpages = {10},
series = {ICSME '15}
}

@inproceedings{10.1145/3368089.3417937,
author = {Molavi, Abtin and Downing, Mara and Schneider, Tommy and Bang, Lucas},
title = {MCBAT: A Practical Tool for Model Counting Constraints on Bounded Integer Arrays},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417937},
doi = {10.1145/3368089.3417937},
abstract = {Model counting procedures for data structures are crucial for advancing the field of automated quantitative program analysis. We present a tool for Model Counting for Bounded Array Theory (MCBAT). MCBAT works on quantified integer array constraints in which all arrays have a finite length. We employ reductions from the theory of arrays to uninterpreted functions and linear integer arithmetic (LIA). Once reduced to LIA, we leverage Barvinok's polynomial time integer lattice point enumeration algorithm. Finally, we present a case study demonstrating applicability to automated quantitative program analysis. MCBAT is available for immediate use as a Docker image and the source code is freely available in our Github repository.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1596–1600},
numpages = {5},
keywords = {Software and its engineering~Formal methods, Mathematics of computing~Combinatoric problems},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1109/CHASE.2019.00021,
author = {Pinheiro, Andr\'{e} M. and Rabello, Caio S. and Furtado, Leonardo B. and Pinto, Gustavo and de Souza, Cleidson R. B.},
title = {Expecting the Unexpected: Distilling Bot Development, Challenges, and Motivations},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CHASE.2019.00021},
doi = {10.1109/CHASE.2019.00021},
abstract = {Software bots are becoming an increasingly popular tool in the software development landscape, which is particularly due to their potential of use in several different contexts. More importantly, software developers interested in transitioning to bot development may have to face challenges intrinsic related to bot software development. However, so far, it is still unclear what is the profile of bot developers, what motivate them, or what challenges do they face when dealing with bot development. To shed an initial light on this direction, we conducted a survey with 43 Github users who have been involved (showing their interest or actively contributing to) in bot software projects.},
booktitle = {Proceedings of the 12th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {51–52},
numpages = {2},
location = {Montreal, Quebec, Canada},
series = {CHASE '19}
}

@inproceedings{10.1145/2950290.2950364,
author = {Zhu, Jiaxin and Zhou, Minghui and Mockus, Audris},
title = {Effectiveness of Code Contribution: From Patch-Based to Pull-Request-Based Tools},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950364},
doi = {10.1145/2950290.2950364},
abstract = { Code contributions in Free/Libre and Open Source Software projects are controlled to maintain high-quality of software. Alternatives to patch-based code contribution tools such as mailing lists and issue trackers have been developed with the pull request systems being the most visible and widely available on GitHub. Is the code contribution process more effective with pull request systems? To answer that, we quantify the effectiveness via the rates contributions are accepted and ignored, via the time until the first response and final resolution and via the numbers of contributions. To control for the latent variables, our study includes a project that migrated from an issue tracker to the GitHub pull request system and a comparison between projects using mailing lists and pull request systems. Our results show pull request systems to be associated with reduced review times and larger numbers of contributions. However, not all the comparisons indicate substantially better accept or ignore rates in pull request systems. These variations may be most simply explained by the differences in contribution practices the projects employ and may be less affected by the type of tool. Our results clarify the importance of understanding the role of tools in effective management of the broad network of potential contributors and may lead to strategies and practices making the code contribution more satisfying and efficient from both contributors' and maintainers' perspectives. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {871–882},
numpages = {12},
keywords = {mailing list, effectiveness, pull request, issue tracker, FLOSS, Code contribution},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1109/Agile.2015.12,
author = {Rahman, Akond Ashfaque Ur and Helms, Eric and Williams, Laurie and Parnin, Chris},
title = {Synthesizing Continuous Deployment Practices Used in Software Development},
year = {2015},
isbn = {9781467371537},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/Agile.2015.12},
doi = {10.1109/Agile.2015.12},
abstract = {Continuous deployment speeds up the process of existing agile methods, such as Scrum, and Extreme Programming (XP) through the automatic deployment of software changes to end-users upon passing of automated tests. Continuous deployment has become an emerging software engineering process amongst numerous software companies, such as Facebook, Github, Netflix, and Rally Software. A systematic analysis of software practices used in continuous deployment can facilitate a better understanding of continuous deployment as a software engineering process. Such analysis can also help software practitioners in having a shared vocabulary of practices and in choosing the software practices that they can use to implement continuous deployment. The goal of this paper is to aid software practitioners in implementing continuous deployment through a systematic analysis of software practices that are used by software companies. We studied the continuous deployment practices of 19 software companies by performing a qualitative analysis of Internet artifacts and by conducting follow-up inquiries. In total, we found 11 software practices that are used by 19 software companies. We also found that in terms of use, eight of the 11 software practices are common across 14 software companies. We observe that continuous deployment necessitates the consistent use of sound software engineering practices such as automated testing, automated deployment, and code review.},
booktitle = {Proceedings of the 2015 Agile Conference},
pages = {1–10},
numpages = {10},
keywords = {agile, internet artifacts, industry practices, follow-up inquiries, continuous deployment, continuous delivery},
series = {AGILE '15}
}

@inproceedings{10.1145/3194932.3194934,
author = {Werder, Karl},
title = {The Evolution of Emotional Displays in Open Source Software Development Teams: An Individual Growth Curve Analysis},
year = {2018},
isbn = {9781450357517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194932.3194934},
doi = {10.1145/3194932.3194934},
abstract = {Software developers communicate and interact with each other in order to solve complex problems. Such communication often includes emotional displays that have been shown to influence team processes and performance. Yet, little is known about the evolution of team emotional displays. Hence, we investigate a sample of 1121 Open Source Software (OSS) projects from GitHub, using longitudinal data analysis. The results from growth curve analysis shows that the team emotional display decrease over time. This negative linear trend decelerates mid-term as suggested by a positive quadratic trend of time. Such deceleration diminishes toward the end as a negative cubic trend suggests.},
booktitle = {Proceedings of the 3rd International Workshop on Emotion Awareness in Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {growth curve analysis, software development, open source software, team emotional display},
location = {Gothenburg, Sweden},
series = {SEmotion '18}
}

@inproceedings{10.1109/ICSE-Companion.2019.00049,
author = {Wang, Kaiyuan and Sullivan, Allison and Khurshid, Sarfraz},
title = {ARepair: A Repair Framework for Alloy},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion.2019.00049},
doi = {10.1109/ICSE-Companion.2019.00049},
abstract = {Researchers have proposed many automated program repair techniques for imperative languages, e.g. Java. However, little work has been done to repair programs written in declarative languages, e.g. Alloy. We proposed ARepair, the first automated program repair technique for faulty Alloy models. ARepair takes as input a faulty Alloy model and a set of tests that capture the desired model properties, and produces a fixed model that passes all tests. ARepair uses tests written for the recently introduced AUnit framework, which provides a notion of unit testing for Alloy models. In this paper, we describes our Java implementation of ARepair, which is a command-line tool, released as an open-source project on GitHub. Our experimental results show that ARepair is able to fix 28 out of 38 real-world faulty models we collected. The demo video for ARepair can be found at https://youtu.be/436drvWvbEU.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Companion Proceedings},
pages = {103–106},
numpages = {4},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3126858.3131598,
author = {Lazzari, Hugo Schroter and Tavares da Costa Filho, Roberto Iraja and Roesler, Valter},
title = {QoE Analyser: A Framework to QoE Knowledge Base Generation},
year = {2017},
isbn = {9781450350969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3126858.3131598},
doi = {10.1145/3126858.3131598},
abstract = {This paper presents a framework for the creation of a knowledge database on QoE. The QoE Analyzer framework enables the simulation of degradations in video playout and also the application of a survey to evaluate the impact of degradations on the user Quality of Experience. In order to show the versatility of the framework, an instantiation of the framework and its application to a group of 62 users was carried out. The framework was implemented using the JavaScript language and, through it, it was possible to show the impacts of degradation patterns on the user experience. The framework was released under GNU GPLv3 license and is available in github (https://github.com/hugoschroterl/qoe-analyser).},
booktitle = {Proceedings of the 23rd Brazillian Symposium on Multimedia and the Web},
pages = {433–436},
numpages = {4},
keywords = {video subjective analysis., qoe framework, video quality, quality of experience, video streaming},
location = {Gramado, RS, Brazil},
series = {WebMedia '17}
}

@inproceedings{10.1145/3338906.3338955,
author = {Islam, Md Johirul and Nguyen, Giang and Pan, Rangeet and Rajan, Hridesh},
title = {A Comprehensive Study on Deep Learning Bug Characteristics},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338955},
doi = {10.1145/3338906.3338955},
abstract = {Deep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {510–520},
numpages = {11},
keywords = {Bugs, Deep learning software, Deep learning bugs, Empirical Study of Bugs, Q&amp;A forums},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1007/978-3-030-31901-4_2,
author = {Kao, Po-Yu and Zhang, Angela and Goebel, Michael and Chen, Jefferson W. and Manjunath, B. S.},
title = {Predicting Fluid Intelligence of Children Using T1-Weighted MR Images and a StackNet},
isbn = {978-3-030-31900-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-030-31901-4_2},
doi = {10.1007/978-3-030-31901-4_2},
abstract = {In this work, we utilize T1-weighted MR images and StackNet to predict fluid intelligence in adolescents. Our framework includes feature extraction, feature normalization, feature denoising, feature selection, training a StackNet, and predicting fluid intelligence. The extracted feature is the distribution of different brain tissues in different brain parcellation regions. The proposed StackNet consists of three layers and 11 models. Each layer uses the predictions from all previous layers including the input layer. The proposed StackNet is tested on a public benchmark Adolescent Brain Cognitive Development Neurocognitive Prediction Challenge 2019 and achieves a mean squared error of 82.42 on the combined training and validation set with 10-fold cross-validation. The proposed StackNet achieves a mean squared error of 94.25 on the testing data. The source code is available on GitHub ().},
booktitle = {Adolescent Brain Cognitive Development Neurocognitive Prediction},
pages = {9–16},
numpages = {8},
keywords = {Machine learning, Fluid intelligence (Gf), T1-weighted MRI, StackNet}
}

@inproceedings{10.1145/3194932.3194935,
author = {Ding, Jin and Sun, Hailong and Wang, Xu and Liu, Xudong},
title = {Entity-Level Sentiment Analysis of Issue Comments},
year = {2018},
isbn = {9781450357517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194932.3194935},
doi = {10.1145/3194932.3194935},
abstract = {Emotions and sentiment of software developers can largely influence the software productivity and quality. However, existing work on emotion mining and sentiment analysis is still in the early stage in software engineering in terms of accuracy, the size of datasets used and the specificity of the analysis. In this work, we are concerned with conducting entity-level sentiment analysis. We first build a manually labeled dataset containing 3,000 issue comments selected from 231,732 issue comments collected from 10 open source projects in GitHub. Then we design and develop SentiSW, an entity-level sentiment analysis tool consisting of sentiment classification and entity recognition, which can classify issue comments into <sentiment, entity=""> tuples. We evaluate the sentiment classification using ten-fold cross validation, and it achieves 68.71% mean precision, 63.98% mean recall and 77.19% accuracy, which is significantly higher than existing tools. We evaluate the entity recognition by manually annotation and it achieves a 75.15% accuracy.},
booktitle = {Proceedings of the 3rd International Workshop on Emotion Awareness in Software Engineering},
pages = {7–13},
numpages = {7},
keywords = {open source software project, entity-level sentiment analysis, entity recognition, sentiment classification},
location = {Gothenburg, Sweden},
series = {SEmotion '18}
}

@inproceedings{10.5555/3172795.3172813,
author = {Dantas, Carlos E. C. and de A. Maia, Marcelo},
title = {On the Actual Use of Inheritance and Interface in Java Projects: Evolution and Implications},
year = {2017},
publisher = {IBM Corp.},
address = {USA},
abstract = {Background: Inheritance is one of the main features in the object-oriented paradigm (OOP). Nonetheless, previous work recommend carefully using it, suggesting alternatives such as the adoption of composition with implementation of interfaces. Despite of being a well-studied theme, there is still little knowledge if such recommendations have been widely adopted by developers in general. Aims: This work aims at evaluating how the inheritance and composition with interfaces have been used in Java, comparing new projects with older ones (transversal), and also the different releases of the same projects (longitudinal). Method: A total of 1, 656 open-source projects built between 1997 and 2013, hosted in the repositories GitHub and SourceForge, were analyzed. The likelihood of more recent projects using inheritance and interfaces differently from older ones was analyzed considering indicators, such as, the prevalence of corrective changes, instanceof operations, and code smells. Regression analysis, chi-squared test of proportions and descriptive statistics were used to analyze the data. In addition, a thematic analysis based method was used to verify how often and why inheritance and interface are added or removed from classes. Results: We observed that developers still use inheritance primarily for code reuse, motivated by the need to avoid duplicity of source code. In newer projects, classes in inheritance had fewer corrective changes and subclasses had fewer use of the instance of operator. However, as they evolve, classes in inheritance tend to become complex as changes occur. Classes implementing interfaces have shown little relation to the interfaces, and there is indication that interfaces are still underutilized. Conclusion: These results show there is still some lack of knowledge about the use of recommended object-oriented practices, suggesting the need of training developers on how to design better classes.},
booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
pages = {151–160},
numpages = {10},
keywords = {interfaces, sourceforge, inheritance, cohesion, code smells, GitHub},
location = {Markham, Ontario, Canada},
series = {CASCON '17}
}

@inproceedings{10.1145/3211346.3211353,
author = {Sachdev, Saksham and Li, Hongyu and Luan, Sifei and Kim, Seohyun and Sen, Koushik and Chandra, Satish},
title = {Retrieval on Source Code: A Neural Code Search},
year = {2018},
isbn = {9781450358347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3211346.3211353},
doi = {10.1145/3211346.3211353},
abstract = {Searching over large code corpora can be a powerful productivity tool for both beginner and experienced developers because it helps them quickly find examples of code related to their intent. Code search becomes even more attractive if developers could express their intent in natural language, similar to the interaction that Stack Overflow supports. In this paper, we investigate the use of natural language processing and information retrieval techniques to carry out natural language search directly over source code, i.e. without having a curated Q&amp;A forum such as Stack Overflow at hand. Our experiments using a benchmark suite derived from Stack Overflow and GitHub repositories show promising results. We find that while a basic word–embedding based search procedure works acceptably, better results can be obtained by adding a layer of supervision, as well as by a customized ranking strategy.},
booktitle = {Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
pages = {31–41},
numpages = {11},
keywords = {word-embedding, TF-IDF, code search},
location = {Philadelphia, PA, USA},
series = {MAPL 2018}
}

@inproceedings{10.1145/3362789.3362877,
author = {Guerrero-Higueras, \'{A}ngel Manuel and S\'{a}nchez-Gonz\'{a}lez, Lidia and Conde, Miguel \'{A}ngel and Lera, Francisco J. Rodr\'{\i}guez and Castej\'{o}n-Limas, Manuel and Petkov, Nicolai},
title = {Facilitating the Learning Process in Parallel Computing by Using Instant Messaging},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362877},
doi = {10.1145/3362789.3362877},
abstract = {Parallel Programming skills may require long time to acquire. "Think in parallel" is a skill which requires time, effort, and experience. In this work, we propose to facilitate the learning process in parallel programming by using instant messaging by students. Our aim is to find out if students' interaction through instant messaging is beneficial for the learning process. We asked several students of an HPC course of the Master's degree in Computer Science to develop a specific parallel application, each of them using a different application program interface: OpenMP, MPI, CUDA, or OpenCL. Even though the used APIs are different, there are common points in the design process. We proposed to these students to interact with each other by using Gitter, an instant messaging tool for GitHub users. Our analysis of the communications and results demonstrate that the direct interaction of students through the Gitter tool has a positive impact on the learning process.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {558–563},
numpages = {6},
keywords = {Parallel Programming, Instant Messaging, High-performance Computing},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

@inproceedings{10.1145/3366424.3382692,
author = {Argyriou, Andreas and Gonz\'{a}lez-Fierro, Miguel and Zhang, Le},
title = {Microsoft Recommenders: Best Practices for Production-Ready Recommendation Systems},
year = {2020},
isbn = {9781450370240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366424.3382692},
doi = {10.1145/3366424.3382692},
abstract = {Recommendation algorithms have been widely applied in various contemporary business areas, however the process of implementing them in production systems is complex and has to address significant challenges. We present Microsoft Recommenders, an open-source Github repository for helping researchers, developers and non-experts in general to prototype, experiment with and bring to production both classic and state-of-the-art recommendation algorithms. A focus of this repository is on best practices in development of recommendation systems. We have also incorporated learnings from our experience with recommendation systems in production, in order to enhance ease of use; speed of implementation and deployment; scalability and performance.},
booktitle = {Companion Proceedings of the Web Conference 2020},
pages = {50–51},
numpages = {2},
keywords = {Recommender systems, Libraries, Algorithms},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1145/3328778.3372599,
author = {Wagner, Paul J.},
title = {The SQL File Evaluation (SQLFE) Tool: A Flexible and Extendible System for Evaluation of SQL Queries},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3372599},
doi = {10.1145/3328778.3372599},
abstract = {The SQL File Evaluation (SQLFE) tool is a GUI-based, flexible, and extendible software system for evaluating the correctness of files of student SQL queries as compared to specified SQL query answers. SQLFE can be configured per question to weight any of over 30 different tests in judging correctness of a submitted answer for a particular question. These tests include successful DBMS interpretation of the query, same result set (as specified answer), the use count of particular keywords, simple formatting style, and partial credit based on simple structural format. SQLFE currently works for databases constructed under Oracle and MySQL database management systems (DBMSs), and can be extended to more DBMSs. SQLFE is available from GitHub.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1334},
numpages = {1},
keywords = {sql, database systems, auto-grading, evaluation},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/2964284.2973796,
author = {Viitanen, Marko and Koivula, Ari and Lemmetti, Ari and Yl\"{a}-Outinen, Arttu and Vanne, Jarno and H\"{a}m\"{a}l\"{a}inen, Timo D.},
title = {Kvazaar: Open-Source HEVC/H.265 Encoder},
year = {2016},
isbn = {9781450336031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2964284.2973796},
doi = {10.1145/2964284.2973796},
abstract = {Kvazaar is an academic software video encoder for the emerging High Efficiency Video Coding (HEVC/H.265) standard. It provides students, academic professionals, and industry experts a free, cross-platform HEVC encoder for x86, x64, PowerPC, and ARM processors on Windows, Linux, and Mac. Kvazaar is being developed from scratch in C and optimized in Assembly under the LGPLv2.1 license. The development is being coordinated by Ultra Video Group at Tampere University of Technology (TUT) and the implementation work is carried out by an active community on GitHub. Developer friendly source code of Kvazaar makes joining easy for new developers. Currently, Kvazaar includes all essential coding tools of HEVC and its modular source code facilitates parallelization on multi and manycore processors as well as algorithm acceleration on hardware. Kvazaar is able to attain real-time HEVC coding speed up to 4K video on an Intel 14-core Xeon processor. Kvazaar is also supported by FFmpeg and Libav. These de-facto standard multimedia frameworks boost Kvazaar popularity and enable its joint usage with other well-known multimedia processing tools. Nowadays, Kvazaar is an integral part of teaching at TUT and it has got a key role in three Eureka Celtic-Plus projects in the fields of 4K TV broadcasting, virtual advertising, Video on Demand, and video surveillance.},
booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
pages = {1179–1182},
numpages = {4},
keywords = {video coding, open source, kvazaar hevc encoder, high efficiency video coding (HEVC), video encoder},
location = {Amsterdam, The Netherlands},
series = {MM '16}
}

@inproceedings{10.1145/2901739.2903497,
author = {Kery, Mary Beth and Le Goues, Claire and Myers, Brad A.},
title = {Examining Programmer Practices for Locally Handling Exceptions},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903497},
doi = {10.1145/2901739.2903497},
abstract = {Many have argued that the current try/catch mechanism for handling exceptions in Java is flawed. A major complaint is that programmers often write minimal and low quality handlers. We used the Boa tool to examine a large number of Java projects on GitHub to provide empirical evidence about how programmers currently deal with exceptions. We found that programmers handle exceptions locally in catch blocks much of the time, rather than propagating by throwing an Exception. Programmers make heavy use of actions like Log, Print, Return, or Throw in catch blocks, and also frequently copy code between handlers. We found bad practices like empty catch blocks or catching Exception are indeed widespread. We discuss evidence that programmers may misjudge risk when catching Exception, and face a tension between handlers that directly address local program statement failure and handlers that consider the program-wide implications of an exception. Some of these issues might be addressed by future tools which autocomplete more complete handlers.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {484–487},
numpages = {4},
keywords = {GitHub, Java exceptions, error handlers, Boa},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/2595188.2595197,
author = {Vobl, Thorsten and Gotscharek, Annette and Reffle, Uli and Ringlstetter, Christoph and Schulz, Klaus U.},
title = {PoCoTo - an Open Source System for Efficient Interactive Postcorrection of OCRed Historical Texts},
year = {2014},
isbn = {9781450325882},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2595188.2595197},
doi = {10.1145/2595188.2595197},
abstract = {When applied to historical texts, OCR engines often produce a non-negligible number of OCR errors. For research in the Humanities, text mining and retrieval, the option is important to improve the quality of OCRed historical texts using interactive postcorrection. We describe a system for interactive postcorrection of OCRed historical documents developed in the EU project IMPACT. Various advanced features of the system help to efficiently correct texts. Language technology used in the background takes orthographic variation in historical language into account. Using this knowledge, the tool visualizes possible OCR errors and series of similar possible OCR errors in a given input document. Error series can be corrected in one shot. Practical user tests in three major European libraries have shown that the system considerably reduces the time needed by human correctors to eliminate a certain number of OCR errors. The system has been published as an open source tool under GitHub.},
booktitle = {Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage},
pages = {57–61},
numpages = {5},
keywords = {error correction, user interfaces, decision support},
location = {Madrid, Spain},
series = {DATeCH '14}
}

@inproceedings{10.1145/3183440.3195054,
author = {Lu, Yao and Mao, Xinjun and Wang, Tao and Yin, Gang and Li, Zude and Wang, Huaimin},
title = {Continuous Inspection in the Classroom: Improving Students' Programming Quality with Social Coding Methods},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3195054},
doi = {10.1145/3183440.3195054},
abstract = {Rich research has shown that both the teaching and learning of high-quality programming are challenging and deficient in most colleges' education systems. Recently the continuous inspection paradigm has been widely used by developers on social coding sites (e.g., GitHub) as an important method to ensure the internal quality of massive code contributions. In this study we designed a specific continuous inspection process for students' collaborative projects and conducted a controlled experiment with 48 students from the same course during two school years to evaluate how the process affects their programming quality. Our results show that continuous inspection can significantly reduce the density of code quality issues introduced in the code.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {141–142},
numpages = {2},
keywords = {programming quality SonarQube, continuous inspection},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2896825.2896827,
author = {Ringlstetter, Andreas and Scherzinger, Stefanie and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Data Model Evolution Using Object-NoSQL Mappers: Folklore or State-of-the-Art?},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896827},
doi = {10.1145/2896825.2896827},
abstract = {In big data software engineering, the schema flexibility of NoSQL document stores is a major selling point: When the document store itself does not actively manage a schema, the data model is maintained within the application. Just like object-relational mappers for relational databases, object-NoSQL mappers are part of professional software development with NoSQL document stores. Some mappers go beyond merely loading and storing Java objects: Using dedicated evolution annotations, developers may conveniently add, remove, or rename attributes from stored objects, and also conduct more complex transformations. In this paper, we analyze the dissemination of this technology in Java open source projects. While we find evidence on GitHub that evolution annotations are indeed being used, developers do not employ them so much for evolving the data model, but to solve different tasks instead. Our observations trigger interesting questions for further research.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {33–36},
numpages = {4},
keywords = {object-NoSQL mappers, data model evolution},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1145/3236024.3264594,
author = {Wang, Kaiyuan and Sullivan, Allison and Marinov, Darko and Khurshid, Sarfraz},
title = {ASketch: A Sketching Framework for Alloy},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264594},
doi = {10.1145/3236024.3264594},
abstract = {Alloy is a declarative modeling language that supports first-order logic with transitive closure. Alloy has been used in a variety of domains to model software systems and find design deficiencies. However, it is often challenging to make an Alloy model correct or to debug a faulty Alloy model. ASketch is a sketching/synthesis technique that can help users write correct Alloy models. ASketch allows users to provide a partial Alloy model with holes, a generator that specifies candidate fragments to be considered for each hole, and a set of tests that capture the desired model properties. Then, the tool completes the holes such that all tests for the completed model pass. ASketch uses tests written for the recently introduced AUnit framework, which provides a foundation of testing (unit tests, test execution, and model coverage) for Alloy models in the spirit of traditional unit testing. This paper describes our Java implementation of ASketch, which is a command-line tool, released as an open-source project on GitHub. Our experimental results show that ASketch can handle partial Alloy models with multiple holes and a large search space. The demo video for ASketch can be found at https://youtu.be/T5NIVsV329E.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {916–919},
numpages = {4},
keywords = {ASketch, first-order logic, Sketching},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1007/978-3-319-35122-3_24,
author = {M\'{e}ndez-Acu\~{n}a, David and Galindo, Jos\'{e} A. and Combemale, Benoit and Blouin, Arnaud and Baudry, Benoit and Guernic, Gurvan},
title = {Reverse-Engineering Reusable Language Modules from Legacy Domain-Specific Languages},
year = {2016},
isbn = {9783319351216},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-35122-3_24},
doi = {10.1007/978-3-319-35122-3_24},
abstract = {The use of domain-specific languages DSLs has become a successful technique in the development of complex systems. Nevertheless, the construction of this type of languages is time-consuming and requires highly-specialized knowledge and skills. An emerging practice to facilitate this task is to enable reuse through the definition of language modules which can be later put together to build up new DSLs. Still, the identification and definition of language modules are complex and error-prone activities, thus hindering the reuse exploitation when developing DSLs. In this paper, we propose a computer-aided approach to i identify potential reuse in a set of legacy DSLs; and ii capitalize such potential reuse by extracting a set of reusable language modules with well defined interfaces that facilitate their assembly. We validate our approach by using realistic DSLs coming out from industrial case studies and obtained from public GitHub repositories.},
booktitle = {Proceedings of the 15th International Conference on Software Reuse: Bridging with Social-Awareness - Volume 9679},
pages = {368–383},
numpages = {16},
location = {Limassol, Cyprus},
series = {ICSR 2016}
}

@inproceedings{10.1145/3387904.3389270,
author = {Caulo, Maria and Lin, Bin and Bavota, Gabriele and Scanniello, Giuseppe and Lanza, Michele},
title = {Knowledge Transfer in Modern Code Review},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389270},
doi = {10.1145/3387904.3389270},
abstract = {Knowledge transfer is one of the main goals of modern code review, as shown by several studies that surveyed and interviewed developers. While knowledge transfer is a clear expectation of the code review process, there are no analytical studies using data mined from software repositories to assess the effectiveness of code review in "training" developers and improve their skills over time. We present a mining-based study investigating how and whether the code review process helps developers to improve their contributions to open source projects over time. We analyze 32,062 peer-reviewed pull requests (PRs) made across 4,981 GitHub repositories by 728 developers who created their GitHub account in 2015. We assume that PRs performed in the past by a developer D that have been subject to a code review process have "transferred knowledge" to D. Then, we verify if over time (i.e., when more and more reviewed PRs are made by D), the quality of the contributions made by D to open source projects increases (as assessed by proxies we defined, such as the acceptance of PRs, or the polarity of the sentiment in the review comments left for the submitted PRs). With the above measures, we were unable to capture the positive impact played by the code review process on the quality of developers' contributions. This might be due to several factors, including the choices we made in our experimental design.Additional investigations are needed to confirm or contradict such a negative result.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {230–240},
numpages = {11},
keywords = {mining software repositories, code review, knowledge transfer},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1109/MSR.2019.00068,
author = {Zhu, Jiaxin and Wei, Jun},
title = {An Empirical Study of Multiple Names and Email Addresses in OSS Version Control Repositories},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00068},
doi = {10.1109/MSR.2019.00068},
abstract = {Data produced by version control systems are widely used in software research and development. Version control data users always use the name or email address field to identify the committer or author of a modification. However, developers may use multiple names and email addresses, which brings difficulties for identification of distinct developers. In this paper, we sample 450 Git repositories from GitHub to study the multiple names and email addresses of developers. We conduct a conservative estimation of its prevalence and impact on related measurements. We merge the multiple names and email addresses of a developer through a method of high precision. With the merged identities, we obtain a number of interesting findings, e.g., about 6% of the developers used multiple names or email addresses in more than 60% of the repositories, and they contributed about half of all the commits. Our impact analysis shows that the multiple names and email addresses issue cannot be ignored for the basic related measurements, e.g., the number of developers in a repository. Our results could help researchers and practitioners have a more clear understanding of multiple names and email addresses in practice to improve the accuracy of related measurements.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {409–420},
numpages = {12},
keywords = {impact, multiple names, data, multiple email addresses, version control, pattern},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1109/ASE.2015.34,
author = {Greene, Gillian J.},
title = {A Generic Framework for Concept-Based Exploration of Semi-Structured Software Engineering Data},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.34},
doi = {10.1109/ASE.2015.34},
abstract = {Software engineering meta-data (SE data), such as revision control data, Github project data or test reports, is typically semi-structured; it comprises a mixture of formatted and free-text fields and is often self-describing. Semi-structured SE data cannot be queried in a SQL-like manner because of its lack of structure. Consequently, there are a variety of customized tools built to analyze specific datasets but these do not generalize. We propose to develop a generic framework for exploration and querying of semi-structured SE data. Our approach investigates the use of a formal concept lattice as a universal data structure and a tag cloud as an intuitive interface to support data exploration.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {894–897},
numpages = {4},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3411502.3418429,
author = {Li, Kaiyuan and Woo, Maverick and Jia, Limin},
title = {On the Generation of Disassembly Ground Truth and the Evaluation of Disassemblers},
year = {2020},
isbn = {9781450380898},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411502.3418429},
doi = {10.1145/3411502.3418429},
abstract = {When a software transformation or software security task needs to analyze a given program binary, the first step is often disassembly. Since many modern disassemblers have become highly accurate on many binaries, we believe reliable disassembler benchmarking requires standardizing the set of binaries used and the disassembly ground truth about these binaries. This paper presents (i) a first version of our work-in-progress disassembly benchmark suite, which comprises $879$ binaries from diverse projects compiled with multiple compilers and optimization settings, and (ii) a novel disassembly ground truth generator leveraging the notion of "listing files'', which has broad support by clang, gcc, icc, and msvc. In additional, it presents our evaluation of four prominent open-source disassemblers using this benchmark suite and a custom evaluation system. Our entire system and all generated data are maintained openly on GitHub to encourage community adoption.},
booktitle = {Proceedings of the 2020 ACM Workshop on Forming an Ecosystem Around Software Transformation},
pages = {9–14},
numpages = {6},
keywords = {disassembly, benchmark suite, ground-truth generation},
location = {Virtual Event, USA},
series = {FEAST'20}
}

@inproceedings{10.1145/3195836.3195848,
author = {Coelho, Jailton and Valente, Marco Tulio and Silva, Luciana L. and Hora, Andr\'{e}},
title = {Why We Engage in FLOSS: Answers from Core Developers},
year = {2018},
isbn = {9781450357258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195836.3195848},
doi = {10.1145/3195836.3195848},
abstract = {The maintenance and evolution of Free/Libre Open Source Software (FLOSS) projects demand the constant attraction of core developers. In this paper, we report the results of a survey with 52 developers, who recently became core contributors of popular GitHub projects. We reveal their motivations to assume a key role in FLOSS projects (e.g., improving the projects because they are also using it), the project characteristics that most helped in their engagement process (e.g., a friendly community), and the barriers faced by the surveyed core developers (e.g., lack of time of the project leaders). We also compare our results with related studies about others kinds of open source contributors (casual, one-time, and newcomers).},
booktitle = {Proceedings of the 11th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {114–121},
numpages = {8},
keywords = {open source software, core developers, github},
location = {Gothenburg, Sweden},
series = {CHASE '18}
}

@inproceedings{10.1145/3321707.3321842,
author = {Hansen, Nikolaus},
title = {A Global Surrogate Assisted CMA-ES},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321842},
doi = {10.1145/3321707.3321842},
abstract = {We explore the arguably simplest way to build an effective surrogate fitness model in continuous search spaces. The model complexity is linear or diagonal-quadratic or full quadratic, depending on the number of available data. The model parameters are computed from the Moore-Penrose pseudoinverse. The model is used as a surrogate fitness for CMA-ES if the rank correlation between true fitness and surrogate value of recently sampled data points is high. Otherwise, further samples from the current population are successively added as data to the model. We empirically compare the IPOP scheme of the new model assisted lq-CMA-ES with a variety of previously proposed methods and with a simple portfolio algorithm using SLSQP and CMA-ES. We conclude that a global quadratic model and a simple portfolio algorithm are viable options to enhance CMA-ES. The model building code is available as part of the pycma Python module on Github and PyPI.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {664–672},
numpages = {9},
keywords = {CMA-ES, covariance matrix adaptation, quadratic model, evolution strategies, surrogate},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.5555/2487085.2487153,
author = {Vasilescu, Bogdan and Serebrenik, Alexander and Mens, Tom},
title = {A Historical Dataset of Software Engineering Conferences},
year = {2013},
isbn = {9781467329361},
publisher = {IEEE Press},
abstract = { The Mining Software Repositories community typically focuses on data from software configuration management tools, mailing lists, and bug tracking repositories to uncover interesting and actionable information about the evolution of software systems. However, the techniques employed and the challenges faced when mining are not restricted to these types of repositories. In this paper, we present an atypical dataset of software engineering conferences, containing historical data about the accepted papers and the composition of programme committees for eleven well-established conferences. The dataset (published on Github at https://github.com/tue-mdse/conferenceMetrics) can be used, e.g., by conference steering committees or programme committee chairs to assess their selection process and compare against other conferences in the field, or by prospective authors to decide in which conferences to publish. },
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
pages = {373–376},
numpages = {4},
location = {San Francisco, CA, USA},
series = {MSR '13}
}

@inproceedings{10.1145/3107411.3107466,
author = {Awan, Muaaz Gul and Saeed, Fahad},
title = {An Out-of-Core GPU Based Dimensionality Reduction Algorithm for Big Mass Spectrometry Data and Its Application in Bottom-up Proteomics},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3107466},
doi = {10.1145/3107411.3107466},
abstract = {Modern high resolution Mass Spectrometry instruments can generate millions of spectra in a single systems biology experiment. Each spectrum consists of thousands of peaks but only a small number of peaks actively contribute to deduction of peptides. Therefore, pre-processing of MS data to detect noisy and non-useful peaks are an active area of research. Most of the sequential noise reducing algorithms are impractical to use as a pre-processing step due to high time-complexity. In this paper, we present a GPU based dimensionality-reduction algorithm, called G-MSR, for MS2 spectra. Our proposed algorithm uses novel data structures which optimize the memory and computational operations inside GPU. These novel data structures include Binary Spectra and Quantized Indexed Spectra (QIS). The former helps in communicating essential information between CPU and GPU using minimum amount of data while latter enables us to store and process complex 3-D data structure into a 1-D array structure while maintaining the integrity of MS data. Our proposed algorithm also takes into account the limited memory of GPUs and switches between in-core and out-of-core modes based upon the size of input data. G-MSR achieves a peak speed-up of 386x over its sequential counterpart and is shown to process over a million spectra in just 32 seconds. The code for this algorithm is available as a GPL open-source at GitHub at the following link: https://github.com/pcdslab/G-MSR.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {550–555},
numpages = {6},
keywords = {parallel computing, bigdata, data reduction, out-of-core, proteomics, algorithms, mass spectrometry, cuda, gpu},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/2901739.2903501,
author = {Sinha, Vinayak and Lazar, Alina and Sharif, Bonita},
title = {Analyzing Developer Sentiment in Commit Logs},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903501},
doi = {10.1145/2901739.2903501},
abstract = {The paper presents an analysis of developer commit logs for GitHub projects. In particular, developer sentiment in commits is analyzed across 28,466 projects within a seven year time frame. We use the Boa infrastructure's online query system to generate commit logs as well as files that were changed during the commit. We analyze the commits in three categories: large, medium, and small based on the number of commits using a sentiment analysis tool. In addition, we also group the data based on the day of week the commit was made and map the sentiment to the file change history to determine if there was any correlation. Although a majority of the sentiment was neutral, the negative sentiment was about 10% more than the positive sentiment overall. Tuesdays seem to have the most negative sentiment overall. In addition, we do find a strong correlation between the number of files changed and the sentiment expressed by the commits the files were part of. Future work and implications of these results are discussed.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {520–523},
numpages = {4},
keywords = {commit logs, Java projects, sentiment analysis},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3340672.3341113,
author = {R\"{u}mmer, Philipp},
title = {JayHorn: A Java Model Checker},
year = {2019},
isbn = {9781450368643},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340672.3341113},
doi = {10.1145/3340672.3341113},
abstract = {This talk will give an overview of the JayHorn verification tool, a model checker for sequential Java programs annotated with assertions expressing safety conditions. JayHorn is fully automatic and based to a large degree on standard infrastructure for compilation and verification: it uses the Soot library as front-end to read Java bytecode and translate it to the Jimple three-address format, and the state-of-the-art Horn solvers SPACER and Eldarica as back-ends that infer loop invariants, object and class invariants, and method contracts. Since JayHorn uses an invariant-based representation of heap data-structures, it is particularly useful for analysing programs with unbounded data-structures and unbounded run-time, while at the same time avoiding the use of logical theories, like the theory of arrays, often considered hard for Horn solvers. The development of JayHorn is ongoing, and the talk will also cover some of the future features of JayHorn, in particular the handling of strings.The talk presents joint work with Daniel Dietsch, Temesghen Kahsai, Rody Kersten, Huascar Sanchez, Martin Sch\"{a}f, and Valentin W\"{u}stholz.JayHorn is open source and distributed under MIT license, and its source code is available on Github (https://github.com/jayhorn/jayhorn). The development of JayHorn is funded in parts by AFRL contract No. FA8750-15-C-0010, NSF Award No. 1422705, by the Swedish Research Council (VR) under grants 2014-5484 and 2018-4727, and by the Swedish Foundation for Strategic Research (SSF) under the project WebSec (Ref. RIT17-0011).},
booktitle = {Proceedings of the 21st Workshop on Formal Techniques for Java-like Programs},
articleno = {1},
numpages = {1},
location = {London, United Kingdom},
series = {FTfJP '19}
}

@inproceedings{10.1145/3368089.3409705,
author = {Lamba, Hemank and Trockman, Asher and Armanios, Daniel and K\"{a}stner, Christian and Miller, Heather and Vasilescu, Bogdan},
title = {Heard It through the Gitvine: An Empirical Study of Tool Diffusion across the Npm Ecosystem},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409705},
doi = {10.1145/3368089.3409705},
abstract = {Automation tools like continuous integration services, code coverage reporters, style checkers, dependency managers, etc. are all known to provide significant improvements in developer productivity and software quality. Some of these tools are widespread, others are not. How do these automation "best practices" spread? And how might we facilitate the diffusion process for those that have seen slower adoption? In this paper, we rely on a recent innovation in transparency on code hosting platforms like GitHub---the use of repository badges---to track how automation tools spread in open-source ecosystems through different social and technical mechanisms over time. Using a large longitudinal data set, multivariate network science techniques, and survival analysis, we study which socio-technical factors can best explain the observed diffusion process of a number of popular automation tools. Our results show that factors such as social exposure, competition, and observability affect the adoption of tools significantly, and they provide a roadmap for software engineers and researchers seeking to propagate best practices and tools.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {505–517},
numpages = {13},
keywords = {open source ecosystem, software tools, innovations, diffusion},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2676723.2677287,
author = {Zadrozny, Wlodek W. and Gallagher, Sean and Shalaby, Walid and Avadhani, Adarsh},
title = {Simulating IBM Watson in the Classroom},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2677287},
doi = {10.1145/2676723.2677287},
abstract = {IBM Watson exemplifies multiple innovations in natural language processing and question answering. In addition, Watson uses most of the known techniques in these two domains as well as many methods from related domains. Hence, there is pedagogical value in a rigorous understanding of its function. The paper provides the description of a text analytics course focused on building a simulator of IBM Watson, conducted in Spring 2014 at UNC Charlotte. We believe this is the first time a simulation containing all the major Watson components was created in a university classroom. The system achieved a respectable (close to) 20% accuracy on Jeopardy! questions, and there remain many known and new avenues of improving performance that can be explored in the future. The code and documentation are available on GitHub. The paper is a joint effort of the teacher and some of the students who were leading teams implementing component technologies, and therefore deeply involved in making the class successful.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {72–77},
numpages = {6},
keywords = {ibm watson, student research, question answering, qa},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1109/ASEW.2015.21,
author = {Bogart, Christopher and Kastner, Christian and Herbsleb, James},
title = {When It Breaks, It Breaks: How Ecosystem Developers Reason about the Stability of Dependencies},
year = {2015},
isbn = {9781467397759},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASEW.2015.21},
doi = {10.1109/ASEW.2015.21},
abstract = {Dependencies among software projects and libraries are an indicator of the often implicit collaboration among many developers in software ecosystems. Negotiating change can be tricky: changes to one module may cause ripple effects to many other modules that depend on it, yet insisting on only backward-compatible changes may incur significant opportunity cost and stifle change. We argue that awareness mechanisms based on various notions of stability can enable developers to make decisions that are independent yet wise and provide stewardship rather than disruption to the ecosystem. In ongoing interviews with developers in two software ecosystems (CRAN and Node.js), we are finding that developers in fact struggle with change, that they often use adhoc mechanisms to negotiate change, and that existing awareness mechanisms like Github notification feeds are rarely used due to information overload. We study the state of the art and current information needs and outline a vision toward a change-based awareness system.},
booktitle = {Proceedings of the 2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)},
pages = {86–89},
numpages = {4},
series = {ASEW '15}
}

@inproceedings{10.5555/2820518.2820588,
author = {Spinellis, Diomidis},
title = {A Repository with 44 Years of Unix Evolution},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {The evolution of the Unix operating system is made available as a version-control repository, covering the period from its inception in 1972 as a five thousand line kernel, to 2015 as a widely-used 26 million line system. The repository contains 659 thousand commits and 2306 merges. The repository employs the commonly used Git system for its storage, and is hosted on the popular GitHub archive. It has been created by synthesizing with custom software 24 snapshots of systems developed at Bell Labs, Berkeley University, and the 386BSD team, two legacy repositories, and the modern repository of the open source FreeBSD system. In total, 850 individual contributors are identified, the early ones through primary research. The data set can be used for empirical research in software engineering, information systems, and software archaeology.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {462–465},
numpages = {4},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3173574.3173606,
author = {Rule, Adam and Tabard, Aur\'{e}lien and Hollan, James D.},
title = {Exploration and Explanation in Computational Notebooks},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173606},
doi = {10.1145/3173574.3173606},
abstract = {Computational notebooks combine code, visualizations, and text in a single document. Researchers, data analysts, and even journalists are rapidly adopting this new medium. We present three studies of how they are using notebooks to document and share exploratory data analyses. In the first, we analyzed over 1 million computational notebooks on GitHub, finding that one in four had no explanatory text but consisted entirely of visualizations or code. In a second study, we examined over 200 academic computational notebooks, finding that although the vast majority described methods, only a minority discussed reasoning or results. In a third study, we interviewed 15 academic data analysts, finding that most considered computational notebooks personal, exploratory, and messy. Importantly, they typically used other media to share analyses. These studies demonstrate a tension between exploration and explanation in constructing and sharing computational notebooks. We conclude with opportunities to encourage explanation in computational media without hindering exploration.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {jupyter notebook, data analysis, narrative, data science, computational notebook},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{10.1145/2901739.2903500,
author = {Asaduzzaman, Muhammad and Ahasanuzzaman, Muhammad and Roy, Chanchal K. and Schneider, Kevin A.},
title = {How Developers Use Exception Handling in Java?},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903500},
doi = {10.1145/2901739.2903500},
abstract = {Exception handling is a technique that addresses exceptional conditions in applications, allowing the normal flow of execution to continue in the event of an exception and/or to report on such events. Although exception handling techniques, features and bad coding practices have been discussed both in developer communities and in the literature, there is a marked lack of empirical evidence on how developers use exception handling in practice. In this paper we use the Boa language and infrastructure to analyze 274k open source Java projects in GitHub to discover how developers use exception handling. We not only consider various exception handling features but also explore bad coding practices and their relation to the experience of developers. Our results provide some interesting insights. For example, we found that bad exception handling coding practices are common in open source Java projects and regardless of experience all developers use bad exception handling coding practices.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {516–519},
numpages = {4},
keywords = {exception, Java, language feature, source code mining},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3382494.3422166,
author = {Hazhirpasand, Mohammadreza and Ghafari, Mohammad and Nierstrasz, Oscar},
title = {Java Cryptography Uses in the Wild},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422166},
doi = {10.1145/3382494.3422166},
abstract = {[Background] Previous research has shown that developers commonly misuse cryptography APIs. [Aim] We have conducted an exploratory study to find out how crypto APIs are used in open-source Java projects, what types of misuses exist, and why developers make such mistakes. [Method] We used a static analysis tool to analyze hundreds of open-source Java projects that rely on Java Cryptography Architecture, and manually inspected half of the analysis results to assess the tool results. We also contacted the maintainers of these projects by creating an issue on the GitHub repository of each project, and discussed the misuses with developers. [Results] We learned that 85% of Cryptography APIs are misused, however, not every misuse has severe consequences. Developer feedback showed that security caveats in the documentation of crypto APIs are rare, developers may overlook misuses that originate in third-party code, and the context where a Crypto API is used should be taken into account. [Conclusion] We conclude that using Crypto APIs is still problematic for developers but blindly blaming them for such misuses may lead to erroneous conclusions.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {40},
numpages = {6},
keywords = {empirical study, Java cryptography, security},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1109/MSR.2019.00024,
author = {Raghuraman, Adithya and Ho-Quang, Truong and Chaudron, Michel R. V. and Serebrenik, Alexander and Vasilescu, Bogdan},
title = {Does UML Modeling Associate with Lower Defect Proneness? A Preliminary Empirical Investigation},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00024},
doi = {10.1109/MSR.2019.00024},
abstract = {The benefits of modeling the design to improve the quality and maintainability of software systems have long been advocated and recognized. Yet, the empirical evidence on this remains scarce. In this paper, we fill this gap by reporting on an empirical study of the relationship between UML modeling and software defect proneness in a large sample of open-source GitHub projects. Using statistical modeling, and controlling for confounding variables, we show that projects containing traces of UML models in their repositories experience, on average, a statistically minorly different number of software defects (as mined from their issue trackers) than projects without traces of UML models.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {101–104},
numpages = {4},
keywords = {UML, software design, open-source-software, software quality},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3236024.3236072,
author = {Wang, Peipei and Stolee, Kathryn T.},
title = {How Well Are Regular Expressions Tested in the Wild?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236072},
doi = {10.1145/3236024.3236072},
abstract = {Developers report testing their regular expressions less than the rest of their code. In this work, we explore how thoroughly tested regular expressions are by examining open source projects.  Using standard metrics of coverage, such as line and branch coverage, gives an incomplete picture of the test coverage of regular expressions. We adopt graph-based coverage metrics for the DFA representation of regular expressions, providing fine-grained test coverage metrics. Using over 15,000 tested regular expressions in 1,225 Java projects on GitHub, we measure node, edge, and edge-pair coverage. Our results show that only 17% of the regular expressions in the repositories are tested at all. For those that are tested, the median number of test inputs is two. For nearly 42% of the tested regular expressions, only one test input is used. Average node and edge coverage levels on the DFAs for tested regular expressions are 59% and 29%, respectively. Due to the lack of testing of regular expressions, we explore whether a string generation tool for regular expressions, Rex, achieves high coverage levels. With some exceptions, we found that tools such as Rex can be used to write test inputs with similar coverage to the developer tests.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {668–678},
numpages = {11},
keywords = {Deterministic Finite Automaton, Regular expressions, Test coverage metrics},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2502081.2502228,
author = {Aamulehto, Rami and Kuhna, Mikko and Tarvainen, Jussi and Oittinen, Pirkko},
title = {Stage Framework: An HTML5 and CSS3 Framework for Digital Publishing},
year = {2013},
isbn = {9781450324045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2502081.2502228},
doi = {10.1145/2502081.2502228},
abstract = {In this paper we present Stage Framework, an HTML5 and CSS3 framework for digital book, magazine and newspaper publishing. The framework offers publishers the means and tools for publishing editorial content in the HTML5 format using a single web application. The approach is cross-platform and is based on open web standards. Stage Framework serves as an alternative for platform-specific native publications using pure HTML5 to deliver book, magazine and newspaper content while retaining the familiar gesture interaction of native applications. Available gesture actions include for example the page swipe and kinetic scrolling. The magazine browsing view relies entirely on CSS3 3D Transforms and Transitions, thus utilizing hardware acceleration in most devices and platforms. The web application also features a magazine stand which, can be used to offer issues of multiple publications. Developed as a part of master's thesis research, the framework has been published under the GPL and MIT licenses and is available to everyone via the framework website (http://stageframework.com) and the GitHub repository (http://github.com/ralatalo/stage).},
booktitle = {Proceedings of the 21st ACM International Conference on Multimedia},
pages = {851–854},
numpages = {4},
keywords = {digital publishing, browser technology, tablet magazine, web application, HTML5, CSS3},
location = {Barcelona, Spain},
series = {MM '13}
}

@inproceedings{10.1145/3307339.3343174,
author = {Tillquist, Richard C.},
title = {Low-Dimensional Representation of Biological Sequence Data},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343174},
doi = {10.1145/3307339.3343174},
abstract = {Systems of interest in bioinformatics and computational biology tend to be large, complex, interdependent, and stochastic. As our ability to collect sequence data at finer resolutions improves, we can better understand and predict system behavior under different conditions. Machine learning algorithms are a powerful set of tools designed to help with this understanding. However, many of the most effective of these algorithms are not immediately amenable to application on symbolic data. It is often necessary to map biological symbols to real vectors before performing analysis or prediction using sequence data. This tutorial will cover several techniques for embedding sequence data. Common methods utilizing k-mer count and binary vector representations will be addressed along with state of the art methods based on neural networks, like BioVec, and graph embeddings, like Node2Vec and multilateration. This tutorial is in collaboration with M. Lladser. Slides, datasets, and code from the tutorial will be made freely available for future use on GitHub.},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {555},
numpages = {1},
keywords = {neural networks, multilateration, sequence data, symbolic data science, graph embeddings},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1145/3148055.3148071,
author = {Torres, Johnny and Vaca, Carmen and Abad, Cristina L.},
title = {What Ignites a Reply? Characterizing Conversations in Microblogs},
year = {2017},
isbn = {9781450355490},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3148055.3148071},
doi = {10.1145/3148055.3148071},
abstract = {Nowadays, microblog platforms provide a medium to share content and interact with other users. With the large-scale data generated on these platforms, the origin and reasons of users engagement in conversations has attracted the attention of the research community. In this paper, we analyze the factors that might spark conversations in Twitter, for the English and Spanish languages. Using a corpus of 2.7 million tweets, we reconstruct existing conversations, then extract several contextual and content features. Based on the features extracted, we train and evaluate several predictive models to identify tweets that will spark a conversation. Our findings show that conversations are more likely to be initiated by users with high activity level and popularity. For less popular users, the type of content generated is a more important factor. Experimental results shows that the best predictive model is able obtain an average score $F1=0.80$. We made available the dataset scripts and code used in this paper to the research community via Github.},
booktitle = {Proceedings of the Fourth IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
pages = {149–156},
numpages = {8},
keywords = {machine learning, social computing, big data},
location = {Austin, Texas, USA},
series = {BDCAT '17}
}

@inproceedings{10.1145/3379597.3387498,
author = {Henkel, Jordan and Bird, Christian and Lahiri, Shuvendu K. and Reps, Thomas},
title = {A Dataset of Dockerfiles},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387498},
doi = {10.1145/3379597.3387498},
abstract = {Dockerfiles are one of the most prevalent kinds of DevOps artifacts used in industry. Despite their prevalence, there is a lack of sophisticated semantics-aware static analysis of Dockerfiles. In this paper, we introduce a dataset of approximately 178,000 unique Dockerfiles collected from GitHub. To enhance the usability of this data, we describe five representations we have devised for working with, mining from, and analyzing these Dockerfiles. Each Dockerfile representation builds upon the previous ones, and the final representation, created by three levels of nested parsing and abstraction, makes tasks such as mining and static checking tractable. The Dockerfiles, in each of the five representations, along with metadata and the tools used to shepard the data from one representation to the next are all available at: https://doi.org/10.5281/zenodo.3628771.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {528–532},
numpages = {5},
keywords = {DevOps, Datasets, Docker, Bash, Mining},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3338906.3338917,
author = {Zhang, Chen and Chen, Bihuan and Chen, Linlin and Peng, Xin and Zhao, Wenyun},
title = {A Large-Scale Empirical Study of Compiler Errors in Continuous Integration},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338917},
doi = {10.1145/3338906.3338917},
abstract = {Continuous Integration (CI) is a widely-used software development practice to reduce risks. CI builds often break, and a large amount of efforts are put into troubleshooting broken builds. Despite that compiler errors have been recognized as one of the most frequent types of build failures, little is known about the common types, fix efforts and fix patterns of compiler errors that occur in CI builds of open-source projects. To fill such a gap, we present a large-scale empirical study on 6,854,271 CI builds from 3,799 open-source Java projects hosted on GitHub. Using the build data, we measured the frequency of broken builds caused by compiler errors, investigated the ten most common compiler error types, and reported their fix time. We manually analyzed 325 broken builds to summarize fix patterns of the ten most common compiler error types. Our findings help to characterize and understand compiler errors during CI and provide practical implications to developers, tool builders and researchers.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {176–187},
numpages = {12},
keywords = {Continuous Integration, Compiler Errors, Build Failures},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/2890046.2890056,
author = {Vassiliadis, Vangelis and Wielemaker, Jan and Mungall, Chris},
title = {Processing OWL2 Ontologies Using Thea: An Application of Logic Programming},
year = {2009},
publisher = {CEUR-WS.org},
address = {Aachen, DEU},
abstract = {Traditional object-oriented programming languages can be difficult to use when working with ontologies, leading to the creation of domain-specific languages designed specifically for ontology processing. Prolog, with its logic-based, declarative semantics offers many advantages as a host programming language for querying and processing OWL2 ontologies. The SWI-Prolog semweb library provides some support for OWL but until now there has been a lack of any library providing direct and comprehensive support for OWL2.We have developed Thea, a library based directly on the OWL2 functional-style syntax, allowing storage and manipulation of axioms as a Prolog database. Thea can translate ontologies to Description Logic programs but the emphasis is on using Prolog as an application programming and processing language rather than a reasoning engine. Thea offers the ability to seamless connect to the java OWL API and OWLLink servers. Thea also includes support for SWRL.In this paper we provide examples of using Thea for processing ontologies, and compare the results to alternative methods. Thea is available from GitHub: http://github.com/vangelisv/thea.},
booktitle = {Proceedings of the 6th International Conference on OWL: Experiences and Directions - Volume 529},
pages = {89–98},
numpages = {10},
location = {Chantilly, VA},
series = {OWLED'09}
}

@inproceedings{10.5555/3306127.3332031,
author = {Menashe, Jacob and Stone, Peter},
title = {Escape Room: A Configurable Testbed for Hierarchical Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Recent successes in Reinforcement Learning have encouraged a fast growing network of RL researchers and a number of breakthroughs in RL research. As the RL community and body of work grows, so does the need for widely applicable benchmarks that can fairly and effectively evaluate a variety of RL algorithms. In this paper we present the Escape Room Domain (ERD), a new flexible, scalable, and fully implemented testing domain for Hierarchical RL that bridges the "moderate complexity" gap left behind by existing alternatives. ERD is open-source and freely available through GitHub, and conforms to widely-used public testing interfaces for simple integration and testing with a variety of public RL agent implementations.},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2123–2125},
numpages = {3},
keywords = {tools, simulation techniques, platforms, reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@inproceedings{10.1145/2567948.2578843,
author = {Loyola, Pablo and Ko, In-Young},
title = {Population Dynamics in Open Source Communities: An Ecological Approach Applied to Github},
year = {2014},
isbn = {9781450327459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2567948.2578843},
doi = {10.1145/2567948.2578843},
abstract = {Open Source Software (OSS) has gained high amount of popularity during the last few years. It is becoming used by public and private institutions, even companies release portions of their code to obtain feedback from the community of voluntary developers. As OSS is based on the voluntary contributions of developers, the number of participants represents one of the key elements that impact the quality of the software. In order to understand how the the population of contributors evolve over time, we propose a methodology that adapts Lotka-Volterra-based biological models used for describing host-parasite interactions. Experiments based on data from the Github collaborative platform showed that the proposed approach performs effectively in terms of providing an estimation of the population of developers for each project over time.},
booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
pages = {993–998},
numpages = {6},
keywords = {open source software development, ecological models, biological mutualism},
location = {Seoul, Korea},
series = {WWW '14 Companion}
}

@inproceedings{10.1145/3379597.3387484,
author = {Cor\`{o}, Federico and Verdecchia, Roberto and Cruciani, Emilio and Miranda†, Breno and Bertolino, Antonia},
title = {JTeC: A Large Collection of Java Test Classes for Test Code Analysis and Processing},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387484},
doi = {10.1145/3379597.3387484},
abstract = {The recent push towards test automation and test-driven development continues to scale up the dimensions of test code that needs to be maintained, analysed, and processed side-by-side with production code. As a consequence, on the one side regression testing techniques, e.g., for test suite prioritization or test case selection, capable to handle such large-scale test suites become indispensable; on the other side, as test code exposes own characteristics, specific techniques for its analysis and refactoring are actively sought. We present JTeC, a large-scale dataset of test cases that researchers can use for benchmarking the above techniques or any other type of tool expressly targeting test code. JTeC collects more than 2.5M test classes belonging to 31K+ GitHub projects and summing up to more than 430 Million SLOCs of ready-to-use real-world test code.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {578–582},
numpages = {5},
keywords = {Java, Test Suite, Large Scale, GitHub, Software Testing},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/MSR.2019.00077,
author = {Pimentel, Jo\~{a}o Felipe and Murta, Leonardo and Braganholo, Vanessa and Freire, Juliana},
title = {A Large-Scale Study about Quality and Reproducibility of Jupyter Notebooks},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00077},
doi = {10.1109/MSR.2019.00077},
abstract = {Jupyter Notebooks have been widely adopted by many different communities, both in science and industry. They support the creation of literate programming documents that combine code, text, and execution results with visualizations and all sorts of rich media. The self-documenting aspects and the ability to reproduce results have been touted as significant benefits of notebooks. At the same time, there has been growing criticism that the way notebooks are being used leads to unexpected behavior, encourage poor coding practices, and that their results can be hard to reproduce. To understand good and bad practices used in the development of real notebooks, we studied 1.4 million notebooks from GitHub. We present a detailed analysis of their characteristics that impact reproducibility. We also propose a set of best practices that can improve the rate of reproducibility and discuss open challenges that require further research and development.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {507–517},
numpages = {11},
keywords = {github, reproducibility, jupyter notebook},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/2993412.3003384,
author = {Constantinou, Eleni and Mens, Tom},
title = {Social and Technical Evolution of Software Ecosystems: A Case Study of Rails},
year = {2016},
isbn = {9781450347815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993412.3003384},
doi = {10.1145/2993412.3003384},
abstract = {Software ecosystems evolve through an active community of developers who contribute to projects within the ecosystem. However, development teams change over time, suggesting a potential impact on the evolution of the technical parts of the ecosystem. The impact of such modifications has been studied by previous works, but only temporary changes have been investigated, while the long-term effect of permanent changes has yet to be explored. In this paper, we investigate the evolution of the ecosystem of Ruby on Rails in GitHub in terms of such temporary and permanent changes of the development team. We use three viewpoints of the Rails ecosystem evolution to discuss our preliminary findings: (1) the base project; (2) the forks; and (3) the entire ecosystem containing both base project and forks.},
booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
articleno = {23},
numpages = {4},
keywords = {technical evolution, software ecosystems, social evolution},
location = {Copenhagen, Denmark},
series = {ECSAW '16}
}

@inproceedings{10.1145/2950290.2950305,
author = {Silva, Danilo and Tsantalis, Nikolaos and Valente, Marco Tulio},
title = {Why We Refactor? Confessions of GitHub Contributors},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950305},
doi = {10.1145/2950290.2950305},
abstract = { Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refactoring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect recently applied refactorings, and asked the developers to explain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring activity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the developers affects the adoption of automated refactoring tools. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {858–870},
numpages = {13},
keywords = {software evolution, Refactoring, GitHub, code smells},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3321707.3321738,
author = {Sobania, Dominik and Rothlauf, Franz},
title = {Teaching GP to Program like a Human Software Developer: Using Perplexity Pressure to Guide Program Synthesis Approaches},
year = {2019},
isbn = {9781450361118},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3321707.3321738},
doi = {10.1145/3321707.3321738},
abstract = {Program synthesis is one of the relevant applications of GP with a strong impact on new fields such as genetic improvement. In order for synthesized code to be used in real-world software, the structure of the programs created by GP must be maintainable. We can teach GP how real-world software is built by learning the relevant properties of mined human-coded software - which can be easily accessed through repository hosting services such as GitHub. So combining program synthesis and repository mining is a logical step. In this paper, we analyze if GP can write programs with properties similar to code produced by human software developers. First, we compare the structure of functions generated by different GP initialization methods to a mined corpus containing real-world software. The results show that the studied GP initialization methods produce a totally different combination of programming language elements in comparison to real-world software. Second, we propose perplexity pressure and analyze how its use changes the properties of code produced by GP. The results are very promising and show that we can guide the search to the desired program structure. Thus, we recommend using perplexity pressure as it can be easily integrated in various search-based algorithms.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1065–1074},
numpages = {10},
keywords = {mining software repositories, software synthesis, genetic improvement, grammatical evolution, genetic programming, language models},
location = {Prague, Czech Republic},
series = {GECCO '19}
}

@inproceedings{10.1145/2876034.2893422,
author = {Martin, Taylor and Brasiel, Sarah and Jeong, Soojeong and Close, Kevin and Lawanto, Kevin and Janisciewcz, Phil},
title = {Macro Data for Micro Learning: Developing the FUN! Tool for Automated Assessment of Learning},
year = {2016},
isbn = {9781450337267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2876034.2893422},
doi = {10.1145/2876034.2893422},
abstract = {Digital learning environments are becoming more common for students to engage in during and outside of school. With the immense amount of data now available from these environments, researchers need tools to process, manage, and analyze the data. Current methods used by many education researchers are inefficient; however, without data science experience tools used in other professions are not accessible. In this paper, we share about a tool we created called the Functional Understanding Navigator! (FUN! Tool). We have used this tool for different research projects which has allowed us the opportunity to (1) organize our workflow process from start to finish, (2) record log data of all of our analyses, and (3) provide a platform to share our analyses with others through GitHub. This paper extends and improves existing work in educational data mining and learning analytics.},
booktitle = {Proceedings of the Third (2016) ACM Conference on Learning @ Scale},
pages = {233–236},
numpages = {4},
keywords = {assessment, micro learning, educational data mining, digital learning environments},
location = {Edinburgh, Scotland, UK},
series = {L@S '16}
}

@inproceedings{10.1145/3307339.3343482,
author = {Eslami, Taban and Saeed, Fahad},
title = {Auto-ASD-Network: A Technique Based on Deep Learning and Support Vector Machines for Diagnosing Autism Spectrum Disorder Using FMRI Data},
year = {2019},
isbn = {9781450366663},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3307339.3343482},
doi = {10.1145/3307339.3343482},
abstract = {Quantitative analysis of brain disorders such as Autism Spectrum Disorder (ASD) is an ongoing field of research. Machine learning and deep learning techniques have been playing an important role in automating the diagnosis of brain disorders by extracting discriminative features from the brain data. In this study, we propose a model called Auto-ASD-Network in order to classify subjects with Autism disorder from healthy subjects using only fMRI data. Our model consists of a multilayer perceptron (MLP) with two hidden layers. We use an algorithm called SMOTE for performing data augmentation in order to generate artificial data and avoid overfitting, which helps increase the classification accuracy. We further investigate the discriminative power of features extracted using MLP by feeding them to an SVM classifier. In order to optimize the hyperparameters of SVM, we use a technique called Auto Tune Models (ATM) which searches over the hyperparameter space to find the best values of SVM hyperparameters. Our model achieves more than 70% classification accuracy for 4 fMRI datasets with the highest accuracy of 80%. It improves the performance of SVM by 26%, the stand-alone MLP by 16% and the state of the art method in ASD classification by 14%. The implemented code will be available as GPL license on GitHub portal of our lab (https://github.com/PCDS).},
booktitle = {Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
pages = {646–651},
numpages = {6},
keywords = {classification, time series, fmri, asd, mlp, deep learning, pearson's correlation coefficient},
location = {Niagara Falls, NY, USA},
series = {BCB '19}
}

@inproceedings{10.1109/MSR.2019.00030,
author = {Pietri, Antoine and Spinellis, Diomidis and Zacchiroli, Stefano},
title = {The Software Heritage Graph Dataset: Public Software Development under One Roof},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00030},
doi = {10.1109/MSR.2019.00030},
abstract = {Software Heritage is the largest existing public archive of software source code and accompanying development history: it currently spans more than five billion unique source code files and one billion unique commits, coming from more than 80 million software projects.This paper introduces the Software Heritage graph dataset: a fully-deduplicated Merkle DAG representation of the Software Heritage archive. The dataset links together file content identifiers, source code directories, Version Control System (VCS) commits tracking evolution over time, up to the full states of VCS repositories as observed by Software Heritage during periodic crawls. The dataset's contents come from major development forges (including GitHub and GitLab), FOSS distributions (e.g., Debian), and language-specific package managers (e.g., PyPI). Crawling information is also included, providing timestamps about when and where all archived source code artifacts have been observed in the wild.The Software Heritage graph dataset is available in multiple formats, including downloadable CSV dumps and Apache Parquet files for local use, as well as a public instance on Amazon Athena interactive query service for ready-to-use powerful analytical processing.Source code file contents are cross-referenced at the graph leaves, and can be retrieved through individual requests using the Software Heritage archive API.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {138–142},
numpages = {5},
keywords = {free software, digital preservation, source code, open source software, mining software repositories, development history graph, dataset},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3311790.3399619,
author = {Nandigam, Viswanath and Lin, Kai and Shantharam, Manu and Sakai, Scott and Sivagnanam, Subhashini},
title = {Research Workflows - Towards Reproducible Science via Detailed Provenance Tracking in Open Science Chain},
year = {2020},
isbn = {9781450366892},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3311790.3399619},
doi = {10.1145/3311790.3399619},
abstract = {Scientific research has always struggled with problems related to reproducibility caused in part by low data sharing rates and lack of provenance. Credibility of the research hypothesis comes into question when results cannot be replicated. While the growing amount of data and widespread use of computational code in research has been pushing scientific breakthroughs, their references in scientific publications is insufficient from a reproducibility perspective. The NSF funded Open Science Chain (OSC) is a cyberinfrastructure platform built using blockchain technologies that enables researchers to efficiently validate the authenticity of published data, track their provenance and view lineage. It does this by leveraging blockchain technology to securely store metadata and verification information about research data and track changes to that data in an auditable manner. In this poster we introduce the concept of ”research workflows”, a tool that allows researchers to create a detailed workflow of their scientific experiment, linking specific data and computational code used in their published results in order to enable independent verification of the analysis. OSC research workflows will allow for detailed provenance tracking both within the OSC platform as well as external repositories like Github, thereby enabling transparency and fostering trust in the scientific process. },
booktitle = {Practice and Experience in Advanced Research Computing},
pages = {484–486},
numpages = {3},
keywords = {Data Reproducibility, Data Integrity, Blockchain, Data Provenance},
location = {Portland, OR, USA},
series = {PEARC '20}
}

@inproceedings{10.5555/2486788.2486869,
author = {Hosek, Petr and Cadar, Cristian},
title = {Safe Software Updates via Multi-Version Execution},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Software systems are constantly evolving, with new versions and patches being released on a continuous basis. Unfortunately, software updates present a high risk, with many releases introducing new bugs and security vulnerabilities.  We tackle this problem using a simple but effective multi-version based approach. Whenever a new update becomes available, instead of upgrading the software to the new version, we run the new version in parallel with the old one; by carefully coordinating their executions and selecting the behaviour of the more reliable version when they diverge, we create a more secure and dependable multi-version application.  We implemented this technique in Mx, a system targeting Linux applications running on multi-core processors, and show that it can be applied successfully to several real applications such as Coreutils, a set of user-level UNIX applications; Lighttpd, a popular web server used by several high-traffic websites such as Wikipedia and YouTube; and Redis, an advanced key-value data structure server used by many well-known services such as GitHub and Flickr. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {612–621},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3341105.3374009,
author = {Misra, Vishal and Reddy, Jakku Sai Krupa and Chimalakonda, Sridhar},
title = {Is There a Correlation between Code Comments and Issues? An Exploratory Study},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374009},
doi = {10.1145/3341105.3374009},
abstract = {Comments in a software code base are one of the key artifacts that help developers in understanding the code with respect to development and maintenance. Comments provide us with the information that is used as a software metric to assess the code quality and which further can be applied to demonstrate its impact on the issues in the code. In this paper, we set out to understand the correlation between code comments and issues in Github. We conduct an empirical study on 625 repositories hosted on GitHub with Python as their primary language. We manually classify comments from a randomly selected sample of python repositories and then train and evaluate classifiers to automatically label comments as Relevant or Auxiliary. We extract the metadata of issues in each repository present in our dataset and perform various experiments to understand the correlation between code comments and issues. From our dataset of python repositories, we then plot a graph between the average time taken to resolve an issue and percentage of relevant comments in a repository to find if there is any relation or a pattern by which the latter affects the former. Our statistical approach of finding out the correlation between code comments and issues gives us the correlation factor by which code comments are related to issues. We conclude from our study that comments are indeed important and play an important role in solving issues of the project. We also found that increasing the percentage of relevant comments along with the source code can help in the reduction of the average number of days before an issue is resolved.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {110–117},
numpages = {8},
keywords = {machine learning, correlation, issues, code comments, python repositories, empirical study},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3379597.3387450,
author = {Pietri, Antoine and Rousseau, Guillaume and Zacchiroli, Stefano},
title = {Forking Without Clicking: On How to Identify Software Repository Forks},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387450},
doi = {10.1145/3379597.3387450},
abstract = {The notion of software "fork" has been shifting over time from the (negative) phenomenon of community disagreements that result in the creation of separate development lines and ultimately software products, to the (positive) practice of using distributed version control system (VCS) repositories to collaboratively improve a single product without stepping on each others toes. In both cases the VCS repositories participating in a fork share parts of a common development history.Studies of software forks generally rely on hosting platform metadata, such as GitHub, as the source of truth for what constitutes a fork. These "forge forks" however can only identify as forks repositories that have been created on the platform, e.g., by clicking a "fork" button on the platform user interface. The increased diversity in code hosting platforms (e.g., GitLab) and the habits of significant development communities (e.g., the Linux kernel, which is not primarily hosted on any single platform) call into question the reliability of trusting code hosting platforms to identify forks. Doing so might introduce selection and methodological biases in empirical studies.In this article we explore various definitions of "software forks", trying to capture forking workflows that exist in the real world. We quantify the differences in how many repositories would be identified as forks on GitHub according to the various definitions, confirming that a significant number could be overlooked by only considering forge forks. We study the structure and size of fork networks, observing how they are affected by the proposed definitions and discuss the potential impact on empirical research.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {277–287},
numpages = {11},
keywords = {version control system, free software, open source, software evolution, source code, software fork},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/2568225.2568305,
author = {Singer, Leif and Figueira Filho, Fernando and Storey, Margaret-Anne},
title = {Software Engineering at the Speed of Light: How Developers Stay Current Using Twitter},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568305},
doi = {10.1145/2568225.2568305},
abstract = { The microblogging service Twitter has over 500 million users posting over 500 million tweets daily. Research has established that software developers use Twitter in their work, but this has not yet been examined in detail. Twitter is an important medium in some software engineering circles—understanding its use could lead to improved support, and learning more about the reasons for non-adoption could inform the design of improved tools. In a qualitative study, we surveyed 271 and interviewed 27 developers active on GitHub. We find that Twitter helps them keep up with the fast-paced development landscape. They use it to stay aware of industry changes, for learning, and for building relationships. We discover the challenges they experience and extract their coping strategies. Some developers do not want to or cannot embrace Twitter for their work—we show their reasons and alternative channels. We validate our findings in a follow-up survey with more than 1,200 respondents. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {211–221},
numpages = {11},
keywords = {Awareness, Learning, Social Media, Microblogging, Twitter},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2855667.2855678,
author = {Mustyatsa, Vadim},
title = {BDD by Example: Russian Bylina Written in Gherkin Language},
year = {2015},
isbn = {9781450341301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2855667.2855678},
doi = {10.1145/2855667.2855678},
abstract = {In this paper is presented the User Stories technique within the Behaviour-Driven Development process by example of the famous Russian bylina (epic poem) "Three trips of Ilya Muromets" written in Gherkin language. In the first part are given explanations about the choice of the bylina as a good example for presentation of this technique and about the choice of GitHub as a good environment for this presentation. In the second part is placed the text of the presentation divided by the stages of the Story development and fitted with the links to the corresponding commits and file versions in the educational repository. A distinct advantage of this presentation is that it reflects a Story in its evolution, as a process. It reflects a more complex and changeable behaviour than in static trivial examples, which are usually used. Also, there are presented the main features of using the User Stories technique in Russian language, which have not been previously covered nowhere. Besides that, the using of the educational repository can significantly increase the possibilities for further spreading and development of the example.},
booktitle = {Proceedings of the 11th Central &amp; Eastern European Software Engineering Conference in Russia},
articleno = {10},
numpages = {15},
keywords = {extreme programming, epic poetry, byliny, examples, Agile, user stories, cucumber, behaviour-driven development, Scrum, JBehave, GitHub, Gherkin},
location = {Moscow, Russia},
series = {CEE-SECR '15}
}

@inproceedings{10.1145/3338906.3338922,
author = {Widder, David Gray and Hilton, Michael and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {A Conceptual Replication of Continuous Integration Pain Points in the Context of Travis CI},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338922},
doi = {10.1145/3338906.3338922},
abstract = {Continuous integration (CI) is an established software quality assurance practice, and the focus of much prior research with a diverse range of methods and populations. In this paper, we first conduct a literature review of 37 papers on CI pain points. We then conduct a conceptual replication study on results from these papers using a triangulation design consisting of a survey with 132 responses, 12 interviews, and two logistic regressions predicting Travis CI abandonment and switching on a dataset of 6,239 GitHub projects. We report and discuss which past results we were able to replicate, those for which we found conflicting evidence, those for which we did not find evidence, and the implications of these findings.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {647–658},
numpages = {12},
keywords = {Continuous integration, open source software, replication},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2801040.2801066,
author = {Trouv\'{e}, Antoine and Murakami, Kazuaki J.},
title = {Interactive Visualization of Quantitative Data with G2D3},
year = {2015},
isbn = {9781450334822},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2801040.2801066},
doi = {10.1145/2801040.2801066},
abstract = {This article introduces G2D3, an implementation of the grammar of graphics in JavaScript, along with two practical use cases that illustrate its practicability. It makes it possible to generate interactive visualization of quantitative data in HTML/SVG. Compared to traditional static data visualization systems such as those featured in Excel and R, G2D3 makes it possible to greatly enhance the amount of conveyed information by means of animation and interaction. Compared to other JavaScript plotting libraries such as Rapha\"{e}l and D3, G2D3 leverages the expressiveness and the flexibility of the grammar of graphics to concisely generate complex visualization with many plotting dimensions, including time. It makes it possible to create a wide range of graphics with a few lines of code. G2D3 is open source and hosted in GitHub: join us!},
booktitle = {Proceedings of the 8th International Symposium on Visual Information Communication and Interaction},
pages = {154–155},
numpages = {2},
location = {Tokyo, AA, Japan},
series = {VINCI '15}
}

@inproceedings{10.5555/3432601.3432618,
author = {Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris},
title = {Report on Evaluation Experiments Using Different Machine Learning Techniques for Defect Prediction},
year = {2020},
publisher = {IBM Corp.},
address = {USA},
abstract = {With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.},
booktitle = {Proceedings of the 30th Annual International Conference on Computer Science and Software Engineering},
pages = {123–132},
numpages = {10},
location = {Toronto, Ontario, Canada},
series = {CASCON '20}
}

@inproceedings{10.1109/IPDPSW.2013.161,
author = {Zhu, Zhaomeng and Zhang, Gongxuan and Zhang, Yongping and Guo, Jian and Xiong, Naixue},
title = {Briareus: Accelerating Python Applications with Cloud},
year = {2013},
isbn = {9780769549798},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IPDPSW.2013.161},
doi = {10.1109/IPDPSW.2013.161},
abstract = {Briareus provides convenient tools to make use of computing resources provided by cloud to accelerate Python applications. In this paper, three techniques are presented. First, some of the functions in a Python program can be migrated to cloud and be evaluated using the hardware and software provided by that cloud platform, while the other parts still running locally. Second, Briareus can automatically parallelize specified loops in a program to accelerate it. And third, specified functions can be called asynchronously after being patched, so that two or more functions can be evaluated simultaneously. By combining these three methods, a Python application can make part of itself to run in a remote cloud platform in parallel. To use Briareus, developers do not need to modify the existing source much, but only need to insert some descriptive comments and invoke a patching function at the beginning. Experiments show that Briareus can significantly speed up the running of programs written by Python, especially for those for scientific and engineering computing. The early beta version of Briareus has been developed for testing and all sources are accessible to public via GitHub and installable via PyPI.},
booktitle = {Proceedings of the 2013 IEEE 27th International Symposium on Parallel and Distributed Processing Workshops and PhD Forum},
pages = {1449–1456},
numpages = {8},
keywords = {Parallel architectures, Computer languages, Software tools, Distributed computing, Clouds},
series = {IPDPSW '13}
}

@inproceedings{10.1109/MSR.2019.00036,
author = {Trockman, Asher and van Tonder, Rijnard and Vasilescu, Bogdan},
title = {Striking Gold in Software Repositories? An Econometric Study of Cryptocurrencies on GitHub},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00036},
doi = {10.1109/MSR.2019.00036},
abstract = {Cryptocurrencies have a significant open source development presence on GitHub. This presents a unique opportunity to observe their related developer effort and software growth. Individual cryptocurrency prices are partly driven by attractiveness, and we hypothesize that high-quality, actively-developed software is one of its influences. Thus, we report on a study of a panel data set containing nearly a year of daily observations of development activity, popularity, and market capitalization for over two hundred open source cryptocurrencies. We find that open source project popularity is associated with higher market capitalization, though development activity and quality assurance practices are insignificant variables in our models. Using Granger causality tests, we find no compelling evidence for a dynamic relation between market capitalization and metrics such as daily stars, forks, watchers, commits, contributors, and lines of code changed.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {181–185},
numpages = {5},
keywords = {cryptocurrency, software quality, open source software, software metrics, github},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3092703.3098238,
author = {Santolucito, Mark},
title = {Version Space Learning for Verification on Temporal Differentials},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098238},
doi = {10.1145/3092703.3098238},
abstract = { Configuration files provide users with the ability to quickly alter the behavior of their software system. Ensuring that a configuration file does not induce errors in the software is a complex verification issue. The types of errors can be easy to measure, such as an initialization failure of system boot, or more insidious such as performance degrading over time under heavy network loads. In order to warn a user of potential configuration errors ahead of time, we propose using version space learning specifications for configuration languages. We frame an existing tool, ConfigC, in terms of version space learning. We extend that algorithm to leverage the temporal structuring available in training sets scraped from versioning control systems. We plan to evaluate our system on a case study using TravisCI configuration files collected from Github. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {428–431},
numpages = {4},
keywords = {Configuration Files, Verification, Machine Learning},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/3217804.3217895,
author = {Celi\'{n}ska, Dorota},
title = {Coding Together in a Social Network: Collaboration among GitHub Users},
year = {2018},
isbn = {9781450363341},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3217804.3217895},
doi = {10.1145/3217804.3217895},
abstract = {In this article we investigate developers involved in the creation of Open Source software to identify which characteristics favor innovation in the Open Source community. The results of the analysis show that higher reputation in the community improves the probability of gaining collaborators to a certain degree, but developers are also driven by reciprocity. This is consistent with the concept of gift economy. A significant network effect exists and emerges from standardization, showing that developers using the most popular programming languages in the service are likely to have more collaborators. Providing additional information (valid URL to developer's homepage) improves the chances of finding coworkers. The results can be generalized for the population of mature users of GitHub.},
booktitle = {Proceedings of the 9th International Conference on Social Media and Society},
pages = {31–40},
numpages = {10},
keywords = {network externality, gift economy, reciprocity, collaboration, innovations, GitHub, forking, reputation, Open Source},
location = {Copenhagen, Denmark},
series = {SMSociety '18}
}

@inproceedings{10.1109/ICSE-C.2017.114,
author = {Hassan, Foyzul and Wang, Xiaoyin},
title = {Mining Readme Files to Support Automatic Building of Java Projects in Software Repositories: Poster},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.114},
doi = {10.1109/ICSE-C.2017.114},
abstract = {Automatic building of software projects provides a desirable foundation to support a large variety of software engineering research tasks based on open software repositories. In this paper, we propose the first technique to automatically extract software build commands from software readme files and Wiki pages, and combine the extracted commands for software building. Specifically, we leverage the Named Entity Recognition (NER) technique for build-command extraction, and prioritize the extracted build commands to identify which one should be used in software build. Our experiment on top Java projects from GitHub reveals that, the proposed technique can correctly identify more than 90% of build commands, and can successfully build 84% of the projects that can be built successfully through manual inspection of software support documents.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {277–279},
numpages = {3},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1109/ICSE-SEIS.2019.17,
author = {Ford, Denae and Behroozi, Mahnaz and Serebrenik, Alexander and Parnin, Chris},
title = {Beyond the Code Itself: How Programmers <i>Really</i> Look at Pull Requests},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIS.2019.17},
doi = {10.1109/ICSE-SEIS.2019.17},
abstract = {Developers in open source projects must make decisions on contributions from other community members, such as whether or not to accept a pull request. However, secondary factors---beyond the code itself---can influence those decisions. For example, signals from GitHub profiles, such as a number of followers, activity, names, or gender can also be considered when developers make decisions. In this paper, we examine how developers use these signals (or not) when making decisions about code contributions. To evaluate this question, we evaluate how signals related to perceived gender identity and code quality influenced decisions on accepting pull requests. Unlike previous work, we analyze this decision process with data collected from an eye-tracker. We analyzed differences in what signals developers said are important for themselves versus what signals they actually used to make decisions about others. We found that after the code snippet (x = 57%), the second place programmers spent their time ixating is on supplemental technical signals (x = 32%), such as previous contributions and popular repositories. Diverging from what participants reported themselves, we also found that programmers ixated on social signals more than recalled.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Society},
pages = {51–60},
numpages = {10},
keywords = {open source software development, transparency, eye-tracking, code contributions, socio-technical ecosystems},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIS '19}
}

@inproceedings{10.1109/ICSM.2015.7332486,
author = {Linares-Vasquez, Mario and Vendome, Christopher and Luo, Qi and Poshyvanyk, Denys},
title = {How Developers Detect and Fix Performance Bottlenecks in Android Apps},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332486},
doi = {10.1109/ICSM.2015.7332486},
abstract = {Performance of rapidly evolving mobile apps is one of the top concerns for users and developers nowadays. Despite the efforts of researchers and mobile API designers to provide developers with guidelines and best practices for improving the performance of mobile apps, performance bottlenecks are still a significant and frequent complaint that impacts the ratings and apps' chances for success. However, little research has been done into understanding actual developers' practices for detecting and fixing performance bottlenecks in mobile apps. In this paper, we present the results of an empirical study aimed at studying and understanding these practices by surveying 485 open source Android app and library developers, and manually analyzing performance bugs and fixes in their app repositories hosted on GitHub. The paper categorizes actual practices and tools used by real developers while dealing with performance issues. In general, our findings indicate that developers heavily rely on user reviews and manual execution of the apps for detecting performance bugs. While developers also use available tools to detect performance bottlenecks, these tools are mostly for profiling and do not help in detecting and fixing performance issues automatically.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {352–361},
numpages = {10},
series = {ICSME '15}
}

@inproceedings{10.1145/3306446.3340827,
author = {Chua, Bee Bee and Zhang, Ying},
title = {Predicting Open Source Programming Language Repository File Survivability from Forking Data},
year = {2019},
isbn = {9781450363198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3306446.3340827},
doi = {10.1145/3306446.3340827},
abstract = {Very few studies have looked at repositories' programming language survivability in response to forking conditions. A high number of repository programming languages does not alone ensure good forking performance. To address this issue and assist project owners in adopting the right programming language, it is necessary to predict programming language survivability from forking in repositories. This paper therefore addresses two related questions: are there statistically meaningful patterns within repository data and, if so, can these patterns be used to predict programming language survival? To answer these questions we analysed 47,000 forking instances in 1000 GitHub projects. We used Euclidean distance applied in the K-Nearest Neighbour algorithm to predict the distance between repository file longevity and forking conditions. We found three pattern types ('once-only', intermittent or steady) and propose reasons for short-lived programming languages.},
booktitle = {Proceedings of the 15th International Symposium on Open Collaboration},
articleno = {2},
numpages = {8},
keywords = {survivability, prediction, open source, programming language, K-nearest neighbour, euclidean distance, forking},
location = {Sk\"{o}vde, Sweden},
series = {OpenSym '19}
}

@inproceedings{10.1109/MSR.2017.52,
author = {Noten, Jeroen and Mengerink, Josh G. M. and Serebrenik, Alexander},
title = {A Data Set of OCL Expressions on GitHub},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.52},
doi = {10.1109/MSR.2017.52},
abstract = {In model driven engineering (MDE), meta-models are the central artifacts. As a complement, the Object Constraint Language (OCL) is a language used to express constraints and operations on meta-models. The Eclipse Modeling Framework (EMF) provides an implementation of OCL, enabling OCL annotated meta-models.Existing empirical studies of the OCL have been conducted on small collections of data. To facilitate empirical research into the OCL on a larger scale, we present the first publicly-available data set of OCL expressions. The data set contains 9188 OCL expressions originating from 504 EMF meta-models in 245 systematically selected GitHub repositories. Both the original meta-models and the generated abstract syntax trees are included, allowing for a variety of empirical studies of the OCL. To illustrate the applicability of this data set in practice, we performed three case studies.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {531–534},
numpages = {4},
keywords = {data set, OCL, GitHub},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/2901739.2901743,
author = {Wittern, Erik and Suter, Philippe and Rajagopalan, Shriram},
title = {A Look at the Dynamics of the JavaScript Package Ecosystem},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901743},
doi = {10.1145/2901739.2901743},
abstract = {The node package manager (npm) serves as the frontend to a large repository of JavaScript-based software packages, which foster the development of currently huge amounts of server-side Node. js and client-side JavaScript applications. In a span of 6 years since its inception, npm has grown to become one of the largest software ecosystems, hosting more than 230, 000 packages, with hundreds of millions of package installations every week. In this paper, we examine the npm ecosystem from two complementary perspectives: 1) we look at package descriptions, the dependencies among them, and download metrics, and 2) we look at the use of npm packages in publicly available applications hosted on GitHub. In both perspectives, we consider historical data, providing us with a unique view on the evolution of the ecosystem. We present analyses that provide insights into the ecosystem's growth and activity, into conflicting measures of package popularity, and into the adoption of package versions over time. These insights help understand the evolution of npm, design better package recommendation engines, and can help developers understand how their packages are being used.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {351–361},
numpages = {11},
keywords = {JavaScript, Node.js, software ecosystem analysis},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1109/NOMS47738.2020.9110458,
author = {Granderath, Malte and Sch\"{o}nw\"{a}lder, J\"{u}rgen},
title = {A Resource Efficient Implementation of the RESTCONF Protocol for OpenWrt Systems},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NOMS47738.2020.9110458},
doi = {10.1109/NOMS47738.2020.9110458},
abstract = {In recent years, the open source operating system OpenWrt has become a popular option for replacing proprietary firmware on networking devices such as home routers or access points. In order to configure an OpenWrt system, like setting up firewall rules, the user has to either sign in to the web interface or use SSH to manually change configuration files on the device. While the current approach is sufficient for small home networks, it only allows for limited automation of management tasks and configuration management becomes time-consuming, for example, on larger campus networks where access control lists on OpenWrt access points need updates regularly.This paper describes our efforts to implement the RESTCONF configuration management protocol standardized by the IETF on OpenWrt systems that have limited CPU and memory resources. We detail our design choices that make our implementation resource efficient for the use cases we target and we compare our implementation against other similar solutions. Our implementation is available on GitHub under an open source license<sup>1</sup>.},
booktitle = {NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium},
pages = {1–6},
numpages = {6},
location = {Budapest, Hungary}
}

@inproceedings{10.1145/3183519.3183540,
author = {Urli, Simon and Yu, Zhongxing and Seinturier, Lionel and Monperrus, Martin},
title = {How to Design a Program Repair Bot? Insights from the Repairnator Project},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183540},
doi = {10.1145/3183519.3183540},
abstract = {Program repair research has made tremendous progress over the last few years, and software development bots are now being invented to help developers gain productivity. In this paper, we investigate the concept of a "program repair bot" and present Repairnator. The Repairnator bot is an autonomous agent that constantly monitors test failures, reproduces bugs, and runs program repair tools against each reproduced bug. If a patch is found, Repairnator bot reports it to the developers. At the time of writing, Repairnator uses three different program repair systems and has been operating since February 2017. In total, it has studied 11 523 test failures over 1 609 open-source software projects hosted on GitHub, and has generated patches for 15 different bugs. Over months, we hit a number of hard technical challenges and had to make various design and engineering decisions. This gives us a unique experience in this area. In this paper, we reflect upon Repairnator in order to share this knowledge with the automatic program repair community.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {95–104},
numpages = {10},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1145/3139903.3139916,
author = {Salgado, Ronie and Bergel, Alexandre},
title = {Pharo Git Thermite: A Visual Tool for Deciding to Weld a Pull Request},
year = {2017},
isbn = {9781450355544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3139903.3139916},
doi = {10.1145/3139903.3139916},
abstract = {Collaborative software development platforms such as GitHub simplify the process of contributing into open source projects by the use of a pull request. The decision of accepting or rejecting a pull request has to be made by an integrator. Because reviewing a pull request can be time consuming, social factors are known to have an important effect on the acceptation of a pull request. This effect can be especially important for large and complicated pull request.In this paper we present Git Thermite, a tool to assess the internal structure of a pull request and simplifying the job of the integrator. Git Thermite details the structural changes made on the source code. In Git Thermite we use a pull request business card visual metaphor for describing a pull request. In this business card, we present the pull request metadata and describe the modified files, and the structural changes in the modified source code.},
booktitle = {Proceedings of the 12th Edition of the International Workshop on Smalltalk Technologies},
articleno = {11},
numpages = {6},
location = {Maribor, Slovenia},
series = {IWST '17}
}

@inproceedings{10.1109/ALLERTON.2019.8919901,
author = {Song, Bowen and Trachtenberg, Ari},
title = {Scalable String Reconciliation by Recursive Content-Dependent Shingling},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ALLERTON.2019.8919901},
doi = {10.1109/ALLERTON.2019.8919901},
abstract = {We consider the problem of reconciling similar, but remote, strings with minimum communication complexity. This “string reconciliation” problem is a fundamental building block for a variety of networking applications, including those that maintain large-scale distributed networks and perform remote file synchronization. We present the novel Recursive Content-Dependent Shingling (RCDS) protocol that is computationally practical for large strings and scales linearly with the edit distance between the remote strings. We provide comparisons to the performance of rsync, one of the most popular file synchronization tools in active use. Our experiments show that, with minimal engineering, RCDS outperforms the heavily optimized rsync in reconciling release revisions for about 51% of the 5000 top starred git repositories on GitHub. The improvement is particularly evident for repositories that see frequent, but small, updates.},
booktitle = {2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
pages = {623–630},
numpages = {8},
location = {Monticello, IL, USA}
}

@inproceedings{10.1145/3379597.3387463,
author = {Meinicke, Jens and Hoyos, Juan and Vasilescu, Bogdan and K\"{a}stner, Christian},
title = {Capture the Feature Flag: Detecting Feature Flags in Open-Source},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387463},
doi = {10.1145/3379597.3387463},
abstract = {Feature flags (a.k.a feature toggles) are a mechanism to keep new features hidden behind a boolean option during development. Flags are used for many purposes, such as A/B testing and turning off a feature more easily in case of failures. While software engineering research on feature flags is burgeoning, examples of software projects using flags rarely come from outside commercial and private projects, stifling academic progress. To address this gap, in this paper we present a novel semi-automated mining software repositories approach to detect feature flags in open-source projects, based on analyzing the projects' commit messages and other project characteristics. With our approach, we search over all open-source GitHub projects, finding multiple thousand plausible and active candidate feature flagging projects. We manually validate projects and assemble a dataset of 100 confirmed feature flagging projects. To demonstrate the benefits of our detection technique, we report on an initial analysis of feature flags in the validated sample of 100 projects, investigating practices that correlate with shorter flag lifespans (typically desirable to reduce technical debt), such as using the issue tracker and having a flag owner.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {169–173},
numpages = {5},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/ASE.2019.00098,
author = {Tavares, Alberto Trindade and Borba, Paulo and Cavalcanti, Guilherme and Soares, S\'{e}rgio},
title = {Semistructured Merge in JavaScript Systems},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00098},
doi = {10.1109/ASE.2019.00098},
abstract = {Industry widely uses unstructured merge tools that rely on textual analysis to detect and resolve conflicts between code contributions. Semistructured merge tools go further by partially exploring the syntactic structure of code artifacts, and, as a consequence, obtaining significant merge accuracy gains for Java-like languages. To understand whether semistructured merge and the observed gains generalize to other kinds of languages, we implement two semistructured merge tools for JavaScript, and compare them to an unstructured tool. We find that current semistructured merge algorithms and frameworks are not directly applicable for scripting languages like JavaScript. By adapting the algorithms, and studying 10,345 merge scenarios from 50 JavaScript projects on GitHub, we find evidence that our JavaScript tools report fewer spurious conflicts than unstructured merge, without compromising the correctness of the merging process. The gains, however, are much smaller than the ones observed for Java-like languages, suggesting that semistructured merge advantages might be limited for languages that allow both commutative and non-commutative declarations at the same syntactic level.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1014–1025},
numpages = {12},
keywords = {semistructured merge, software merging, JavaScript, version control systems, collaborative development},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3383583.3398548,
author = {Yang, Le and Zhang, Zhongda and Chen, Enci},
title = {Customization and Localization of DSpace-CRIS in China},
year = {2020},
isbn = {9781450375856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383583.3398548},
doi = {10.1145/3383583.3398548},
abstract = {Traditional institutional repository (IR) has been broadly used and improved in practice for decades. Current Research Information System (CRIS) is one of the extended systems that broadens the traditional IR systems' functionality by expanding the data and visualization modules. Beyond the basic functions of an IR, CRIS extends to distribute multimedia scholarly publications, manage research data, provide evaluation on research performance, visualize research network, enable research profiling, support project-based activities, integrate citation metrics, etc. This paper introduces the first DSpace-CRIS system that was implemented in mainland China at Wenzhou-Kean University (WKU) and explains the localized efforts of technologies and customized development of modules. The development team has also released the installation of developed modules as open sources on GitHub. The paper outlines the future development plan for the institution.},
booktitle = {Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020},
pages = {545–546},
numpages = {2},
keywords = {current research information system, institutional repository, visualization tools, DSpace-CRIS, readership map},
location = {Virtual Event, China},
series = {JCDL '20}
}

@inproceedings{10.1145/3213846.3213875,
author = {Shi, August and Gyori, Alex and Mahmood, Suleman and Zhao, Peiyuan and Marinov, Darko},
title = {Evaluating Test-Suite Reduction in Real Software Evolution},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213875},
doi = {10.1145/3213846.3213875},
abstract = {Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.  We perform the first extensive study of TSR using real test failures in (failed) builds that occurred for real code changes. We analyze 1478 failed builds from 32 GitHub projects that run their tests on Travis. Each failed build can have multiple faults, so we propose a family of mappings from test failures to faults. We use these mappings to compute Failed-Build Detection Loss (FBDL), the percentage of failed builds where the reduced test suite misses to detect all the faults detected by the original test suite. We find that FBDL can be up to 52.2%, which is higher than suggested by traditional TSR metrics. Moreover, traditional TSR metrics are not good predictors of FBDL, making it difficult for developers to decide whether to use reduced test suites.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {84–94},
numpages = {11},
keywords = {continuous integration, Test-suite reduction, regression testing},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/2642937.2648627,
author = {Thung, Ferdian and Kochhar, Pavneet Singh and Lo, David},
title = {DupFinder: Integrated Tool Support for Duplicate Bug Report Detection},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2648627},
doi = {10.1145/2642937.2648627},
abstract = {To track bugs that appear in a software, developers often make use of a bug tracking system. Users can report bugs that they encounter in such a system. Bug reporting is inherently an uncoordinated distributed process though and thus when a user submits a new bug report, there might be cases when another bug report describing exactly the same problem is already present in the system. Such bug reports are duplicate of each other and these duplicate bug reports need to be identified. A number of past studies have proposed a number of automated approaches to detect duplicate bug reports. However, these approaches are not integrated to existing bug tracking systems. In this paper, we propose a tool named DupFinder, which implements the state-of-the-art unsupervised duplicate bug report approach by Runeson et al., as a Bugzilla extension. DupFinder does not require any training data and thus can easily be deployed to any project. DupFinder extracts texts from summary and description fields of a new bug report and recent bug reports present in a bug tracking system, uses vector space model to measure similarity of bug reports, and provides developers with a list of potential duplicate bug reports based on the similarity of these reports with the new bug report. We have released DupFinder as an open source tool in GitHub, which is available at: https://github.com/smagsmu/dupfinder.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {871–874},
numpages = {4},
keywords = {bugzilla, duplicate bug reports, integrated tool support},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3202667.3202694,
author = {Peng, Zhenhui and Yoo, Jeehoon and Xia, Meng and Kim, Sunghun and Ma, Xiaojuan},
title = {Exploring How Software Developers Work with Mention Bot in GitHub},
year = {2018},
isbn = {9781450365086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3202667.3202694},
doi = {10.1145/3202667.3202694},
abstract = {Recently, major software development platforms have started to provide automatic reviewer recommendation (ARR) services for pull requests, to improve the collaborative coding review process. However, the user experience of ARR is under-investigated. In this paper, we use a two-stage mixed-methods approach to study how software developers perceive and work with the Facebook mention bot, one of the most popular ARR bots in GitHub. Specifically, in Stage I, we conduct archival analysis on projects employing mention bot and a user survey to investigate the bot's performance. A year later, in Stage II, we revisit these projects and conduct additional surveys and interviews with three user groups: project owners, contributors and reviewers. Results show that developers appreciate mention bot saving their effort, but are bothered by its unstable setting and unbalanced workload allocation. We conclude with design considerations for improving ARR services.},
booktitle = {Proceedings of the Sixth International Symposium of Chinese CHI},
pages = {152–155},
numpages = {4},
keywords = {Automatic reviewer recommendation services, user experience, software development platform, mixed-methods},
location = {Montreal, QC, Canada},
series = {ChineseCHI '18}
}

@inproceedings{10.5555/3155562.3155698,
author = {Li, Yi},
title = {Managing Software Evolution through Semantic History Slicing},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history instances obtained from real-world open source software projects on GitHub. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {1014–1017},
numpages = {4},
keywords = {software changes, program analysis, version histories, software reuse},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.5555/3155562.3155577,
author = {Mirhosseini, Samim and Parnin, Chris},
title = {Can Automated Pull Requests Encourage Software Developers to Upgrade Out-of-Date Dependencies?},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Developers neglect to update legacy software dependencies, resulting in buggy and insecure software. One explanation for this neglect is the difficulty of constantly checking for the availability of new software updates, verifying their safety, and addressing any migration efforts needed when upgrading a dependency. Emerging tools attempt to address this problem by introducing automated pull requests and project badges to inform the developer of stale dependencies. To understand whether these tools actually help developers, we analyzed 7,470 GitHub projects that used these notification mechanisms to identify any change in upgrade behavior. Our results find that, on average, projects that use pull request notifications upgraded 1.6x as often as projects that did not use any tools. Badge notifications were slightly less effective: users upgraded 1.4x more frequently. Unfortunately, although pull request notifications are useful, developers are often overwhelmed by notifications: only a third of pull requests were actually merged. Through a survey, 62 developers indicated that their most significant concerns are breaking changes, understanding the implications of changes, and migration effort. The implications of our work suggests ways in which notifications can be improved to better align with developers' expectations and the need for new mechanisms to reduce notification fatigue and improve confidence in automated pull requests. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {84–94},
numpages = {11},
keywords = {notification fatigue, software dependencies, automated pull requests},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3377811.3380395,
author = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
title = {Taxonomy of Real Faults in Deep Learning Systems},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380395},
doi = {10.1145/3377811.3380395},
abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1110–1121},
numpages = {12},
keywords = {deep learning, taxonomy, real faults, software testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/HICSS.2014.405,
author = {Squire, Megan},
title = {Forge++: The Changing Landscape of FLOSS Development},
year = {2014},
isbn = {9781479925049},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/HICSS.2014.405},
doi = {10.1109/HICSS.2014.405},
abstract = {Software forges are centralized online systems that provide useful tools to help distributed development teams work together, especially in free, libre, and open source software (FLOSS). Forge-provided tools may include web space, version control systems, mailing lists and communication forums, bug tracking systems, file downloads, wikis, and the like. Empirical software engineering researchers can mine the artifacts from these tools to better understand how FLOSS is made. As the landscape of distributed software development has grown and changed, the tools needed to make FLOSS have changed as well. There are three newer tools at the center of FLOSS development today: distributed version control based forges (like Github), programmer question-and-answer communities (like Stack Overflow), and paste bin tools (like Gist or Pastebin.com). These tools are extending and changing the toolset used for FLOSS development, and redefining what a software forge looks like. The main contributions of this paper are to describe each of these tools, to identify the data and artifacts available for mining from these tools, and to outline some of the ways researchers can use these artifacts to continue to understand how FLOSS is made.},
booktitle = {Proceedings of the 2014 47th Hawaii International Conference on System Sciences},
pages = {3266–3275},
numpages = {10},
keywords = {forges, repositories, software development, open source software, stack overflow, pastebin, github},
series = {HICSS '14}
}

@inproceedings{10.1145/3308558.3313700,
author = {Kanakia, Anshul and Shen, Zhihong and Eide, Darrin and Wang, Kuansan},
title = {A Scalable Hybrid Research Paper Recommender System for Microsoft Academic},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313700},
doi = {10.1145/3308558.3313700},
abstract = {We present the design and methodology for the large scale hybrid paper recommender system used by Microsoft Academic. The system provides recommendations for approximately 160 million English research papers and patents. Our approach handles incomplete citation information while also alleviating the cold-start problem that often affects other recommender systems. We use the Microsoft Academic Graph (MAG), titles, and available abstracts of research papers to build a recommendation list for all documents, thereby combining co-citation and content based approaches. Tuning system parameters also allows for blending and prioritization of each approach which, in turn, allows us to balance paper novelty versus authority in recommendation results. We evaluate the generated recommendations via a user study of 40 participants, with over 2400 recommendation pairs graded and discuss the quality of the results using P@10 and nDCG scores. We see that there is a strong correlation between participant scores and the similarity rankings produced by our system but that additional focus needs to be put towards improving recommender precision, particularly for content based recommendations. The results of the user survey and associated analysis scripts are made available via GitHub and the recommendations produced by our system are available as part of the MAG on Azure to facilitate further research and light up novel research paper recommendation applications.},
booktitle = {The World Wide Web Conference},
pages = {2893–2899},
numpages = {7},
keywords = {document collection, big data, k-means, clustering, word embedding, recommender system},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1145/3397537.3397559,
author = {Ehmueller, Jan and Riese, Alexander and Tjabben, Hendrik and Niephaus, Fabio and Hirschfeld, Robert},
title = {Polyglot Code Finder},
year = {2020},
isbn = {9781450375078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397537.3397559},
doi = {10.1145/3397537.3397559},
abstract = {With the increasing complexity of software, it becomes even more important to build on the work of others. At the same time, websites, such as Stack Overflow or GitHub, are used by millions of developers to host their code, which could potentially be reused.  The process of finding the right code, however, is often time-consuming. In addition, the right solution may be written in a programming language that does not fit the developer's requirements. Current approaches to automate code search allow users to search for code based on keywords and transformation rules, but they are limited to one programming language.  Our approach enables developers to find code for reuse written in different languages, which is especially useful when building polyglot applications. In addition to conventional search filters, users can filter code by providing example input and expected output. Based on our approach, we have implemented a tool prototype in GraalSqueak. We evaluate both approach and prototype with an experience report.},
booktitle = {Conference Companion of the 4th International Conference on Art, Science, and Engineering of Programming},
pages = {106–112},
numpages = {7},
keywords = {GraalVM, polyglot, programming experience, code reuse, code search},
location = {Porto, Portugal},
series = {20}
}

@inproceedings{10.1145/3295453.3295457,
author = {Murillo, Andr\'{e}s Felipe and C\'{o}mbita, Luis Francisco and Gonzalez, Andrea Calder\'{o}n and Rueda, Sandra and Cardenas, Alvaro A. and Quijano, Nicanor},
title = {A Virtual Environment for Industrial Control Systems: A Nonlinear Use-Case in Attack Detection, Identification, and Response},
year = {2018},
isbn = {9781450362207},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295453.3295457},
doi = {10.1145/3295453.3295457},
abstract = {The integration of modern information technologies with industrial control systems has created an enormous interest in the security of industrial control, however, given the cost, variety, and industry practices, it is hard for researchers to test and deploy security solutions in real-world systems. Industrial control testbeds can be used as tools to test security solutions before they are deployed, and in this paper we extend our previous work to develop open-source virtual industrial control testbeds where computing and networking components are emulated and virtualized, and the physical system is simulated through differential equations. In particular, we implement a nonlinear control system emulating a three-water tank with the associated sensors, PLCs, and actuators that communicate through an emulated network. In addition, we design unknown input observers (UIO) to not only detect that an attack is occurring, but also to identify the source of the malicious false data injections and mitigate its impact. Our system is available through Github to the academic community.},
booktitle = {Proceedings of the 4th Annual Industrial Control System Security Workshop},
pages = {25–32},
numpages = {8},
keywords = {Virtual Environment Testbeds, Industrial Control Systems, Network Function Virtualization, Network Security},
location = {San Juan, PR, USA},
series = {ICSS '18}
}

@inproceedings{10.1145/3379597.3387483,
author = {Wu, Yiwen and Zhang, Yang and Wang, Tao and Wang, Huaimin},
title = {An Empirical Study of Build Failures in the Docker Context},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387483},
doi = {10.1145/3379597.3387483},
abstract = {Docker containers have become the de-facto industry standard. Docker builds often break, and a large amount of efforts are put into troubleshooting broken builds. Prior studies have evaluated the rate at which builds in large organizations fail. However, little is known about the frequency and fix effort of failures that occur in Docker builds of open-source projects. This paper provides a first attempt to present a preliminary study on 857,086 Docker builds from 3,828 open-source projects hosted on GitHub. Using the Docker build data, we measure the frequency of broken builds and report their fix time. Furthermore, we explore the evolution of Docker build failures across time. Our findings help to characterize and understand Docker build failures and motivate the need for collecting more empirical evidence.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {76–80},
numpages = {5},
keywords = {Open-source, Build failure, Docker},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3196321.3196334,
author = {Hu, Xing and Li, Ge and Xia, Xin and Lo, David and Jin, Zhi},
title = {Deep Code Comment Generation},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196334},
doi = {10.1145/3196321.3196334},
abstract = {During software maintenance, code comments help developers comprehend programs and reduce additional time spent on reading and navigating source code. Unfortunately, these comments are often mismatched, missing or outdated in the software projects. Developers have to infer the functionality from the source code. This paper proposes a new approach named DeepCom to automatically generate code comments for Java methods. The generated comments aim to help developers understand the functionality of Java methods. DeepCom applies Natural Language Processing (NLP) techniques to learn from a large code corpus and generates comments from learned features. We use a deep neural network that analyzes structural information of Java methods for better comments generation. We conduct experiments on a large-scale Java corpus built from 9,714 open source projects from GitHub. We evaluate the experimental results on a machine translation metric. Experimental results demonstrate that our method DeepCom outperforms the state-of-the-art by a substantial margin.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {200–210},
numpages = {11},
keywords = {comment generation, deep learning, program comprehension},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1109/ICPC.2017.36,
author = {Lin, Bin and Ponzanelli, Luca and Mocci, Andrea and Bavota, Gabriele and Lanza, Michele},
title = {On the Uniqueness of Code Redundancies},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.36},
doi = {10.1109/ICPC.2017.36},
abstract = {Code redundancy widely occurs in software projects. Researchers have investigated the existence, causes, and impacts of code redundancy, showing that it can be put to good use, for example in the context of code completion. When analyzing source code redundancy, previous studies considered software projects as sequences of tokens, neglecting the role of the syntactic structures enforced by programming languages. However, differences in the redundancy of such structures may jeopardize the performance of applications leveraging code redundancy.We present a study of the redundancy of several types of code constructs in a large-scale dataset of active Java projects mined from GitHub, unveiling that redundancy is not uniform and mainly resides in specific code constructs. We further investigate the implications of the locality of redundancy by analyzing the performance of language models when applied to code completion. Our study discloses the perils of exploiting code redundancy without taking into account its strong locality in specific code constructs.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {121–131},
numpages = {11},
keywords = {code redundancy, empirical study, code completion},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/3377811.3380410,
author = {Overney, Cassandra and Meinicke, Jens and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {How to Not Get Rich: An Empirical Study of Donations in Open Source},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380410},
doi = {10.1145/3377811.3380410},
abstract = {Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1209–1221},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2901739.2903498,
author = {D\'{e}sarmeaux, Casimir and Pecatikov, Andrea and McIntosh, Shane},
title = {The Dispersion of Build Maintenance Activity across Maven Lifecycle Phases},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903498},
doi = {10.1145/2901739.2903498},
abstract = {Build systems describe how source code is translated into deliverables. Developers use build management tools like Maven to specify their build systems. Past work has shown that while Maven provides invaluable features (e.g., incremental building), it introduces an overhead on software development. Indeed, Maven build systems require maintenance. However, Maven build systems follow the build lifecycle, which is comprised of validate, compile, test, packaging, install, and deploy phases. Little is known about how build maintenance activity is dispersed among these lifecycle phases. To bridge this gap, in this paper, we analyze the dispersion of build maintenance activity across build lifecycle phases. Through analysis of 1,181 GitHub repositories that use Maven, we find that: (1) the compile phase accounts for 24% more of the build maintenance activity than the other phases; and (2) while the compile phase generates a consistent amount of maintenance activity over time, the other phases tend to generate peaks and valleys of maintenance activity. Software teams that use Maven should plan for these shifts in the characteristics of build maintenance activity.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {492–495},
numpages = {4},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3041021.3053052,
author = {Wang, Jingbo and Aryani, Amir and Wyborn, Lesley and Evans, Ben},
title = {Providing Research Graph Data in JSON-LD Using Schema.Org},
year = {2017},
isbn = {9781450349147},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3041021.3053052},
doi = {10.1145/3041021.3053052},
abstract = {In this position paper, we describe a pilot project that provides Research Graph records to external web services using JSON-LD. The Research Graph database contains a large-scale graph that links research datasets (i.e., data used to support research) to funding records (i.e. grants), publications and researcher records such as ORCID profiles. This database was derived from the work of the Research Data Alliance Working Group on Data Description Registry Interoperability (DDRI), and curated using the Research Data Switchboard open source software. By being available in Linked Data format, the Research Graph database is more accessible to third-party web services over the Internet, which thus opens the opportunity to connect to the rest of the world in the semantic format.The primary purpose of this pilot project is to evaluate the feasibility of converting registry objects in Research Graph to JSON-LD by accessing widely used vocabularies published at Schema.org. In this paper, we provide examples of publications, datasets and grants from international research institutions such as CERN INSPIREHEP, National Computational Infrastructure (NCI) in Australia, and Australian Research Council (ARC). Furthermore, we show how these Research Graph records are made semantically available as Linked Data through using Schema.org. The mapping between Research Graph schema and Schema.org is available on GitHub repository. We also discuss the potential need for an extension to Schema.org vocabulary for scholarly communication.},
booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
pages = {1213–1218},
numpages = {6},
keywords = {semantic web, schema.org, json-ld, linked data},
location = {Perth, Australia},
series = {WWW '17 Companion}
}

@inproceedings{10.1145/3387905.3388597,
author = {Rahkema, Kristiina and Pfahl, Dietmar},
title = {Empirical Study on Code Smells in IOS Applications},
year = {2020},
isbn = {9781450379595},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387905.3388597},
doi = {10.1145/3387905.3388597},
abstract = {Code smells are recurring patterns in code that have been identified as bad practices. They have been analysed extensively in Java desktop applications. For mobile applications most of the research has been done for Android with very little research done for iOS. Although Android has the largest market share, iOS is a very popular platform. Our goal is to understand the distribution of code smells in iOS applications. For this analysis we used a collaborative list of open source iOS applications from GitHub. We combined code smells defined by Fowler and object oriented code smells studied on Android. We developed a tool that can detect these code smells in Swift applications. We discovered that iOS applications are most often affected by Lazy Class, Long Method and Message Chain code smells. Most often occurring code smells are Internal Duplication, Lazy Class and Long Method.},
booktitle = {Proceedings of the IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems},
pages = {61–65},
numpages = {5},
keywords = {mobile applications, code smells, iOS, empirical study},
location = {Seoul, Republic of Korea},
series = {MOBILESoft '20}
}

@inproceedings{10.1145/3183713.3193539,
author = {He, Yeye and Ganjam, Kris and Lee, Kukjin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit and Chu, Xu and Zheng, Yudian},
title = {Transform-Data-by-Example (TDE): Extensible Data Transformation in Excel},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3193539},
doi = {10.1145/3183713.3193539},
abstract = {Business analysts and data scientists today increasingly need to clean, standardize and transform diverse data sets, such as name, address, date time, phone number, etc., before they can perform analysis. These ad-hoc transformation problems are typically solved by one-off scripts, which is both difficult and time-consuming.Our observation is that these domain-specific transformation problems have long been solved by developers with code libraries, which are often shared in places like GitHub. We thus develop an extensible data transformation system called Transform-Data-by-Example (TDE) that can leverage rich transformation logic in source code, DLLs, web services and mapping tables, so that end-users only need to provide a few (typically 3) input/output examples, and TDE can synthesize desired programs using relevant transformation logic from these sources. The beta version of TDE was released in Office Store for Excel.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {1785–1788},
numpages = {4},
keywords = {data cleaning, etl, data preparation, data integration},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3377811.3380378,
author = {Islam, Md Johirul and Pan, Rangeet and Nguyen, Giang and Rajan, Hridesh},
title = {Repairing Deep Neural Networks: Fix Patterns and Challenges},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380378},
doi = {10.1145/3377811.3380378},
abstract = {Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1135–1146},
numpages = {12},
keywords = {bugs, bug fix, deep neural networks, bug fix patterns},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3131151.3131163,
author = {Wessel, Mairieli Santos and Aniche, Maur\'{\i}cio Finavaro and Oliva, Gustavo Ansaldi and Gerosa, Marco Aur\'{e}lio and Wiese, Igor Scaliante},
title = {Tweaking Association Rules to Optimize Software Change Recommendations},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131163},
doi = {10.1145/3131151.3131163},
abstract = {Past researchs have been trying to recommend artifacts that are likely to change together in a task to assist developers in making changes to a software system, often using techniques like association rules. Association rules learning is a data mining technique that has been frequently used to discover evolutionary couplings. These couplings constitute a fundamental piece of modern change prediction techniques. However, using association rules to detect evolutionary coupling requires a number of configuration parameters, such as measures of interest (e.g. support and confidence), their cut-off values, and the portion of the commit history from which co-change relationships will be extracted. To accomplish this set up, researchers have to carry out empirical studies for each project, testing a few variations of the parameters before choosing a configuration. This makes it difficult to use association rules in practice, since developers would need to perform experiments before applying the technique and would end up choosing non-optimal solutions that lead to wrong predictions. In this paper, we propose a fitness function for a Genetic Algorithm that optimizes the co-change recommendations and evaluate it on five open source projects (CPython, Django, Laravel, Shiny and Gson). The results indicate that our genetic algorithm is able to find optimized cut-off values for support and confidence, as well as to determine which length of commit history yields the best recommendations. We also find that, for projects with less commit history (5k commits), our approach produced better results than the regression function proposed in the literature. This result is particularly encouraging, because repositories such as GitHub host many young projects. Our results can be used by researchers when conducting co-change prediction studies and by tool developers to produce automated support to be used by practitioners.},
booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
pages = {94–103},
numpages = {10},
keywords = {Genetic Algorithm, Change Recommendation, Association Rules},
location = {Fortaleza, CE, Brazil},
series = {SBES'17}
}

@inproceedings{10.1145/3186411.3186418,
author = {Rigger, Manuel and Marr, Stefan and Kell, Stephen and Leopoldseder, David and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {An Analysis of X86-64 Inline Assembly in C Programs},
year = {2018},
isbn = {9781450355797},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3186411.3186418},
doi = {10.1145/3186411.3186418},
abstract = {C codebases frequently embed nonportable and unstandardized elements such as inline assembly code. Such elements are not well understood, which poses a problem to tool developers who aspire to support C code. This paper investigates the use of x86-64 inline assembly in 1264 C projects from GitHub and combines qualitative and quantitative analyses to answer questions that tool authors may have. We found that 28.1% of the most popular projects contain inline assembly code, although the majority contain only a few fragments with just one or two instructions. The most popular instructions constitute a small subset concerned largely with multicore semantics, performance optimization, and hardware control. Our findings are intended to help developers of C-focused tools, those testing compilers, and language designers seeking to reduce the reliance on inline assembly. They may also aid the design of tools focused on inline assembly itself.},
booktitle = {Proceedings of the 14th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
pages = {84–99},
numpages = {16},
keywords = {Empirical Survey, GitHub, C, Inline Assembly},
location = {Williamsburg, VA, USA},
series = {VEE '18}
}

@article{10.1145/3296975.3186418,
author = {Rigger, Manuel and Marr, Stefan and Kell, Stephen and Leopoldseder, David and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {An Analysis of X86-64 Inline Assembly in C Programs},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296975.3186418},
doi = {10.1145/3296975.3186418},
abstract = {C codebases frequently embed nonportable and unstandardized elements such as inline assembly code. Such elements are not well understood, which poses a problem to tool developers who aspire to support C code. This paper investigates the use of x86-64 inline assembly in 1264 C projects from GitHub and combines qualitative and quantitative analyses to answer questions that tool authors may have. We found that 28.1% of the most popular projects contain inline assembly code, although the majority contain only a few fragments with just one or two instructions. The most popular instructions constitute a small subset concerned largely with multicore semantics, performance optimization, and hardware control. Our findings are intended to help developers of C-focused tools, those testing compilers, and language designers seeking to reduce the reliance on inline assembly. They may also aid the design of tools focused on inline assembly itself.},
journal = {SIGPLAN Not.},
month = mar,
pages = {84–99},
numpages = {16},
keywords = {GitHub, Inline Assembly, C, Empirical Survey}
}

@inproceedings{10.1145/3236024.3275527,
author = {Celik, Ahmet and Lee, Young Chul and Gligoric, Milos},
title = {Regression Test Selection for TizenRT},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275527},
doi = {10.1145/3236024.3275527},
abstract = {Regression testing - running tests after code modifications - is widely practiced in industry, including at Samsung. Regression Test Selection (RTS) optimizes regression testing by skipping tests that are not affected by recent code changes. Recent work has developed robust RTS tools, which mostly target managed languages, e.g., Java and C#, and thus are not applicable to large C projects, e.g., TizenRT, a lightweight RTOS-based platform.  We present Selfection, an RTS tool for projects written in C; we discuss the key challenges to develop Selfection and our design decisions. Selfection uses the objdump and readelf tools to statically build a dependency graph of functions from binaries and detect modified code elements. We integrated Selfection in TizenRT and evaluated its benefits if tests are run in an emulator and on a supported hardware platform (ARTIK 053). We used the latest 150 revisions of TizenRT available on GitHub. We measured the benefits of Selfection as the reduction in the number of tests and reduction in test execution time over running all tests at each revision (i.e., RetestAll). Our results show that Selfection can reduce, on average, the number of tests to 4.95% and end-to-end execution time to 7.04% when tests are executed in the emulator, and to 5.74% and 26.82% when tests are executed on the actual hardware. Our results also show that the time taken to maintain the dependency graph and detect modified functions is negligible.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {845–850},
numpages = {6},
keywords = {TizenRT, Regression test selection, static dependency analysis},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ICSE.2019.00089,
author = {Nguyen, Hoan Anh and Nguyen, Tien N. and Dig, Danny and Nguyen, Son and Tran, Hieu and Hilton, Michael},
title = {Graph-Based Mining of in-the-Wild, Fine-Grained, Semantic Code Change Patterns},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00089},
doi = {10.1109/ICSE.2019.00089},
abstract = {Prior research exploited the repetitiveness of code changes to enable several tasks such as code completion, bug-fix recommendation, library adaption, etc. These and other novel applications require accurate detection of semantic changes, but the state-of-the-art methods are limited to algorithms that detect specific kinds of changes at the syntactic level. Existing algorithms relying on syntactic similarity have lower accuracy, and cannot effectively detect semantic change patterns. We introduce a novel graph-based mining approach, CPatMiner, to detect previously unknown repetitive changes in the wild, by mining fine-grained semantic code change patterns from a large number of repositories. To overcome unique challenges such as detecting meaningful change patterns and scaling to large repositories, we rely on fine-grained change graphs to capture program dependencies.We evaluate CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub across a population of 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to 108 open-source developers. We found that 70% of respondents recognized those patterns as their meaningful frequent changes. Moreover, 79% of respondents even named the patterns, and 44% wanted future IDEs to automate such repetitive changes. We found that the mined change patterns belong to various development activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%, including refactorings). Second, we compared our tool with the state-of-the-art, AST-based technique, and reported that it detects 2.1x more meaningful patterns. Third, we use CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K instances of change patterns from which we provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the change patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns are not intermittent but spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {819–830},
numpages = {12},
keywords = {graph mining, semantic change pattern mining},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/2791060.2791083,
author = {Montalvillo, Leticia and D\'{\i}az, Oscar},
title = {Tuning GitHub for SPL Development: Branching Models &amp; Repository Operations for Product Engineers},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791083},
doi = {10.1145/2791060.2791083},
abstract = {SPLs distinguish between domain engineering (DE) and application engineering (AE). Though each realm has its own lifecycle, they might need to be regularly synchronized to avoid SPL erosion during evolution. This introduces two sync paths: update propagation (from DE to AE) and feedback propagation (from AE to DE). This work looks at how to support sync paths in Version Control Systems (VCSs) using traditional VCS constructs (i.e. merge, branch, fork and pull). In this way, synchronization mismatches can be resolved \`{a} la VCS, i.e. highlighting difference between distinct versions of the same artifact. However, this results in a conceptual gap between how propagations are conceived (i.e. update, feedback) and how propagation are realized (i.e. merge, branch, etc). To close this gap, we propose to enhance existing VCSs with SPL sync paths as first-class operations. As a proof-of-concept, we use Web Augmentation techniques to extend GitHub's Web pages with this extra functionality. Through a single click, product engineers can now (1) generate product repositories, (2) update propagating newer feature versions, or (3), feedback propagating product customizations amenable to be upgraded as core assets.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {111–120},
numpages = {10},
keywords = {SPL evolution, change propagation, VCS, branching model},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1007/978-3-319-09195-2_5,
author = {L\"{a}mmel, Ralf and Varanovich, Andrei},
title = {Interpretation of Linguistic Architecture},
year = {2014},
isbn = {9783319091945},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-09195-2_5},
doi = {10.1007/978-3-319-09195-2_5},
abstract = {The megamodeling language MegaL is designed to model the linguistic architecture of software systems: the relationships between software artifacts (e.g., files), software languages (e.g., programming languages), and software technologies (e.g., code generators) used in a system. The present paper delivers a form of interpretation for such megamodels: resolution of megamodel elements to resources (e.g., system artifacts) and evaluation of relationships, subject to designated programs (such as pluggable 'tools' for checking). Interpretation reduces concerns about the adequacy and meaning of megamodels, as it helps to apply the megamodels to actual systems. We leverage Linked Data principles for surfacing resolved megamodels by linking, for example, artifacts to GitHub repositories or concepts to DBpedia resources. We provide an executable specification (i.e., semantics) of interpreted megamodels and we discuss an implementation in terms of an object-oriented framework with dynamically loaded plugins.},
booktitle = {Proceedings of the 10th European Conference on Modelling Foundations and Applications - Volume 8569},
pages = {67–82},
numpages = {16},
keywords = {Linked Data, megamodel, technological space, software technology, interpretation, ontology, software language}
}

@inproceedings{10.1145/3238147.3240485,
author = {Sung, Chungha and Paulsen, Brandon and Wang, Chao},
title = {CANAL: A Cache Timing Analysis Framework via LLVM Transformation},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240485},
doi = {10.1145/3238147.3240485},
abstract = {A unified modeling framework for non-functional properties of a program is essential for research in software analysis and verification, since it reduces burdens on individual researchers to implement new approaches and compare existing approaches. We present CANAL, a framework that models the cache behaviors of a program by transforming its intermediate representation in the LLVM compiler. CANAL inserts auxiliary variables and instructions over these variables, to allow standard verification tools to handle a new class of cache related properties, e.g., for computing the worst-case execution time and detecting side-channel leaks. We demonstrate the effectiveness of using three verification tools: KLEE, SMACK and Crab-llvm. We confirm the accuracy of our cache model by comparing with CPU cycle-accurate simulation results of GEM5. CANAL is available on GitHub(https://github.com/canalcache/canal) and YouTube(https://youtu.be/JDou3F1j2nY).},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {904–907},
numpages = {4},
keywords = {bounded model checking, execution time, abstract interpretation, verification, cache, side channel, symbolic execution},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.5555/2487085.2487131,
author = {Wagstrom, Patrick and Jergensen, Corey and Sarma, Anita},
title = {A Network of Rails: A Graph Dataset of Ruby on Rails and Associated Projects},
year = {2013},
isbn = {9781467329361},
publisher = {IEEE Press},
abstract = { Software projects, whether open source, proprietary, or a combination thereof, rarely exist in isolation. Rather, most projects build on a network of people and ideas from dozens, hundreds, or even thousands of other projects. Using the GitHub APIs it is possible to extract these relationships for millions of users and projects. In this paper we present a dataset of a large network of open source projects centered around Ruby on Rails. This dataset provides insight into the relationships between Ruby on Rails and an ecosystem involving 1116 projects. To facilitate understanding of this data in the context of relationships between projects, users, and their activities, it is provided as a graph database suitable for assessing network properties of the community and individuals within those communities and can be found at https://github.com/pridkett/gitminer-data-rails. },
booktitle = {Proceedings of the 10th Working Conference on Mining Software Repositories},
pages = {229–232},
numpages = {4},
location = {San Francisco, CA, USA},
series = {MSR '13}
}

@inproceedings{10.1145/3292500.3330699,
author = {Svyatkovskiy, Alexey and Zhao, Ying and Fu, Shengyu and Sundaresan, Neel},
title = {Pythia: AI-Assisted Code Completion System},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330699},
doi = {10.1145/3292500.3330699},
abstract = {In this paper, we propose a novel end-to-end approach for AI-assisted code completion called Pythia. It generates ranked lists of method and API recommendations which can be used by software developers at edit time. The system is currently deployed as part of Intellicode extension in Visual Studio Code IDE. Pythia exploits state-of-the-art large-scale deep learning models trained on code contexts extracted from abstract syntax trees. It is designed to work at a high throughput predicting the best matching code completions on the order of 100 ms. We describe the architecture of the system, perform comparisons to frequency-based approach and invocation-based Markov Chain language model, and discuss challenges serving Pythia models on lightweight client devices. The offline evaluation results obtained on 2700 Python open source software GitHub repositories show a top-5 accuracy of 92%, surpassing the baseline models by 20% averaged over classes, for both intra and cross-project settings.},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2727–2735},
numpages = {9},
keywords = {naturalness of software, neural networks, code completion},
location = {Anchorage, AK, USA},
series = {KDD '19}
}

@inproceedings{10.1145/3359591.3359735,
author = {Allamanis, Miltiadis},
title = {The Adverse Effects of Code Duplication in Machine Learning Models of Code},
year = {2019},
isbn = {9781450369954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359591.3359735},
doi = {10.1145/3359591.3359735},
abstract = {The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research.},
booktitle = {Proceedings of the 2019 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software},
pages = {143–153},
numpages = {11},
keywords = {big code, machine learning, code naturalness, dataset collection, duplication},
location = {Athens, Greece},
series = {Onward! 2019}
}

@inproceedings{10.1145/3338906.3342505,
author = {Kr\"{u}ger, Jacob},
title = {Tackling Knowledge Needs during Software Evolution},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3342505},
doi = {10.1145/3338906.3342505},
abstract = {Developers use a large amount of their time to understand the system they work on, an activity referred to as program comprehension. Especially software evolution and forgetting over time lead to developers becoming unfamiliar with a system. To support them during program comprehension, we can employ knowledge recovery to reverse engineer implicit information from the system and the platform (e.g., GitHub) it is hosted on. However, to recover useful knowledge and to provide it in a useful way, we first need to understand what knowledge developers forget to what extent, what sources are reliable to recover knowledge, and how to trace knowledge to the features in a system. We tackle these three issues, aiming to provide empirical insights and tooling to support developers during software evolution and maintenance. The results help practitioners, as we support the analysis and understanding of systems, as well as researchers, showing opportunities to automate, for example, reverse-engineering techniques.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1244–1246},
numpages = {3},
keywords = {Feature traceability, Memory, Software evolution, Program comprehension, Software maintenance, Forgetting},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3196398.3196405,
author = {Sanchez, Beatriz A. and Barmpis, Konstantinos and Neubauer, Patrick and Paige, Richard F. and Kolovos, Dimitrios S.},
title = {Restmule: Enabling Resilient Clients for Remote APIs},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196405},
doi = {10.1145/3196398.3196405},
abstract = {Mining data from remote repositories, such as GitHub and StackExchange, involves the execution of requests that can easily reach the limitations imposed by the respective APIs to shield their services from overload and abuse. Therefore, data mining clients are left alone to deal with such protective service policies which usually involves an extensive amount of manual implementation effort. In this work we present RestMule, a framework for handling various service policies, such as limited number of requests within a period of time and multi-page responses, by generating resilient clients that are able to handle request rate limits, network failures, response caching, and paging in a graceful and transparent manner. As a result, RestMule clients generated from OpenAPI specifications (i.e. standardized REST API descriptors), are suitable for intensive data-fetching scenarios. We evaluate our framework by reproducing an existing repository mining use case and comparing the results produced by employing a popular hand-written client and a RestMule client.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {537–541},
numpages = {5},
keywords = {HTTP API clients, resilience, openAPI specification},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1109/ISDEA.2012.223,
author = {Ben, Xu and Beijun, Shen and Weicheng, Yang},
title = {Mining Developer Contribution in Open Source Software Using Visualization Techniques},
year = {2013},
isbn = {9780769549231},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ISDEA.2012.223},
doi = {10.1109/ISDEA.2012.223},
abstract = {The research of developers' contribution is an important part of the software evolution area. It allows project owners to find potential long-term contributors earlier and helps the newcomers to improve their behaviors. In this paper, we examined the contribution characteristics of developers in open source environment based on visual analysis, and presented approaches from three aspects-influencing factors, time characteristics and region characteristics. Our analysis used data from github and revealed some regular patterns. We found that the code which newcomers started to contribute with more people engaged in would lead to less contribution in some degree. We also found that there's a relation between developers' early and later period contribution. In addition, developers from different regions were more likely to have dominant relationship. Our findings may provide some support for future research in the area of software evolution.},
booktitle = {Proceedings of the 2013 Third International Conference on Intelligent System Design and Engineering Applications},
pages = {934–937},
numpages = {4},
keywords = {software evolution, visual analysis, contribution characteristics, open source software},
series = {ISDEA '13}
}

@inproceedings{10.5555/3408207.3408219,
author = {Nobel, Parth},
title = {Auto_diff: An Automatic Differentiation Package for Python},
year = {2020},
isbn = {9781713812883},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {We present auto_diff, a package that performs automatic differentiation of numerical Python code. auto_diff overrides Python's NumPy package's functions, augmenting them with seamless automatic differentiation capabilities. Notably, auto_diff is non-intrusive, i.e., the code to be differentiated does not require auto_diff-specific alterations. We illustrate auto_diff on electronic devices, a circuit simulation, and a mechanical system simulation. In our evaluations so far, we found that running simulations with auto_diff takes less than 4 times as long as simulations with hand-written differentiation code. We believe that auto_diff, which was written after attempts to use existing automatic differentiation packages on our applications ran into difficulties, caters to an important need within the numerical Python community. We have attempted to write this paper in a tutorial style to make it accessible to those without prior background in automatic differentiation techniques and packages. We have released auto_diff as open source on GitHub.},
booktitle = {Proceedings of the 2020 Spring Simulation Conference},
articleno = {10},
numpages = {12},
keywords = {automatic differentiation, numerical methods, implementation, Python, library},
location = {Fairfax, Virginia},
series = {SpringSim '20}
}

@inproceedings{10.1145/3372297.3420015,
author = {Vu, Duc Ly and Pashchenko, Ivan and Massacci, Fabio and Plate, Henrik and Sabetta, Antonino},
title = {Towards Using Source Code Repositories to Identify Software Supply Chain Attacks},
year = {2020},
isbn = {9781450370899},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372297.3420015},
doi = {10.1145/3372297.3420015},
abstract = {Increasing popularity of third-party package repositories, like NPM, PyPI, or RubyGems, makes them an attractive target for software supply chain attacks. By injecting malicious code into legitimate packages, attackers were known to gain more than 100,000 downloads of compromised packages. Current approaches for identifying malicious payloads are resource demanding. Therefore, they might not be applicable for the on-the-fly detection of suspicious artifacts being uploaded to the package repository. In this respect, we propose to use source code repositories (e.g., those in Github) for detecting injections into the distributed artifacts of a package. Our preliminary evaluation demonstrates that the proposed approach captures known attacks when malicious code was injected into PyPI packages. The analysis of the 2666 software artifacts (from all versions of the top ten most downloaded Python packages in PyPI) suggests that the technique is suitable for lightweight analysis of real-world packages.},
booktitle = {Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2093–2095},
numpages = {3},
keywords = {software supply chain attacks, source code repositories, lightweight analysis, package repositories, code injection},
location = {Virtual Event, USA},
series = {CCS '20}
}

@inproceedings{10.5555/2820518.2820539,
author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Bacchelli, Alberto},
title = {Will They like This? Evaluating Code Contributions with Language Models},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected changesets do contain code significantly less similar to the project than accepted ones; furthermore, the less similar changesets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {157–167},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/2462932.2462950,
author = {West, Andrew G. and Lee, Insup},
title = {Towards Content-Driven Reputation for Collaborative Code Repositories},
year = {2012},
isbn = {9781450316057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462932.2462950},
doi = {10.1145/2462932.2462950},
abstract = {As evidenced by SourceForge and GitHub, code repositories now integrate Web 2.0 functionality that enables global participation with minimal barriers-to-entry. To prevent detrimental contributions enabled by crowdsourcing, reputation is one proposed solution. Fortunately this is an issue that has been addressed in analogous version control systems such as the wiki for natural language content. The WikiTrust algorithm ("content-driven reputation"), while developed and evaluated in wiki environments operates under a possibly shared collaborative assumption: actions that "survive" subsequent edits are reflective of good authorship.In this paper we examine WikiTrust's ability to measure author quality in collaborative code development. We first define a mapping from repositories to wiki environments and use it to evaluate a production SVN repository with 92,000 updates. Analysis is particularly attentive to reputation loss events and attempts to establish ground truth using commit comments and bug tracking. A proof-of-concept evaluation suggests the technique is promising (about two-thirds of reputation loss is justified) with false positives identifying areas for future refinement. Equally as important, these false positives exemplify differences in content evolution and the cooperative process between wikis and code repositories.},
booktitle = {Proceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration},
articleno = {13},
numpages = {4},
keywords = {SVN, reputation, WikiTrust, wikis, code repository, trust management, code quality, content persistence},
location = {Linz, Austria},
series = {WikiSym '12}
}

@inproceedings{10.1145/3361242.3362774,
author = {Liu, Bohong and Wang, Tao and Zhang, Xunhui and Fan, Qiang and Yin, Gang and Deng, Jinsheng},
title = {A Neural-Network Based Code Summarization Approach by Using Source Code and Its Call Dependencies},
year = {2019},
isbn = {9781450377010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361242.3362774},
doi = {10.1145/3361242.3362774},
abstract = {Code summarization aims at generating natural language abstraction for source code, and it can be of great help for program comprehension and software maintenance. The current code summarization approaches have made progress with neural-network. However, most of these methods focus on learning the semantic and syntax of source code snippets, ignoring the dependency of codes. In this paper, we propose a novel method based on neural-network model using the knowledge of the call dependency between source code and its related codes. We extract call dependencies from the source code, transform it as a token sequence of method names, and leverage the Seq2Seq model for code summarization using the combination of source code and call dependency information. About 100,000 code data is collected from 1,000 open source Java proejects on github for experiment. The large-scale code experiment shows that by considering not only the code itself but also the codes it called, the code summarization model can be improved with the BLEU score to 33.08.},
booktitle = {Proceedings of the 11th Asia-Pacific Symposium on Internetware},
articleno = {12},
numpages = {10},
keywords = {Call Dependency, Neural Network, Open Source, Code Summarization},
location = {Fukuoka, Japan},
series = {Internetware '19}
}

@inproceedings{10.1145/3243127.3243132,
author = {Gelman, Ben and Hoyle, Bryan and Moore, Jessica and Saxe, Joshua and Slater, David},
title = {A Language-Agnostic Model for Semantic Source Code Labeling},
year = {2018},
isbn = {9781450359726},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243127.3243132},
doi = {10.1145/3243127.3243132},
abstract = {Code search and comprehension have become more difficult in recent years due to the rapid expansion of available source code. Current tools lack a way to label arbitrary code at scale while maintaining up-to-date representations of new programming languages, libraries, and functionalities. Comprehensive labeling of source code enables users to search for documents of interest and obtain a high-level understanding of their contents. We use Stack Overflow code snippets and their tags to train a language-agnostic, deep convolutional neural network to automatically predict semantic labels for source code documents. On Stack Overflow code snippets, we demonstrate a mean area under ROC of 0.957 over a long-tailed list of 4,508 tags. We also manually validate the model outputs on a diverse set of unlabeled source code documents retrieved from Github, and obtain a top-1 accuracy of 86.6%. This strongly indicates that the model successfully transfers its knowledge from Stack Overflow snippets to arbitrary source code documents.},
booktitle = {Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis},
pages = {36–44},
numpages = {9},
keywords = {deep learning, semantic labeling, natural language processing, crowdsourcing, source code, multilabel classification},
location = {Montpellier, France},
series = {MASES 2018}
}

@inproceedings{10.1007/978-3-319-21155-8_4,
author = {Criado, Javier and Mart\'{\i}nez, Salvador and Iribarne, Luis and Cabot, Jordi},
title = {Enabling the Reuse of Stored Model Transformations Through Annotations},
year = {2015},
isbn = {9783319211541},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-319-21155-8_4},
doi = {10.1007/978-3-319-21155-8_4},
abstract = {With the increasing adoption of MDE, model transformations, one of its core concepts together with metamodeling, stand out as a valuable asset. Therefore, a mechanism to annotate and store existing model transformations appears as a critical need for their efficient exploitation and reuse. Unfortunately, although several reuse mechanisms have been proposed for software artifacts in general and models in particular, none of them is specially tailored to the domain of model transformations. In order to fill this gap, we present here such a mechanism. Our approach is composed by two elements 1 a new DSL specially conceived for describing model transformations in terms of their functional and non-functional properties 2 a semi-automatic process for annotating and querying repositories of model transformations using as criteria the properties of our DSL. We validate the feasibility of our approach through a prototype implementation that integrates our approach in a GitHub repository.},
booktitle = {Proceedings of the 8th International Conference on Theory and Practice of Model Transformations - Volume 9152},
pages = {43–58},
numpages = {16}
}

@inproceedings{10.1145/3386527.3405916,
author = {Piech, Chris and Abu-El-Haija, Sami},
title = {Human Languages in Source Code: Auto-Translation for Localized Instruction},
year = {2020},
isbn = {9781450379519},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3386527.3405916},
doi = {10.1145/3386527.3405916},
abstract = {Computer science education has promised open access around the world, but access is largely determined by what human language you speak. As younger students learn computer science it is less appropriate to assume that they should learn English beforehand. To that end, we present CodeInternational, the first tool to translate code between human languages. To develop a theory of non-English code, and inform our translation decisions, we conduct a study of public code repositories on GitHub. The study is to the best of our knowledge the first on human-language in code and covers 2.9 million Java repositories. To demonstrate CodeInternational's educational utility, we build an interactive version of the popular English-language Karel reader and translate it into 100 spoken languages. Our translations have already been used in classrooms around the world, and represent a first step in an important open CS-education problem.},
booktitle = {Proceedings of the Seventh ACM Conference on Learning @ Scale},
pages = {167–174},
numpages = {8},
keywords = {translation, source-code, human-language, github},
location = {Virtual Event, USA},
series = {L@S '20}
}

@inproceedings{10.1145/3194793.3194797,
author = {Alsaeed, Ziyad and Young, Michal},
title = {Extending Existing Inference Tools to Mine Dynamic APIs},
year = {2018},
isbn = {9781450357548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194793.3194797},
doi = {10.1145/3194793.3194797},
abstract = {APIs often feature dynamic relations between client and service provider, such as registering for notifications or establishing a connection to a service. Dynamic specification mining techniques attempt to fill gaps in missing or decaying documentation, but current miners are blind to relations established dynamically. Because they cannot recover properties involving these dynamic structures, they may produce incomplete or misleading specifications. We have devised an extension to current dynamic specification mining techniques that ameliorates this shortcoming. The key insight is to monitor not only values dynamically, but also properties to track dynamic data structures that establish new relations between client and service provider. We have implemented this approach as an extension to the instrumentation component of Daikon, the leading example of dynamic invariant mining in the research literature. We evaluated our tool by applying it to selected modules of widely used software systems published on GitHub.},
booktitle = {Proceedings of the 2nd International Workshop on API Usage and Evolution},
pages = {23–26},
numpages = {4},
keywords = {specification mining, design patterns, dynamic analysis},
location = {Gothenburg, Sweden},
series = {WAPI '18}
}

@inproceedings{10.1145/3180155.3180209,
author = {Trockman, Asher and Zhou, Shurui and K\"{a}stner, Christian and Vasilescu, Bogdan},
title = {Adding Sparkle to Social Coding: An Empirical Study of Repository Badges in the <i>Npm</i> Ecosystem},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180209},
doi = {10.1145/3180155.3180209},
abstract = {In fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {511–522},
numpages = {12},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3368089.3417942,
author = {Escobar-Vel\'{a}squez, Camilo and Riveros, Diego and Linares-V\'{a}squez, Mario},
title = {MutAPK 2.0: A Tool for Reducing Mutation Testing Effort of Android Apps},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417942},
doi = {10.1145/3368089.3417942},
abstract = {Mutation testing is a time consuming process because large sets of fault-injected-versions of an original app are generated and executed with the purpose of evaluating the quality of a given test suite. In the case of Android apps, recent studies even suggest that mutant generation and mutation testing effort could be greater when the mutants are generated at the APK level. To reduce that effort, useless (e.g., equivalent) mutants should be avoided and mutant selection techniques could be used to reduce the set of mutants used with mutation testing. However, despite the existence of mutation testing tools, none of those tools provides features for removing useless mutants and sampling mutant sets. In this paper, we present MutAPK 2.0, an improved version of our open source mutant generation tool (MutAPK) for Android apps at APK level. To the best of our knowledge, MutAPK 2.0 is the first tool that enables the removal of dead-code mutants, provides a set of mutant selection strategies, and removes automatically equivalent and duplicate mutants. MutAPK 2.0 is publicly available at GitHub: https://thesoftwaredesignlab.github.io/MutAPK/ VIDEO: https://thesoftwaredesignlab.github.io/MutAPK/video.html},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1611–1615},
numpages = {5},
keywords = {Duplicate, Dead code, Mutant Selection, Equivalent, Mutation Testing},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3379597.3387441,
author = {Nakamaru, Tomoki and Matsunaga, Tomomasa and Yamazaki, Tetsuro and Akiyama, Soramichi and Chiba, Shigeru},
title = {An Empirical Study of Method Chaining in Java},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387441},
doi = {10.1145/3379597.3387441},
abstract = {While some promote method chaining as a good practice for improving code readability, others refer to it as a bad practice that worsens code quality. In this paper, we first investigate whether method chaining is a programming style accepted by real-world programmers. To answer this question, we collected 2,814 Java repositories on GitHub and analyzed historical trends in the frequency of method chaining. The results of our analysis revealed the increasing use of method chaining; 23.1% of method invocations were part of method chains in 2018, whereas only 16.0% were such invocations in 2010. We then explore language features that are helpful to the method-chaining style but have not been supported yet in Java. For this aim, we conducted manual inspections of method chains that are randomly sampled from the collected repositories. We also estimated how effective they are to encourage the method-chaining style if they are adopted in Java.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {93–102},
numpages = {10},
keywords = {Quantitative analysis, Repository mining, Method chaining},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/ICCPS.2018.00050,
author = {Lukina, Anna and Kumar, Arjun and Schmittle, Matt and Singh, Abhijeet and Das, Jnaneshwar and Rees, Stephen and Buskirk, Christopher P. and Sztipanovits, Janos and Grosu, Radu and Kumar, Vijay},
title = {Formation Control and Persistent Monitoring in the OpenUAV Swarm Simulator on the NSF CPS-VO},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCPS.2018.00050},
doi = {10.1109/ICCPS.2018.00050},
abstract = {Simulation tools offer a low barrier to entry and enable testing and validation before field trials. However, most of the well-known simulators today are challenging to use at scale due to the need for powerful computers and the time required for initial set up. The OpenUAV Swarm Simulator was developed to address these challenges, enabling multi-UAV simulations on the cloud through the NSF CPS-VO. We leverage the Containers as a Service (CaaS) technology to enable students and researchers carry out simulations on the cloud on demand. We have based our framework on open-source tools including ROS, Gazebo, Docker, and the PX4 flight stack, and we designed the simulation framework so that it has no special hardware requirements. The demo and poster will showcase UAV swarm trajectory optimization, and multi-UAV persistent monitoring on the CPS-VO. The code for the simulator is available on GitHub: https://github.com/Open-UAV.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {353–354},
numpages = {2},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@inproceedings{10.1109/MSR.2019.00041,
author = {Dietrich, Jens and Luczak-Roesch, Markus and Dalefield, Elroy},
title = {Man vs Machine: A Study into Language Identification of Stack Overflow Code Snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00041},
doi = {10.1109/MSR.2019.00041},
abstract = {Software engineers produce large amounts of publicly accessible data that enables researchers to mine knowledge, fostering a better understanding of the field. Knowledge extraction often relies on meta data. This meta data can either be harvested from user-provided tags, or inferred by algorithms from the respective data. The question arises to which extent either type of meta data can be trusted and relied upon.We study this problem in the context of language identification of code snippets posted on Stack Overflow. We analyse the consistency between user-provided tags and the classification obtained with GitHub linguist, an industry-strength automated language recognition tool. We find that the results obtained by both approaches are often not consistent. This indicates that both have to be used with great care. Our results also suggest that developers may not follow the evolutionary path of programming languages beyond one step when seeking or providing answers to software engineering challenges encountered.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {205–209},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3407197.3407211,
author = {Lotfi Rezaabad, Ali and Vishwanath, Sriram},
title = {Long Short-Term Memory Spiking Networks and Their Applications},
year = {2020},
isbn = {9781450388511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407197.3407211},
doi = {10.1145/3407197.3407211},
abstract = { Recent advances in event-based neuromorphic systems have resulted in significant interest in the use and development of spiking neural networks (SNNs). However, the non-differentiable nature of spiking neurons makes SNNs incompatible with conventional backpropagation techniques. In spite of the significant progress made in training conventional deep neural networks (DNNs), training methods for SNNs still remain relatively poorly understood. In this paper, we present a novel framework for training recurrent SNNs. Analogous to the benefits presented by recurrent neural networks (RNNs) in learning time series models within DNNs, we develop SNNs based on long short-term memory (LSTM) networks. We show that LSTM spiking networks learn the timing of the spikes and temporal dependencies. We also develop a methodology for error backpropagation within LSTM-based SNNs. The developed architecture and method for backpropagation within LSTM-based SNNs enable them to learn long-term dependencies with comparable results to conventional LSTMs. Code is available on github; https://github.com/AliLotfi92/SNNLSTM },
booktitle = {International Conference on Neuromorphic Systems 2020},
articleno = {3},
numpages = {9},
location = {Oak Ridge, TN, USA},
series = {ICONS 2020}
}

@inproceedings{10.1109/ICPC.2019.00052,
author = {Xu, Shengzhe and Dong, Ziqi and Meng, Na},
title = {Meditor: Inference and Application of API Migration Edits},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00052},
doi = {10.1109/ICPC.2019.00052},
abstract = {Developers build programs based on software libraries. When a library evolves, programmers need to migrate their client code from the library's old release(s) to new release(s). Due to the API backwards incompatibility issues, such code migration may require developers to replace API usage and apply extra edits (e.g., statement insertions or deletions) to ensure the syntactic or semantic correctness of migrated code. Existing tools extract API replacement rules without handling the additional edits necessary to fulfill a migration task. This paper presents our novel approach, Meditor, which extracts and applies the necessary edits together with API replacement changes.Meditor has two phases: inference and application of migration edits. For edit inference, Meditor mines open source repositories for migration-related (MR) commits, and conducts program dependency analysis on changed Java files to locate and cluster MR code changes. From these changes, Meditor further generalizes API migration edits by abstracting away unimportant details (e.g., concrete variable identifiers). For edit application, Meditor matches a given program with inferred edits to decide which edit is applicable, customizes each applicable edit, and produces a migrated version for developers to review.We applied Meditor to four popular libraries: Lucene, Craft-Bukkit, Android SDK, and Commons IO. By searching among 602,249 open source projects on GitHub, Meditor identified 1,368 unique migration edits. Among these edits, 885 edits were extracted from single updated statements, while the other 483 more complex edits were from multiple co-changed statements. We sampled 937 inferred edits for manual inspection and found all of them to be correct. Our evaluation shows that Meditor correctly applied code migrations in 218 out of 225 cases. This research will help developers automatically adapt client code to different library versions.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {335–346},
numpages = {12},
keywords = {program dependency analysis, automatic program transformation, API migration edits},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1109/ICSE-C.2017.8,
author = {Li, Yuanchun and Yang, Ziyue and Guo, Yao and Chen, Xiangqun},
title = {DroidBot: A Lightweight UI-Guided Test Input Generator for Android},
year = {2017},
isbn = {9781538615898},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-C.2017.8},
doi = {10.1109/ICSE-C.2017.8},
abstract = {As many automated test input generation tools for Android need to instrument the system or the app, they cannot be used in some scenarios such as compatibility testing and malware analysis. We introduce DroidBot, a lightweight UI-guided test input generator, which is able to interact with an Android app on almost any device without instrumentation. The key technique behind DroidBot is that it can generate UI-guided test inputs based on a state transition model generated on-the-fly, and allow users to integrate their own strategies or algorithms. DroidBot is lightweight as it does not require app instrumentation, thus no need to worry about the inconsistency between the tested version and the original version. It is compatible to most Android apps, and able to run on almost all Android-based systems, including customized sandboxes and commodity devices. Droidbot is released as an open-source tool on GitHub [1], and the demo video can be found at https://youtu.be/3-aHGSazMY.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering Companion},
pages = {23–26},
numpages = {4},
keywords = {automated testing, Android, malware detection, dynamic analysis, compatibility testing},
location = {Buenos Aires, Argentina},
series = {ICSE-C '17}
}

@inproceedings{10.1109/SEAA.2014.63,
author = {Paolo, Giampiero Di and Malavolta, Ivano and Muccini, Henry},
title = {How Do You Feel Today? Buggy!},
year = {2014},
isbn = {9781479957958},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SEAA.2014.63},
doi = {10.1109/SEAA.2014.63},
abstract = {It is well-known that moods and emotions strongly affect our performances. Clearly, this holds also for software developers. Thus, modern managers, trainers, and coaches should be aware of moods and emotions of software developers in their teams. In this context, mining software repositories and social networks in combination can be an invaluable instrument for understanding how the moods and emotions of software developers impact their performance, even in real-time. In this paper, we propose our first steps in mining software repositories for (i) getting information about developers' moods and emotion throughout the development process, and (ii) investigating on the existence of the correlation between software developers' performance (in terms of their commits bugginess) and mood. For what concerns data sources, we use publicly-available information on GitHub for getting insights about the performance of software developers, while we semantically analyse developers' posts on Twitter for extracting their moods during the whole duration of the project.},
booktitle = {Proceedings of the 2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications},
pages = {391},
numpages = {1},
series = {SEAA '14}
}

@inproceedings{10.1145/3196321.3196330,
author = {Jaffe, Alan and Lacomis, Jeremy and Schwartz, Edward J. and Goues, Claire Le and Vasilescu, Bogdan},
title = {Meaningful Variable Names for Decompiled Code: A Machine Translation Approach},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196330},
doi = {10.1145/3196321.3196330},
abstract = {When code is compiled, information is lost, including some of the structure of the original source code as well as local identifier names. Existing decompilers can reconstruct much of the original source code, but typically use meaningless placeholder variables for identifier names. Using variable names which are more natural in the given context can make the code much easier to interpret, despite the fact that variable names have no effect on the execution of the program. In theory, it is impossible to recover the original identifier names since that information has been lost. However, most code is natural: it is highly repetitive and predictable based on the context. In this paper we propose a technique that assigns variables meaningful names by taking advantage of this naturalness property. We consider decompiler output to be a noisy distortion of the original source code, where the original source code is transformed into the decompiler output. Using this noisy channel model, we apply standard statistical machine translation approaches to choose natural identifiers, combining a translation model trained on a parallel corpus with a language model trained on unmodified C code. We generate a large parallel corpus from 1.2 TB of C source code obtained from GitHub. Under the most conservative assumptions, our technique is still able to recover the original variable names up to 16.2% of the time, which represents a lower bound for performance.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {20–30},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3035918.3058739,
author = {Singh, Rohit and Meduri, Vamsi and Elmagarmid, Ahmed and Madden, Samuel and Papotti, Paolo and Quian\'{e}-Ruiz, Jorge-Arnulfo and Solar-Lezama, Armando and Tang, Nan},
title = {Generating Concise Entity Matching Rules},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3058739},
doi = {10.1145/3035918.3058739},
abstract = {Entity matching (EM) is a critical part of data integration and cleaning. In many applications, the users need to understand why two entities are considered a match, which reveals the need for interpretable and concise EM rules. We model EM rules in the form of General Boolean Formulas (GBFs) that allows arbitrary attribute matching combined by conjunctions (∨), disjunctions (∧), and negations. (¬) GBFs can generate more concise rules than traditional EM rules represented in disjunctive normal forms (DNFs). We use program synthesis, a powerful tool to automatically generate rules (or programs) that provably satisfy a high-level specification, to automatically synthesize EM rules in GBF format, given only positive and negative matching examples.In this demo, attendees will experience the following features: (1) Interpretability -- they can see and measure the conciseness of EM rules defined using GBFs; (2) Easy customization -- they can provide custom experiment parameters for various datasets, and, easily modify a rich predefined (default) synthesis grammar, using a Web interface; and (3) High performance -- they will be able to compare the generated concise rules, in terms of accuracy, with probabilistic models (e.g., machine learning methods), and hand-written EM rules provided by experts. Moreover, this system will serve as a general platform for evaluating different methods that discover EM rules, which will be released as an open-source tool on GitHub.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1635–1638},
numpages = {4},
keywords = {entity matching, general boolean formulas, program synthesis, disjunctive normal forms},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3387514.3405871,
author = {Kakarla, Siva Kesava Reddy and Beckett, Ryan and Arzani, Behnaz and Millstein, Todd and Varghese, George},
title = {GRooT: Proactive Verification of DNS Configurations},
year = {2020},
isbn = {9781450379557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387514.3405871},
doi = {10.1145/3387514.3405871},
abstract = {The Domain Name System (DNS) plays a vital role in today's Internet but relies on complex distributed management of records. DNS misconfiguration related outages have rendered popular services like GitHub, HBO, LinkedIn, and Azure inaccessible for extended periods. This paper introduces GRoot, the first verifier that performs static analysis of DNS configuration files, enabling proactive and exhaustive checking for common DNS bugs; by contrast, existing solutions are reactive and incomplete. GRoot uses a new, fast verification algorithm based on generating and enumerating DNS query equivalence classes. GRoot symbolically executes the set of queries in each equivalence class to efficiently find (or prove the absence of) any bugs such as rewrite loops. To prove the correctness of our approach, we develop a formal semantic model of DNS resolution. Applied to the configuration files from a campus network with over a hundred thousand records, GRoot revealed 109 bugs within seconds. When applied to internal zone files consisting of over 3.5 million records from a large infrastructure service provider, GRoot revealed around 160k issues of blackholing, initiating a cleanup. Finally, on a synthetic dataset with over 65 million real records, we find GRoot can scale to networks with tens of millions of records.},
booktitle = {Proceedings of the Annual Conference of the ACM Special Interest Group on Data Communication on the Applications, Technologies, Architectures, and Protocols for Computer Communication},
pages = {310–328},
numpages = {19},
keywords = {Formal Methods, DNS, Verification, Static Analysis},
location = {Virtual Event, USA},
series = {SIGCOMM '20}
}

@inproceedings{10.1145/3196321.3196344,
author = {H\"{a}rtel, Johannes and Aksu, Hakan and L\"{a}mmel, Ralf},
title = {Classification of APIs by Hierarchical Clustering},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196344},
doi = {10.1145/3196321.3196344},
abstract = {APIs can be classified according to the programming domains (e.g., GUIs, databases, collections, or security) that they address. Such classification is vital in searching repositories (e.g., the Maven Central Repository for Java) and for understanding the technology stack used in software projects. We apply hierarchical clustering to a curated suite of Java APIs to compare the computed API clusters with preexisting API classifications. Clustering entails various parameters (e.g., the choice of IDF versus LSI versus LDA). We describe the corresponding variability in terms of a feature model. We exercise all possible configurations to determine the maximum correlation with respect to two baselines: i) a smaller suite of APIs manually classified in previous research; ii) a larger suite of APIs from the Maven Central Repository, thereby taking advantage of crowd-sourced classification while relying on a threshold-based approach for identifying important APIs and versions thereof, subject to an API dependency analysis on GitHub. We discuss the configurations found in this way and we examine the influence of particular features on the correlation between computed clusters and baselines. To this end, we also leverage interactive exploration of the parameter space and the resulting dendrograms. In this manner, we can also identify issues with the use of classifiers (e.g., missing classifiers) in the baselines and limitations of the clustering approach.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {233–243},
numpages = {11},
keywords = {feature modeling, clustering exploration, APIs, maven central repository, hierarchical clustering, github},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/3397536.3429335,
author = {Saxena, Nikita},
title = {Efficient Downscaling of Satellite Oceanographic Data With Convolutional Neural Networks},
year = {2020},
isbn = {9781450380195},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397536.3429335},
doi = {10.1145/3397536.3429335},
abstract = {Space-borne satellite radiometers measure Sea Surface Temperature (SST), which is pivotal to studies of air-sea interactions and ocean features. Under clear sky conditions, high resolution measurements are obtainable. But under cloudy conditions, data analysis is constrained to the available low resolution measurements. We assess the efficiency of Deep Learning (DL) architectures, particularly Convolutional Neural Networks (CNN) to downscale oceanographic data from low spatial resolution (SR) to high SR. With a focus on SST Fields of Bay of Bengal, this study proves that Very Deep Super Resolution CNN can successfully reconstruct SST observations from 15 km SR to 5km SR, and 5km SR to 1km SR. This outcome calls attention to the significance of DL models explicitly trained for the reconstruction of high SR SST fields by using low SR data. Inference on DL models can act as a substitute to the existing computationally expensive downscaling technique: Dynamical Downsampling. The complete code is available on this Github Repository.},
booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
pages = {659–660},
numpages = {2},
keywords = {Single Image Super Resolution, Deep Convolutional Neural Networks, Ocean Remote Sensing Data},
location = {Seattle, WA, USA},
series = {SIGSPATIAL '20}
}

@inproceedings{10.5555/2486788.2486844,
author = {Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.},
title = {Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {422–431},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.5555/3021955.3022007,
author = {Oliveira, Johnatan and Fernandes, Eduardo and Souza, Mauricio and Figueiredo, Eduardo},
title = {A Method Based on Naming Similarity to Identify Reuse Opportunities},
year = {2016},
isbn = {9788576693178},
publisher = {Brazilian Computer Society},
address = {Porto Alegre, BRA},
abstract = {Software reuse is a development strategy in which existing software components, called reusable assets, are used in the development of new software systems. There are many advantages of reuse in software development, such as minimization of development efforts and improvement of software quality. New methods for reusable asset extraction are essential to achieve these advantages. Extraction methods may be used in different contexts including software product lines derivation. However, few methods have been proposed in literature for reusable asset extraction and recommendation of these reuse opportunities. In this paper, we propose a method for extraction of reuse opportunities based on naming similarity of two types of object-oriented entities: classes and methods. Our method, called JReuse, computes a similarity function to identify similarly named classes and methods from a set of software systems from a domain. These classes and methods compose a repository with reuse opportunities. We also present a prototype tool to support the extraction by applying our method. We evaluate the method with 38 e-commerce information systems mined from GitHub. As a result, we observe that our method is able to identify classes and methods that are relevant in the e-commerce domain.},
booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
pages = {305–312},
numpages = {8},
keywords = {naming similarity, reusable assets, Software reuse, tool},
location = {Florianopolis, Santa Catarina, Brazil},
series = {SBSI 2016}
}

@inproceedings{10.1109/eScience.2014.54,
author = {Aierken, Ailifan and Davis, Delmar B. and Zhang, Qi and Gupta, Kriti and Wong, Alex and Asuncion, Hazeline U.},
title = {A Multi-Level Funneling Approach to Data Provenance Reconstruction},
year = {2014},
isbn = {9781479942879},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/eScience.2014.54},
doi = {10.1109/eScience.2014.54},
abstract = {When data are retrieved from a file storage system or the Internet, is there information about their provenance (i.e., their origin or history)? It is possible that data could have been copied from another source and then transformed. Often, provenance is not readily available for data sets created in the past. Solving such a problem is the motivation behind the 2014 Provenance Reconstruction Challenge. This challenge is aimed at recovering lost provenance for two data sets: one data set (WikiNews articles) in which a list of possible sources has been provided, and another data set (files from GitHub repositories) in which the file sources are not provided. To address this challenge, we present a multi-level funneling approach to provenance reconstruction, a technique that incorporates text processing techniques from different disciplines to approximate the provenance of a given data set. We built three prototypes using this technique and evaluated them using precision and recall metrics. Our preliminary results indicate that our technique is capable of reconstructing some of the lost provenance.},
booktitle = {Proceedings of the 2014 IEEE 10th International Conference on E-Science - Volume 02},
pages = {71–74},
numpages = {4},
keywords = {longest common subsequence, data provenance reconstruction, vector space model, topic modeling, similarity metrics, semantic analysis},
series = {E-SCIENCE '14}
}

@inproceedings{10.1145/3275219.3275223,
author = {Wang, Tao and Zhang, Yang and Yin, Gang and Yu, Yue and Wang, Huaimin},
title = {Who Will Become a Long-Term Contributor? A Prediction Model Based on the Early Phase Behaviors},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275223},
doi = {10.1145/3275219.3275223},
abstract = {The continuous contribution from peripheral participants is crucial for the success of open source projects. Thus, how to identify the potential Long-Term Contributors (LTC) early and retain them is of great importance. We propose a prediction model to measure the chance for an individual to become a LTC contributor through his capacity, willingness, and the opportunity to contribute at the time of joining. Using data of Rails hosted on GitHub, we find that the probability for a new joiner to become a LTC is associated with his willingness and environment. Specifically, future LTCs tend to be more active and show more community-oriented attitude than other joiners during their first month. This implies that the interaction between individual's attitude and project's climate are associated with the odds that an individual would become a valuable contributor or disengage from the project. We evaluated our prediction model by using the 10 cross-validation method. Results show that our model archives the mean AUC as 0.807, which is valuable for OSS projects to identify potential long-term contributors and adopt better strategies to retain them for continuous contribution.},
booktitle = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
articleno = {9},
numpages = {10},
keywords = {GitHub, Open Source, Long Term Contributor, Developer behavior},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1109/CloudCom.2015.26,
author = {Zhuang, Hao and Rahman, Rameez and Hui, Pan and Aberer, Karl},
title = {StoreSim: Optimizing Information Leakage in Multicloud Storage Services},
year = {2015},
isbn = {9781467395601},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CloudCom.2015.26},
doi = {10.1109/CloudCom.2015.26},
abstract = {Many schemes have been recently advanced for storing data on multiple clouds. Distributing data over different cloud storage providers (CSPs) automatically provides users with a certain degree of information leakage control, as no single point of attack can leak all user's information. However, unplanned distribution of data chunks can lead to high information disclosure even while using multiple clouds. In this paper, to address this problem we present StoreSim, an information leakage aware storage system in multicloud. StoreSim aims to store syntactically similar data on the same cloud, thus minimizing the user's information leakage across multiple clouds. We design an approximate algorithm to efficiently generate similarity-preserving signatures for data chunks based on MinHash and Bloom filter, and also design a function to compute the information leakage based on these signatures. Next, we present an effective storage plan generation algorithm based on clustering for distributing data chunks with minimal information leakage across multiple clouds. Finally, we evaluate our scheme using two real datasets from Wikipedia and GitHub. We show that our scheme can reduce the information leakage by up to 60% compared to unplanned placement.},
booktitle = {Proceedings of the 2015 IEEE 7th International Conference on Cloud Computing Technology and Science (CloudCom)},
pages = {379–386},
numpages = {8},
series = {CLOUDCOM '15}
}

@inproceedings{10.1145/3052973.3052982,
author = {Felsch, Dennis and Mainka, Christian and Mladenov, Vladislav and Schwenk, J\"{o}rg},
title = {SECRET: On the Feasibility of a Secure, Efficient, and Collaborative Real-Time Web Editor},
year = {2017},
isbn = {9781450349444},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3052973.3052982},
doi = {10.1145/3052973.3052982},
abstract = {Real-time editing tools like Google Docs, Microsoft Office Online, or Etherpad have changed the way of collaboration. Many of these tools are based on Operational Transforms (OT), which guarantee that the views of different clients onto a document remain consistent over time. Usually, documents and operations are exposed to the server in plaintext -- and thus to administrators, governments, and potentially cyber criminals. Therefore, it is highly desirable to work collaboratively on encrypted documents. Previous implementations do not unleash the full potential of this idea: They either require large storage, network, and computation overhead, are not real-time collaborative, or do not take the structure of the document into account. The latter simplifies the approach since only OT algorithms for byte sequences are required, but the resulting ciphertexts are almost four times the size of the corresponding plaintexts.We present SECRET, the first secure, efficient, and collaborative real-time editor. In contrast to all previous works, SECRET is the first tool that (1.) allows the encryption of whole documents or arbitrary sub-parts thereof, (2.) uses a novel combination of tree-based OT with a structure preserving encryption, and (3.) requires only a modern browser without any extra software installation or browser extension.We evaluate our implementation and show that its encryption overhead is three times smaller in comparison to all previous approaches. SECRET can even be used by multiple users in a low-bandwidth scenario. The source code of SECRET is published on GitHub as an open-source project:https://github.com/RUB-NDS/SECRET/},
booktitle = {Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security},
pages = {835–848},
numpages = {14},
keywords = {XML encryption, operational transforms, collaborative editing, JSON, structure preserving encryption},
location = {Abu Dhabi, United Arab Emirates},
series = {ASIA CCS '17}
}

@inproceedings{10.1145/3402942.3409789,
author = {Stephens, Conor and Exton, Dr. Chris},
title = {Assessing Multiplayer Level Design Using Deep Learning Techniques},
year = {2020},
isbn = {9781450388078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3402942.3409789},
doi = {10.1145/3402942.3409789},
abstract = { This paper proposes a new framework to measure the fairness of asymmetric level-design in multiplayer games. This work achieves real time prediction of the degree to which asymmetric levels are balanced using deep learning. The proposed framework provides both cost and time savings, by removing the requirement of numerous designed levels and the need to gather player data samples. This advancement with the field is possible through the combination of deep reinforcement learning (made accessible to developers with Unity’s ML-Agents framework), and Procedural Content Generation (PCG). The result of this merger is the acquisition of accelerated training data, which is established using parallel simulations. This paper showcases the proposed approach on a simple two player top-down -shooter game implemented using MoreMountains: Top Down Engine an extension to Unity 3D a popular game engine. Levels are generated using the same PCG approaches found in ’Nuclear Throne’ a popular cross platform Roguelike published by Vlambeer. This approach is accessible and easy to implement allowing games developers to test human-designed content in real time using the predictions. This research is open source and available on Github: https://github.com/Taikatou/top-down-shooter.},
booktitle = {International Conference on the Foundations of Digital Games},
articleno = {16},
numpages = {3},
keywords = {Game Balance, Procedural Content Generation, Reinforcement Learning, Level Design},
location = {Bugibba, Malta},
series = {FDG '20}
}

@inproceedings{10.1145/3233547.3233577,
author = {Amin, Mohammad Ruhul and Yurovsky, Alisa and Tian, Yingtao and Skiena, Steven},
title = {DeepAnnotator: Genome Annotation with Deep Learning},
year = {2018},
isbn = {9781450357944},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233547.3233577},
doi = {10.1145/3233547.3233577},
abstract = {Genome annotation is the process of labeling DNA sequences of an organism with its biological features, and is one of the fundamental problems in Bioinformatics. Public annotation pipelines such as NCBI integrate a variety of algorithms and homology searches on public and private databases. However, they build on the information of varying consistency and quality, produced over the last two decades. We identified 12,415 errors in NCBI RNA gene annotations, demonstrating the need for improved annotation programs. We use Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) to demonstrate the potential of deep learning networks to annotate genome sequences, and evaluate different approaches on prokaryotic sequences from NCBI database. Particularly, we evaluate DNA $K-$mer embeddings and the application of RNNs for genome annotation. We show how to improve the performance of our deep networks by incorporating intermediate objectives and downstream algorithms to achieve better accuracy. Our method, called DeepAnnotator, achieves an F-score of ~94%, and establishes a generalized computational approach for genome annotation using deep learning. Our results are very encouraging as our method eliminates the requirement of hand crafted features and motivates further research in application of deep learning to full genome annotation. DeepAnnotator algorithms and models can be accessed in Github: urlhttps://github.com/ruhulsbu/DeepAnnotator.},
booktitle = {Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics},
pages = {254–259},
numpages = {6},
keywords = {deep learning, rnns, genome annotation, dna embeddings},
location = {Washington, DC, USA},
series = {BCB '18}
}

@inproceedings{10.1145/3416505.3423562,
author = {Steinhauer, Martin and Palomba, Fabio},
title = {Speeding up the Data Extraction of Machine Learning Approaches: A Distributed Framework},
year = {2020},
isbn = {9781450381246},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416505.3423562},
doi = {10.1145/3416505.3423562},
abstract = {In the last decade, mining software repositories (MSR) has become one of the most important sources to feed machine learning models. Especially open-source projects on platforms like GitHub are providing a tremendous amount of data and make them easily accessible. Nevertheless, there is still a lack of standardized pipelines to extract data in an automated and fast way. Even though several frameworks and tools exist which can fulfill specific tasks or parts of the data extraction process, none of them allow neither building an automated mining pipeline nor the possibility for full parallelization. As a consequence, researchers interested in using mining software repositories to feed machine learning models are often forced to re-implement commonly used tasks leading to additional development time and libraries may not be integrated optimally.  This preliminary study aims to demonstrate current limitations of existing tools and Git itself which are threatening the prospects of standardization and parallelization. We also introduce the multi-dimensionality aspects of a Git repository and how they affect the computation time. Finally, as a proof of concept, we define an exemplary pipeline for predicting refactoring operations, assessing its performance. Finally, we discuss the limitations of the pipeline and further optimizations to be done.},
booktitle = {Proceedings of the 4th ACM SIGSOFT International Workshop on Machine-Learning Techniques for Software-Quality Evaluation},
pages = {13–18},
numpages = {6},
keywords = {Mining Software Repositories, Machine Learning Pipelines, Distributed Mining},
location = {Virtual, USA},
series = {MaLTeSQuE 2020}
}

@inproceedings{10.1145/3322790.3330596,
author = {David, Andrea and Souppe, Mariette and Jimenez, Ivo and Obraczka, Katia and Mansfield, Sam and Veenstra, Kerry and Maltzahn, Carlos},
title = {Reproducible Computer Network Experiments: A Case Study Using Popper},
year = {2019},
isbn = {9781450367561},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322790.3330596},
doi = {10.1145/3322790.3330596},
abstract = {Computer network research experiments can be broadly grouped in three categories: simulated, controlled, and real-world experiments. Simulation frameworks, experiment testbeds and measurement tools, respectively, are commonly used as the platforms for carrying out network experiments. In many cases, given the nature of computer network experiments, properly configuring these platforms is a complex and time-consuming task, which makes replicating and validating research results quite challenging. This complexity can be reduced by leveraging tools that enable experiment reproducibility. In this paper, we show how a recently proposed reproducibility tool called Popper facilitates the reproduction of networking experiments. In particular, we detail the steps taken to reproduce results in two published articles that rely on simulations. The outcome of this exercise is a generic workflow for carrying out network simulation experiments. In addition, we briefly present two additional Popper workflows for running experiments on controlled testbeds, as well as studies that gather real-world metrics (all code is publicly available on Github). We close by providing a list of lessons we learned throughout this process.},
booktitle = {Proceedings of the 2nd International Workshop on Practical Reproducible Evaluation of Computer Systems},
pages = {29–34},
numpages = {6},
keywords = {software automation, network experiment simulation, reproducible network experiments, popper},
location = {Phoenix, AZ, USA},
series = {P-RECS '19}
}

@inproceedings{10.1145/2989238.2989244,
author = {Shakiba, Abbas and Green, Robert and Dyer, Robert},
title = {FourD: Do Developers Discuss Design? Revisited},
year = {2016},
isbn = {9781450343954},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2989238.2989244},
doi = {10.1145/2989238.2989244},
abstract = {Software repositories contain a variety of information that can be mined and utilized to enhance software engineering processes. Patterns stored in software repository meta-data can provide useful and informative information about different aspects of a project, particularly those that may not be obvious for developers. One such aspect is the role of software design in a project. The messages connected to each commit in the repository note not only what changes have been made to project files, but potentially if those changes have somehow manipulated the design of the software.In this paper, a sample of commit messages from a random sample of projects on GitHub and SourceForge are manually classified as "design" or "non-design" based on a survey. The resulting data is then used to train multiple machine learning algorithms in order to determine if it is possible to predict whether or not a single commit is discussing software design. Our results show the Random Forest classifier performed best on our combined data set with a G-mean of 75.01.},
booktitle = {Proceedings of the 2nd International Workshop on Software Analytics},
pages = {43–46},
numpages = {4},
keywords = {machine-learning, Boa, software design, mining},
location = {Seattle, WA, USA},
series = {SWAN 2016}
}

@inproceedings{10.1145/2627508.2627512,
author = {Dai, Meixi and Shen, Beijun and Zhang, Tao and Zhao, Min},
title = {Impact of Consecutive Changes on Later File Versions},
year = {2014},
isbn = {9781450329651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2627508.2627512},
doi = {10.1145/2627508.2627512},
abstract = { By analyzing histories of program versions, many researches have shown that software quality is associated with history-related metrics, such as code-related metrics, commit-related metrics, developer-related metrics, process-related metrics, and organizational metrics etc. It has also been revealed that consecutive changes on commit-level are strongly associated with software defects. In this paper, we introduce two novel concepts of consecutive changes: CFC (chain of consecutive bug-fixing file versions) and CAC (chain of consecutive file versions where each pair of adjacent versions are submitted by different developers). And then several experiments are conducted to explore the correlation between consecutive changes and software quality by using three open-source projects from Github. Our main findings include: 1) CFCs and CACs widely exist in file version histories; 2) Consecutive changes have a negative and strong impact on the later file versions in a short term, especially when the length of consecutive change chain is 4 or 5. },
booktitle = {Proceedings of the 2014 3rd International Workshop on Evidential Assessment of Software Technologies},
pages = {17–24},
numpages = {8},
keywords = {consecutive change, Software quality, file version histories, mining software repository},
location = {Nanjing, China},
series = {EAST 2014}
}

@inproceedings{10.1145/3368926.3369711,
author = {Ha, Duy-An and Chen, Ting-Hsuan and Yuan, Shyan-Ming},
title = {Unsupervised Methods for Software Defect Prediction},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369711},
doi = {10.1145/3368926.3369711},
abstract = {Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github.},
booktitle = {Proceedings of the Tenth International Symposium on Information and Communication Technology},
pages = {49–55},
numpages = {7},
keywords = {Software Defect Prediction, Community Structure Detection, Software Engineering, Unsupervised Learning, Machine Learning},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT 2019}
}

@inproceedings{10.1145/3191697.3214341,
author = {Horschig, Siegfried and Mattis, Toni and Hirschfeld, Robert},
title = {Do Java Programmers Write Better Python? Studying off-Language Code Quality on GitHub},
year = {2018},
isbn = {9781450355131},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3191697.3214341},
doi = {10.1145/3191697.3214341},
abstract = {There are style guides and best practices for many programming languages. Their goal is to promote uniformity and readability of code, consequentially reducing the chance of errors.  While programmers who are frequently using the same programming language tend to internalize most of its best practices eventually, little is known about what happens when they casually switch languages and write code in a less familiar language. Insights into the factors that lead to coding convention violations could help to improve tutorials for programmers switching languages, make teachers aware of mistakes they might expect depending on what language students have been using before, or influence the order in which programming languages are taught.  To approach this question, we make use of a large-scale data set representing a major part of the open source development activity happening on GitHub. In this data set, we search for Java and C++ programmers that occasionally program Python and study their Python code quality using a lint tool.  Comparing their defect rates to those from Python programmers reveals significant effects in both directions: We observe that some of Python's best practices have more widespread adoption among Java and C++ programmers than Python experts. At the same time, python-specific coding conventions, especially indentation, scoping, and the use of semicolons, are violated more frequently.  We conclude that programming off-language is not generally associated with better or worse code quality, but individual coding conventions are violated more or less frequently depending on whether they are more universal or language-specific. We intend to motivate a discussion and more research on what causes these effects, how we can mitigate or use them for good, and which related effects can be studied using the presented data set.},
booktitle = {Conference Companion of the 2nd International Conference on Art, Science, and Engineering of Programming},
pages = {127–134},
numpages = {8},
keywords = {explorative study, lint, github, best practices, code quality},
location = {Nice, France},
series = {Programming'18 Companion}
}

@inproceedings{10.1109/MSR.2017.62,
author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
title = {Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.62},
doi = {10.1109/MSR.2017.62},
abstract = {Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {356–367},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/3384772.3385125,
author = {Frid-Jimenez, Amber and Carson, Jesi and Scott, Alanna and Khantidhara, Paninee and Elza, Dethe},
title = {Designing Participedia: A Collaborative Research Platform},
year = {2020},
isbn = {9781450376068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384772.3385125},
doi = {10.1145/3384772.3385125},
abstract = {A transformation of democratic governance is occurring as public participation empowers citizens to have their voices heard beyond the vote.&nbsp;Participedia is a research community and online crowdsourcing platform designed to document and share emerging knowledge about participatory democracy. Participedia's women-led Design &amp; Technology (D&amp;T) team used participatory design (PD) and feminist human computer interaction (HCI) strategies to evaluate Participedia's formerly proprietary website and design and build a new, open source platform.By shifting Participedia to an open source technological approach, the D&amp;T team deliberately created opportunities for women and students and initiated new collaborations through channels like Github. Key design improvements, such as improved accessibility and reducing bias in the data model of Participedia, further align the project with feminist values of equity, diversity and inclusion (EDI). The D&amp;T team is part of a new generation of designers and developers contributing to interdisciplinary research through design and technology for social good.},
booktitle = {Proceedings of the 16th Participatory Design Conference 2020 - Participation(s) Otherwise - Volume 2},
pages = {21–25},
numpages = {5},
keywords = {Equity, Diversity, Inclusion, Participatory Design, Participatory Democracy, Open Source, Feminist Human Computer Interaction, Public Participation},
location = {Manizales, Colombia},
series = {PDC '20}
}

@inproceedings{10.1145/3149457.3149480,
author = {Takizawa, Ryota and Kawashima, Hideyuki and Mitsuhashi, Ryuya and Tatebe, Osamu},
title = {Performing External Join Operator on PostgreSQL with Data Transfer Approach},
year = {2018},
isbn = {9781450353724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3149457.3149480},
doi = {10.1145/3149457.3149480},
abstract = {With the development of sensing devices, the size of data managed by human being has been rapidly increasing. To manage such huge data, relational database management system (RDBMS) plays a key role. RDBMS models the real world data as n-ary relational tables. Join operator is one of the most important relational operators, and its acceleration has been studied widely and deeply. How can an RDBMS provide such an efficient join operator? The performance improvement of join operator has been deeply studied for a decade, and many techniques are proposed already. The problem that we face is how to actually use such excellent techniques in real RDBMSs. We propose to implement an efficient join technique by the data transfer approach. The approach makes a hook point inside an RDBMS internal, and pulls data streams from the operator pipeline in the RDBMS, and applies our original join operator to the data, and finally returns the result to the operator pipeline in the RDBMS. The result of the experiment showed that our proposed method achieved 1.42x speedup compared with PostgreSQL. Our code is available on GitHub.},
booktitle = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
pages = {271–277},
numpages = {7},
keywords = {Relational Database, Parallel Hash Join, PostgreSQL},
location = {Chiyoda, Tokyo, Japan},
series = {HPC Asia 2018}
}

@inproceedings{10.1145/3236024.3264600,
author = {Hua, Jinru and Zhang, Mengshi and Wang, Kaiyuan and Khurshid, Sarfraz},
title = {SketchFix: A Tool for Automated Program Repair Approach Using Lazy Candidate Generation},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264600},
doi = {10.1145/3236024.3264600},
abstract = {Manually locating and removing bugs in faulty program is often tedious and error-prone. A common automated program repair approach called generate-and-validate (G&amp;V) iteratively creates candidate fixes, compiles them, and runs these candidates against the given tests. This approach can be costly due to a large number of re-compilations and re-executions of the program. To tackle this limitation, recent work introduced the SketchFix approach that tightly integrates the generation and validation phases, and utilizes runtime behaviors to substantially prune a large amount of repair candidates. This tool paper describes our Java implementation of SketchFix, which is an open-source library that we released on Github. Our experimental evaluation using Defects4J benchmark shows that SketchFix can significantly reduce the number of re-compilations and re-executions compared to other approaches and work particularly well in repairing expression manipulation at the AST node-level granularity.The demo video is at: https://youtu.be/AO-YCH8vGzQ.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {888–891},
numpages = {4},
keywords = {Program Synthesis, Program Repair, Program Sketching, SketchFix},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ESEM.2017.46,
author = {Nayebi, Maleknaz and Farrahi, Homayoon and Ruhe, Guenther},
title = {Which Version Should Be Released to App Store?},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.46},
doi = {10.1109/ESEM.2017.46},
abstract = {Background: Several mobile app releases do not find their way to the end users. Our analysis of 11,514 releases across 917 open source mobile apps revealed that 44.3% of releases created in GitHub never shipped to the app store (market). Aims: We introduce "marketability" of open source mobile apps as a new release decision problem. Considering app stores as a complex system with unknown treatments, we evaluate performance of predictive models and analogical reasoning for marketability decisions. Method: We performed a survey with 22 release engineers to identify the importance of marketability release decision. We compared different classifiers to predict release marketability. For guiding the transition of not successfully marketable releases into successful ones, we used analogical reasoning. We evaluated our results both internally (over time) and externally (by developers). Results: Random forest classification performed best with F1 score of 78%. Analyzing 58 releases over time showed that, for 81% of them, analogical reasoning could correctly identify changes in the majority of release attributes. A survey with seven developers showed the usefulness of our method for supporting real world decisions. Conclusions: Marketability decisions of mobile apps can be supported by using predictive analytics and by considering and adopting similar experience from the past.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {324–333},
numpages = {10},
keywords = {empirical study, release management, survey, marketability, mobile apps, analogical reasoning},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.5555/3154768.3154773,
author = {Unruh, Tommi and Shastry, Bhargava and Skoruppa, Malte and Maggi, Federico and Rieck, Konrad and Seifert, Jean-Pierre and Yamaguchi, Fabian},
title = {Leveraging Flawed Tutorials for Seeding Large-Scaleweb Vulnerability Discovery},
year = {2017},
publisher = {USENIX Association},
address = {USA},
abstract = {The Web is replete with tutorial-style content on how to accomplish programming tasks. Unfortunately, even topranked tutorials suffer from severe security vulnerabilities, such as cross-site scripting (XSS), and SQL injection (SQLi). Assuming that these tutorials influence real-world software development, we hypothesize that code snippets from popular tutorials can be used to bootstrap vulnerability discovery at scale. To validate our hypothesis, we propose a semi-automated approach to find recurring vulnerabilities starting from a handful of top-ranked tutorials that contain vulnerable code snippets. We evaluate our approach by performing an analysis of tens of thousands of open-source web applications to check if vulnerabilities originating in the selected tutorials recur. Our analysis framework has been running on a standard PC, analyzed 64,415 PHP codebases hosted on GitHub thus far, and found a total of 117 vulnerabilities that have a strong syntactic similarity to vulnerable code snippets present in popular tutorials. In addition to shedding light on the anecdotal belief that programmers reuse web tutorial code in an ad hoc manner, our study finds disconcerting evidence of insufficiently reviewed tutorials compromising the security of open-source projects. Moreover, our findings testify to the feasibility of large-scale vulnerability discovery using poorly written tutorials as a starting point.},
booktitle = {Proceedings of the 11th USENIX Conference on Offensive Technologies},
pages = {5},
numpages = {1},
location = {Vancouver, BC, Canada},
series = {WOOT'17}
}

@inproceedings{10.1145/3092703.3092731,
author = {Zhang, Mengshi and Li, Xia and Zhang, Lingming and Khurshid, Sarfraz},
title = {Boosting Spectrum-Based Fault Localization Using PageRank},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092731},
doi = {10.1145/3092703.3092731},
abstract = { Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.  We present PRFL, a lightweight technique that boosts spectrum-based fault localization by differentiating tests using PageRank algorithm. Given the original program spectrum information, PRFL uses PageRank to recompute the spectrum information by considering the contributions of different tests. Then, traditional SBFL techniques can be applied on the recomputed spectrum information to achieve more effective fault localization. Although simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 42% more real faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 2 minute average extra overhead on real faults) on 357 real faults from 5 Defects4J projects and 30692 artificial (i.e., mutation) faults from 87 GitHub projects, demonstrating a promising future for considering the contributions of different tests during fault localization. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–272},
numpages = {12},
keywords = {Spectrum-based fault localization, Software testing, PageRank},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/3368089.3417928,
author = {Song, Yang and Chaparro, Oscar},
title = {BEE: A Tool for Structuring and Analyzing Bug Reports},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417928},
doi = {10.1145/3368089.3417928},
abstract = {This paper introduces BEE, a tool that automatically analyzes user-written bug reports and provides feedback to reporters and developers about the system’s observed behavior (OB), expected behavior (EB), and the steps to reproduce the bug (S2R). BEE employs machine learning to (i) detect if an issue describes a bug, an enhancement, or a question; (ii) identify the structure of bug descriptions by automatically labeling the sentences that correspond to the OB, EB, or S2R; and (iii) detect when bug reports fail to provide these elements. BEE is integrated with GitHub and offers a public web API that researchers can use to investigate bug management tasks based on bug reports. We evaluated BEE’s underlying models on more than 5k existing bug reports and found they can correctly detect OB, EB, and S2R sentences as well as missing information in bug reports. BEE is an open-source project that can be found at <a>https://git.io/JfFnN</a>. A screencast showing the full capabilities of BEE can be found at <a>https://youtu.be/8pC48f_hClw</a>.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1551–1555},
numpages = {5},
keywords = {text analysis, bug report structure, Bug reporting, bug report quality},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3134271.3134292,
author = {Zhu, Shaochen and Wang, Betty},
title = {Predicative Model for Uber Ridership in New York City},
year = {2017},
isbn = {9781450352765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134271.3134292},
doi = {10.1145/3134271.3134292},
abstract = {Objective: This study aimed to build a predictive model for Uber ridership in New York City.Data: Uber rides data is downloaded from GitHub. The data were collected during January 2015 to June 2015 and had ridership information for all the five boroughs of New York City. We used a random sample of 50% whole data to build a predictive model, and used the other 50% to validate the model and further used bootstrap data for model validation. Mean squared errors (MSE) were calculated. The predicted riders and the observed riders were compared to measure the performance of the predictive model. All the analysis was done using free statistical software R.Results: A total of 20490 observations including hourly ridership information were extracted for this study. A total of 10245 observations were selected in training sample for model building, and 10245 in test sample, and 500000 in bootstrap sample which was based on test sample were used for model validation. A total of 7171685 riders were in training sample, 7092530 in test sample, and 345936605 in bootstrap sample. The predicted risers were 7171685 riders in training sample, 7134560 in test sample, and 348357233 in bootstrap sample. The predictive model performed well in the split sample which was not used for model building and bootstrap sample. The MSE was 142,866 in training sample, and 141,142 in test sample and 141480 in bootstrap sample. The observed ridership and predicted ridership were close to each other in each month of Jan-June, in each hour, in each week day, and in each district in New York city.Conclusions: A predictive model was built and validated using public available data for Uber ridership in New York city. It could be used to predict the business opportunities for Uber and help to make an informed decision regarding resource allocation.},
booktitle = {Proceedings of the International Conference on Business and Information Management},
pages = {112–115},
numpages = {4},
keywords = {Bootstrap, Predicative Model, Uber Ridership, New York City, MSE (Mean Squared Errors)},
location = {Bei Jing, China},
series = {ICBIM 2017}
}

@inproceedings{10.5555/3155562.3155583,
author = {Jiang, Siyuan and Armaly, Ameer and McMillan, Collin},
title = {Automatically Generating Commit Messages from Diffs Using Neural Machine Translation},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically ``translate'' diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead. },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {135–146},
numpages = {12},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3368089.3418539,
author = {Wessel, Mairieli},
title = {Enhancing Developers’ Support on Pull Requests Activities with Software Bots},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3418539},
doi = {10.1145/3368089.3418539},
abstract = {Software bots are employed to support developers' activities, serving as conduits between developers and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save development cost, time, and effort, the bots' presence can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers enhance existing bots. Toward this end, we are interviewing maintainers, contributors, and bot developers to understand the problems in the human-bot interaction and how they affect the collaboration in a project. Afterward, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1674–1677},
numpages = {4},
keywords = {Open-source Software, GitHub Bots, Software Bots},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3107411.3108203,
author = {Dunn, Tamsen and Berry, Gwenn and Emig-Agius, Dorothea and Jiang, Yu and Iyer, Anita and Udar, Nitin and Str\"{o}mberg, Michael},
title = {Pisces: An Accurate and Versatile Single Sample Somatic and Germline Variant Caller},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3108203},
doi = {10.1145/3107411.3108203},
abstract = {A method for robustly and accurately detecting rare DNA mutations in tumor samples is critical to cancer research. Because many clinical tissue repositories have only FFPE-degraded tumor samples, and no matched normal sample from healthy tissue available, being able to discriminate low frequency mutations from background noise in the absence of a matched normal sample is of particular importance to research. Current state of the art variant callers such as GATK and VarScan focus on germline variant calling (used for detecting inherited mutations following a Mendelian inheritance pattern) or, in the case of FreeBayes and MuTect, focus on tumor-normal joint variant calling (using the normal sample to help discriminate low frequency somatic mutations from back ground noise). We present Pisces, a tumor-only variant caller exclusively developed at Illumina for detecting low frequency mutations from next generation sequencing data. Pisces has been an integral part of the Illumina Truseq Amplicon workflow since 2012, and is available on BaseSpace and on the MiSeq sequencing platforms. Pisces has been available to the public on github, since 2015. (https://github.com/Illumina/Pisces) Since that time, the Pisces variant calling team have continued to develop Pisces, and have made available a suite of variant calling tools, including a ReadStitcher, Variant Phaser, and Variant Quality Recalibration tool, to be used along with the core variant caller, Pisces. Here, we describe the Pisces variant calling tools and core algorithms. We describe the common use cases for Pisces (not necessarily restricted to somatic variant calling). We also evaluate Pisces performance on somatic and germline datasets, both from the titration of well characterized samples, and from a corpus of 500 FFPE-treated clinical trial tumor samples, against other variant callers. Our results show that Pisces gives highly accurate results in a variety of contexts. We recommend Pisces for amplicon somatic and germline variant calling.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {595},
numpages = {1},
keywords = {somatic, single sample, variant caller, tumor only, pisces, germline},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/2858036.2858479,
author = {Xu, Pingmei and Sugano, Yusuke and Bulling, Andreas},
title = {Spatio-Temporal Modeling and Prediction of Visual Attention in Graphical User Interfaces},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858479},
doi = {10.1145/2858036.2858479},
abstract = {We present a computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces. Like existing models of bottom-up visual attention in computer vision, our model does not require any eye tracking equipment. Instead, it predicts attention solely using information available to the interface, specifically users' mouse and keyboard input as well as the UI components they interact with. To study our model in a principled way, we further introduce a method to synthesize user interface layouts that are functionally equivalent to real-world interfaces, such as from Gmail, Facebook, or GitHub. We first quantitatively analyze attention allocation and its correlation with user input and UI components using ground-truth gaze, mouse, and keyboard data of 18 participants performing a text editing task. We then show that our model predicts attention maps more accurately than state-of-the-art methods. Our results underline the significant potential of spatio-temporal attention modeling for user interface evaluation, optimization, or even simulation.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {3299–3310},
numpages = {12},
keywords = {visual attention, interactive environment, physical action, saliency, spatio-temporal modeling, graphical user interfaces},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{10.1145/3368926.3369733,
author = {Nguyen, Ngoc-Hoa and Le, Viet-Ha and Phung, Van-On and Du, Phuong-Hanh},
title = {Toward a Deep Learning Approach for Detecting PHP Webshell},
year = {2019},
isbn = {9781450372459},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368926.3369733},
doi = {10.1145/3368926.3369733},
abstract = {The most efficient way of securing Web applications is searching and eliminating threats therein (from both malwares and vulnerabilities). In case of having Web application source codes, Web security can be improved by performing the task to detecting malicious codes, such as Web shells. In this paper, we proposed a model using a deep learning approach to detect and identify the malicious codes inside PHP source files. Our method relies on (i) pattern matching techniques by applying Yara rules to build a malicious and benign datasets, (ii) converting the PHP source codes to a numerical sequence of PHP opcodes and (iii) applying the Convolutional Neural Network model to predict a PHP file whether embedding a malicious code such as a webshell. Thus, we validate our approach with different webshell collections from reliable source published in Github. The experiment results show that the proposed method achieved the accuracy of 99.02% with 0.85% false positive rate.},
booktitle = {Proceedings of the Tenth International Symposium on Information and Communication Technology},
pages = {514–521},
numpages = {8},
keywords = {yara rules, opcode sequence, CNN, pattern matching, deep learning, webshell detection},
location = {Hanoi, Ha Long Bay, Viet Nam},
series = {SoICT 2019}
}

@inproceedings{10.1109/CCGRID.2018.00090,
author = {Beineke, Kevin and Nothaas, Stefan and Sch\"{o}ttner, Michael},
title = {Efficient Messaging for Java Applications Running in Data Centers},
year = {2018},
isbn = {9781538658154},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CCGRID.2018.00090},
doi = {10.1109/CCGRID.2018.00090},
abstract = {Big data and large-scale Java applications often aggregate the resources of many servers. Low-latency and high-throughput network communication is important, if the applications have to process many concurrent interactive queries. We designed DXNet to address these challenges providing fast object de-/serialization, automatic connection management and zero-copy messaging. The latter includes sending of asynchronous messages as well as synchronous requests/responses and an event-driven message receiving approach. DXNet is optimized for small messages (&lt; 64 bytes) in order to support highly interactive web applications, e.g., graph-based information retrieval, but works well with larger messages (e.g., 8 MB) as well. DXNet is available as standalone component on Github and its modular design is open for different transports currently supporting Ethernet and InfiniBand. The evaluation with micro benchmarks and YCSB using Ethernet and InfiniBand shows request-response latencies sub 10 μs (round-trip) including object de-/serialization, as well as a maximum throughput of more than 9 GByte/s.},
booktitle = {Proceedings of the 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing},
pages = {589–598},
numpages = {10},
keywords = {Java, ethernet networks, data centers, InfiniBand, message passing, cloud computing},
location = {Washington, District of Columbia},
series = {CCGrid '18}
}

@inproceedings{10.1145/2889160.2889244,
author = {Rahman, Mohammad Masudur and Roy, Chanchal K. and Collins, Jason A.},
title = {CoRReCT: Code Reviewer Recommendation in GitHub Based on Cross-Project and Technology Experience},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889244},
doi = {10.1145/2889160.2889244},
abstract = {Peer code review locates common coding rule violations and simple logical errors in the early phases of software development, and thus reduces overall cost. However, in GitHub, identifying an appropriate code reviewer for a pull request is a non-trivial task given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation technique that considers not only the relevant cross-project work history (e.g., external library experience) but also the experience of a developer in certain specialized technologies associated with a pull request for determining her expertise as a potential code reviewer. We first motivate our technique using an exploratory study with 10 commercial projects and 10 associated libraries external to those projects. Experiments using 17,115 pull requests from 10 commercial projects and six open source projects show that our technique provides 85%-- 92% recommendation accuracy, about 86% precision and 79%--81% recall in code reviewer recommendation, which are highly promising. Comparison with the state-of-the-art technique also validates the empirical findings and the superiority of our recommendation technique.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {222–231},
numpages = {10},
keywords = {GitHub, pull request, specialized technology experience, code reviewer recommendation, cross-project experience},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.5555/2663360.2663368,
author = {Rigby, Peter C. and Barr, Earl T. and Bird, Christian and Devanbu, Prem and German, Daniel M.},
title = {What Effect Does Distributed Version Control Have on OSS Project Organization?},
year = {2013},
isbn = {9781467364416},
publisher = {IEEE Press},
abstract = {Many Open Source Software (OSS) projects are moving form Centralized Version Control (CVC) to Distributed Version Control (DVC). The effect of this shift on project organization and developer collaboration is not well understood. In this paper, we use a theoretical argument to evaluate the appropriateness of using DVC in the context of two very common organization forms in OSS: a dictatorship and a peer group. We find that DVC facilitates large hierarchical communities as well as smaller groups of developers, while CVC allows for consensus-building by a peer group. We also find that the flexibility of DVC systems allows for diverse styles of developer collaboration. With CVC, changes flow up and down (and publicly) via a central repository. In contrast, DVC facilitates collaboration in which work output can flow sideways (and privately) between collaborators, with no repository being inherently more important or central. These sideways flows are a relatively new concept. Developers on the Linux project, who tend to be experienced DVC users, cluster around "sandboxes:" repositories where developers can work together on a particular topic, isolating their changes from other developers. In this work, we focus on two large, mature OSS projects to illustrate these findings. However, we suggest that social media sites like GitHub may engender other original styles of collaboration that deserve further study.},
booktitle = {Proceedings of the 1st International Workshop on Release Engineering},
pages = {29–32},
numpages = {4},
location = {San Francisco, California},
series = {RELENG '13}
}

@inproceedings{10.1109/NOMS47738.2020.9110380,
author = {Hauser, Frederik and Schmidt, Mark and Menth, Michael},
title = {XRAC: Execution and Access Control for Restricted Application Containers on Managed Hosts},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/NOMS47738.2020.9110380},
doi = {10.1109/NOMS47738.2020.9110380},
abstract = {We propose xRAC to permit users to run special applications on managed hosts and to grant them access to protected network resources. We use restricted application containers (RACs) for that purpose. A RAC is a virtualization container with only a selected set of applications. Authentication verifies the RAC user’s identity and the integrity of the RAC image. If the user is permitted to use the RAC on a managed host, launching the RAC is authorized and access to protected network resources may be given, e.g., to internal networks, servers, or the Internet. xRAC simplifies traffic control as the traffic of a RAC has a unique IPv6 address so that it can be easily identified in the network. The architecture of xRAC reuses standard technologies, protocols, and infrastructure. Those are the Docker virtualization platform and 802.1X including EAP-over-UDP and RADIUS. Thus, xRAC improves network security without modifying core parts of applications, hosts, and infrastructure. In this paper, we review the technological background of xRAC, explain its architecture, discuss selected use cases, and investigate on the performance. To demonstrate the feasibility of xRAC, we implement it based on standard components with only a few modifications. Finally, we validate xRAC through experiments. We publish the testbed setup guide and prototypical implementation on GitHub [1].},
booktitle = {NOMS 2020 - 2020 IEEE/IFIP Network Operations and Management Symposium},
pages = {1–9},
numpages = {9},
location = {Budapest, Hungary}
}

@inproceedings{10.1145/3338906.3338907,
author = {Rigger, Manuel and Marr, Stefan and Adams, Bram and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Understanding GCC Builtins to Develop Better Tools},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338907},
doi = {10.1145/3338906.3338907},
abstract = {C programs can use compiler builtins to provide functionality that the C language lacks. On Linux, GCC provides several thousands of builtins that are also supported by other mature compilers, such as Clang and ICC. Maintainers of other tools lack guidance on whether and which builtins should be implemented to support popular projects. To assist tool developers who want to support GCC builtins, we analyzed builtin use in 4,913 C projects from GitHub. We found that 37% of these projects relied on at least one builtin. Supporting an increasing proportion of projects requires support of an exponentially increasing number of builtins; however, implementing only 10 builtins already covers over 30% of the projects. Since we found that many builtins in our corpus remained unused, the effort needed to support 90% of the projects is moderate, requiring about 110 builtins to be implemented. For each project, we analyzed the evolution of builtin use over time and found that the majority of projects mostly added builtins. This suggests that builtins are not a legacy feature and must be supported in future tools. Systematic testing of builtin support in existing tools revealed that many lacked support for builtins either partially or completely; we also discovered incorrect implementations in various tools, including the formally verified CompCert compiler.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {74–85},
numpages = {12},
keywords = {C GitHub projects, compiler intrinsics, GCC builtins},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3219104.3229268,
author = {Chen, Huan and Fietkiewicz, Chris},
title = {Version Control Graphical Interface for Open OnDemand},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229268},
doi = {10.1145/3219104.3229268},
abstract = {Use of high performance computing (HPC) clusters is challenging for the average researcher who is not familiar with the necessary tools such as the Linux operating system and job management software. Of those using HPC systems, the majority are not trained specifically in computer programming. Nearly 70% of users on the NSF-funded XSEDE system are in fields other than computer science and computing science. The graphical user interfaces (GUIs) that most users rely on for common software on personal computers are typically not available on HPCs. The Ohio Supercomputer Center has addressed these issues through the development of an open source, web-based GUI called Open OnDemand. Because it is open source, administrators are free to deploy it on their own systems, and developers are free to enhance it. To improve workforce development and diversity, we sought to: (1) install Open OnDemand on a private cluster and (2) develop a custom add-on module for version control of HPC application software. We successfully installed and configured Open OnDemand for a private cluster at Case Western Reserve University in order to evaluate the difficulty level of deployment. Despite our lack of experience in HPC system administration, the installation process was straight forward due to a streamlined installer package and thorough documentation. In order to evaluate the extensibility of Open OnDemand, we developed a custom web-based interface for use of the popular Git version control system. Version control tools are important in maintaining software by easily tracking and accessing file changes in both single- and multi-developer projects. Our module successfully integrates with the existing Open OnDemand interface and provides common version control operations that can be used during typical HPC workflows. It is our hope that making version control software easier to use will encourage HPC users to adopt the use of version control into their own workflows to improve productivity and repeatability. Source code for the app will be made available on the author's github site.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {103},
numpages = {4},
keywords = {graphical user interface, high performance computing, version control},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.5555/2820518.2820554,
author = {Burlet, Gregory and Hindle, Abram},
title = {An Empirical Study of End-User Programmers in the Computer Music Community},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and interviews with individuals in this end-user community. Based on this analysis and feedback from computer musicians we find that there are many avenues where software engineering can be applied to help aid this community of end-user programmers.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {292–302},
numpages = {11},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/2745802.2745833,
author = {Romo, Bilyaminu Auwal and Capiluppi, Andrea},
title = {Towards an Automation of the Traceability of Bugs from Development Logs: A Study Based on Open Source Software},
year = {2015},
isbn = {9781450333504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2745802.2745833},
doi = {10.1145/2745802.2745833},
abstract = {Context: Information and tracking of defects can be severely incomplete in almost every Open Source project, resulting in a reduced traceability of defects into the development logs (i.e., version control commit logs). In particular, defect data often appears not in sync when considering what developers logged as their actions. Synchronizing or completing the missing data of the bug repositories, with the logs detailing the actions of developers, would benefit various branches of empirical software engineering research: prediction of software faults, software reliability, traceability, software quality, effort and cost estimation, bug prediction and bug fixing.Objective: To design a framework that automates the process of synchronizing and filling the gaps of the development logs and bug issue data for open source software projects.Method: We instantiate the framework with a sample of OSS projects from GitHub, and by parsing, linking and filling the gaps found in their bug issue data, and development logs. UML diagrams show the relevant modules that will be used to merge, link and connect the bug issue data with the development data.Results: Analysing a sample of over 300 OSS projects we observed that around 1/2 of bug-related data is present in either development logs or issue tracker logs: the rest of the data is missing from one or the other source. We designed an automated approach that fills the gaps of either source by making use of the available data, and we successfully mapped all the missing data of the analysed projects, when using one heuristics of annotating bugs. Other heuristics need to be investigated and implemented.Conclusion: In this paper a framework to synchronise the development logs and bug data used in empirical software engineering was designed to automatically fill the missing parts of development logs and bugs of issue data.},
booktitle = {Proceedings of the 19th International Conference on Evaluation and Assessment in Software Engineering},
articleno = {33},
numpages = {6},
keywords = {bug accuracy, bug traceability, bug-fixing commits},
location = {Nanjing, China},
series = {EASE '15}
}

@inproceedings{10.1145/3319535.3363283,
author = {Alam, Aftab and Krombholz, Katharina and Bugiel, Sven},
title = {Poster: Let History Not Repeat Itself (This Time) -- Tackling WebAuthn Developer Issues Early On},
year = {2019},
isbn = {9781450367479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319535.3363283},
doi = {10.1145/3319535.3363283},
abstract = {The FIDO2 open authentication standard, developed jointly by the FIDO Alliance and the W3C, provides end-users with the means to use public-key cryptography in addition to or even instead of text-based passwords for authentication on the web. Its WebAuthn protocol has been adopted by all major browser vendors and recently also by major service providers (e.g., Google, GitHub, Dropbox, Microsoft, and others). Thus, FIDO2 is a very strong contender for finally tackling the problem of insecure user authentication on the web. However, there remain a number of open questions to be answered for FIDO2 to succeed as expected. In this poster, we focus specifically on the critical question of how well web-service developers can securely roll out WebAuthn in their own services and which issues have to be tackled to help developers in this task. The past has unfortunately shown that software developers struggle with correctly implementing or using security-critical APIs, such as TLS/SSL, password storage, or cryptographic APIs. We report here on ongoing work that investigates potential problem areas and concrete pitfalls for adopters of WebAuthn and tries to lay out a plan of how our community can help developers. We believe that raising awareness for foreseeable developer problems and calling for action to support developers early on is critical on the path for establishing FIDO2 as a de-facto authentication solution.},
booktitle = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2669–2671},
numpages = {3},
keywords = {webauthn, fido2, usable security for developers},
location = {London, United Kingdom},
series = {CCS '19}
}

@inproceedings{10.1145/3406865.3418368,
author = {Wessel, Mairieli},
title = {Leveraging Software Bots to Enhance Developers' Collaboration in Online Programming Communities},
year = {2020},
isbn = {9781450380591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3406865.3418368},
doi = {10.1145/3406865.3418368},
abstract = {Software bots are applications that are integrated into human communication channels, serving as an interface between users and other tools. Due to their focus on task automation, bots have become particularly relevant for Open Source Software (OSS) projects hosted on GitHub. While bots are adopted to save developers' costs, time, and effort, the interaction of these bots can be disruptive to the community. My research goal is two-fold: (i) identify problems caused by bots that interact in pull requests, and (ii) help bot designers to enhance existing bots, thereby improving the partnership with contributors and maintainers. Toward this end, we are interviewing developers to understand what are the problems on the human-bot interaction and how they affect human collaboration. Afterwards, we will employ Design Fiction to capture the developers' vision of bots' capabilities, in order to define guidelines for the design of bots on social coding platforms, and derive requirements for a meta-bot to deal with the problems. This work contributes more broadly to the design and use of software bots to enhance developers' collaboration and interaction.},
booktitle = {Conference Companion Publication of the 2020 on Computer Supported Cooperative Work and Social Computing},
pages = {183–188},
numpages = {6},
keywords = {open source software, software engineering, github bots, software bots},
location = {Virtual Event, USA},
series = {CSCW '20 Companion}
}

@inproceedings{10.1145/3017680.3017822,
author = {Brown, Neil C.C. and Altadmri, Amjad},
title = {What's New in BlueJ 4: Git, Stride and More (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3017822},
doi = {10.1145/3017680.3017822},
abstract = {BlueJ is a beginner's IDE for Java which has been in popular use for over ten years. But it continues to improve and evolve: BlueJ 4.0.0 was recently released with several new features. Git support has been added in a user-friendly way, and the support for writing JavaFX GUI applications has been improved. BlueJ 4 also includes the frame-based Stride editor (previously seen in Greenfoot), which allows for block-like programming. BlueJ 4 also retains all its existing functionality such as interactive object creation and method invocation, a "REPL"-like code pad, a debugger and testing support. This workshop, run by the developers of BlueJ, will take the participants, whether new to BlueJ and Java or long-time users, through the new features while also providing an introduction/refresher on the existing capabilities of the software. Participants will learn how to share BlueJ projects via Github, create a new JavaFX application, dabble with Stride and get a tour of the existing BlueJ functionality. A laptop with BlueJ 4.0 installed is required.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {734},
numpages = {1},
keywords = {Git, stride, BlueJ, javafx, java},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1109/GLOBECOM38437.2019.9013941,
author = {Yan, Haonan and Li, Hui and Xiao, Mingchi and Dai, Rui and Zheng, Xianchun and Zhao, Xingwen and Li, Fenghua},
title = {PGSM-DPI: Precisely Guided Signature Matching of Deep Packet Inspection for Traffic Analysis},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/GLOBECOM38437.2019.9013941},
doi = {10.1109/GLOBECOM38437.2019.9013941},
abstract = {In the field of network traffic analysis, Deep Packet Inspection (DPI) technology is widely used at present. However, the increase in network traffic has brought tremendous processing pressure on the DPI. Consequently, detection speed has become the bottleneck of the entire application. In order to speed up the traffic detection of DPI, a lot of research works have been applied to improve signature matching algorithms, which is the most influential factor in DPI performance. In this paper, we present a novel method from a different angle called Precisely Guided Signature Matching (PGSM). Instead of matching packets with signature directly, we use supervised learning to automate the rules of specific protocol in PGSM. By testing the performance of a packet in the rules, the target packet could be decided when and which signatures should be matched with. Thus, the PGSM method reduces the number of aimless matches which are useless and numerous. After proposing PGSM, we build a framework called PGSM-DPI to verify the effectiveness of guidance rules. The PGSM-DPI framework consists of PGSM method and open source DPI library. The framework is running on a distributed platform with better throughput and computational performance. Finally, the experimental results demonstrate that our PGSM-DPI can reduce 59.23% original DPI time and increase 21.31% throughput. Besides, all source codes and experimental results can be accessed on our GitHub.},
booktitle = {2019 IEEE Global Communications Conference (GLOBECOM)},
pages = {1–6},
numpages = {6},
location = {Waikoloa, HI, USA}
}

@inproceedings{10.1145/2642937.2643002,
author = {Campos, Jos\'{e} and Arcuri, Andrea and Fraser, Gordon and Abreu, Rui},
title = {Continuous Test Generation: Enhancing Continuous Integration with Automated Test Generation},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643002},
doi = {10.1145/2642937.2643002},
abstract = {In object oriented software development, automated unit test generation tools typically target one class at a time. A class, however, is usually part of a software project consisting of more than one class, and these are subject to changes over time. This context of a class offers significant potential to improve test generation for individual classes. In this paper, we introduce Continuous Test Generation (CTG), which includes automated unit test generation during continuous integration (i.e., infrastructure that regularly builds and tests software projects). CTG offers several benefits: First, it answers the question of how much time to spend on each class in a project. Second, it helps to decide in which order to test them. Finally, it answers the question of which classes should be subjected to test generation in the first place. We have implemented CTG using the EvoSuite unit test generation tool, and performed experiments using eight of the most popular open source projects available on GitHub, ten randomly selected projects from the SF100 corpus, and five industrial projects. Our experiments demonstrate improvements of up to +58% for branch coverage and up to +69% for thrown undeclared exceptions, while reducing the time spent on test generation by up to +83%.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {55–66},
numpages = {12},
keywords = {continuous integration, unit testing, automated test generation, continuous testing},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/3379597.3387442,
author = {Pfeiffer, Rolf-Helge},
title = {What Constitutes Software? An Empirical, Descriptive Study of Artifacts},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387442},
doi = {10.1145/3379597.3387442},
abstract = {The term software is ubiquitous, however, it does not seem as if we as a community have a clear understanding of what software actually is. Imprecise definitions of software do not help other professions, in particular those acquiring and sourcing software from third-parties, when deciding what precisely are potential deliverables. In this paper we investigate which artifacts constitute software by analyzing 23 715 repositories from Github, we categorize the found artifacts into high-level categories, such as, code, data, and documentation (and into 19 more concrete categories) and we can confirm the notion of others that software is more than just source code or programs, for which the term is often used synonymously. With this work we provide an empirical study of more than 13 million artifacts, we provide a taxonomy of artifact categories, and we can conclude that software most often consists of variously distributed amounts of code in different forms, such as source code, binary code, scripts, etc., data, such as configuration files, images, databases, etc., and documentation, such as user documentation, licenses, etc.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {481–491},
numpages = {11},
keywords = {software, empirical study, artifacts},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1145/3349266.3351392,
author = {White, Laurie and Engelke, Charles},
title = {Serverless Distributed Architecture by Incremental Examples},
year = {2019},
isbn = {9781450369213},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3349266.3351392},
doi = {10.1145/3349266.3351392},
abstract = {Cloud computing has the potential to be a great equalizer, providing powerful and sophisticated computing tools to anyone with an internet connection and even a minimal budget. In fact, educational institutions can often get cloud grants providing access at no cost. Although any traditional computing paradigm can be implemented with cloud computing, there are now new and potentially more useful options available. This workshop is a hands-on exploration of several of those new paradigms in a single gamifiable system. Participants will explore cloud computing through a game-playing case study. Concepts covered include serverless computing, distributed systems, event-driven software, message passing, asynchronous communication, and non-relational databases. Each new idea will be used to advance the example system a step at a time. The workshop will emphasize ways to incorporate the case study in class and possibilities for extending it. All code for the system is available on GitHub for participants to work with to implement their own games or extend the concepts in other ways, such as a programming assignment submission and scoring system.},
booktitle = {Proceedings of the 20th Annual SIG Conference on Information Technology Education},
pages = {138–139},
numpages = {2},
keywords = {serverless computing, functions as a service, case study, cloud, event driven},
location = {Tacoma, WA, USA},
series = {SIGITE '19}
}

@inproceedings{10.1109/MSR.2019.00021,
author = {Rahman, Musfiqur and Rigby, Peter C and Palani, Dharani and Nguyen, Tien},
title = {Cleaning StackOverflow for Machine Translation},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00021},
doi = {10.1109/MSR.2019.00021},
abstract = {Generating source code API sequences from an English query using Machine Translation (MT) has gained much interest in recent years. For any kind of MT, the model needs to be trained on a parallel corpus. In this paper we clean StackOverflow, one of the most popular online discussion forums for programmers, to generate a parallel English-Code corpus from Android posts. We contrast three data cleaning approaches: standard NLP, title only, and software task extraction. We evaluate the quality of the each corpus for MT. To provide indicators of how useful each corpus will be for machine translation, we provide researchers with measurements of the corpus size, percentage of unique tokens, and per-word maximum likelihood alignment entropy. We have used these corpus cleaning approaches to translate between English and Code [22, 23], to compare existing SMT approaches from word mapping to neural networks [24], and to re-examine the "natural software" hypothesis [29]. After cleaning and aligning the data, we create a simple maximum likelihood MT model to show that English words in the corpus map to a small number of specific code elements. This model provides a basis for the success of using StackOverflow for search and other tasks in the software engineering literature and paves the way for MT. Our scripts and corpora are publicly available on GitHub [1] as well as at https://search.datacite.org/works/10.5281/zenodo.2558551.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {79–83},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3196398.3196437,
author = {Accioly, Paola and Borba, Paulo and Silva, L\'{e}uson and Cavalcanti, Guilherme},
title = {Analyzing Conflict Predictors in Open-Source Java Projects},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196437},
doi = {10.1145/3196398.3196437},
abstract = {In collaborative development environments integration conflicts occur frequently. To alleviate this problem, different awareness tools have been proposed to alert developers about potential conflicts before they become too complex. However, there is not much empirical evidence supporting the strategies used by these tools. Learning about what types of changes most likely lead to conflicts might help to derive more appropriate requirements for early conflict detection, and suggest improvements to existing conflict detection tools. To bring such evidence, in this paper we analyze the effectiveness of two types of code changes as conflict predictors. Namely, editions to the same method, and editions to directly dependent methods. We conduct an empirical study analyzing part of the development history of 45 Java projects from GitHub and Travis CI, including 5,647 merge scenarios, to compute the precision and recall for the conflict predictors aforementioned. Our results indicate that the predictors combined have a precision of 57.99% and a recall of 82.67%. Moreover, we conduct a manual analysis which provides insights about strategies that could further increase the precision and the recall.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {576–586},
numpages = {11},
keywords = {awareness tools, precision and recall, collaborative development, conflict predictors},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3097983.3098101,
author = {Maurus, Samuel and Plant, Claudia},
title = {Let's See Your Digits: Anomalous-State Detection Using Benford's Law},
year = {2017},
isbn = {9781450348874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3097983.3098101},
doi = {10.1145/3097983.3098101},
abstract = {Benford's Law explains a curious phenomenon in which the leading digits of "naturally-occurring" numerical data are distributed in a precise fashion. In this paper we begin by showing that system metrics generated by many modern information systems like Twitter, Wikipedia, YouTube and GitHub obey this law. We then propose a novel unsupervised approach called BenFound that exploits this property to detect anomalous system events. BenFound tracks the "Benfordness" of key system metrics, like the follower counts of tweeting Twitter users or the change deltas in Wikipedia page edits. It then applies a novel Benford-conformity test in real-time to identify "non-Benford events". We investigate a variety of such events, showing that they correspond to unnatural and often undesirable system interactions like spamming, hashtag-hijacking and denial-of-service attacks. The result is a technically-uncomplicated and effective "red flagging" technique that can be used to complement existing anomaly-detection approaches. Although not without its limitations, it is highly efficient and requires neither obscure parameters, nor text streams, nor natural-language processing.},
booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {977–986},
numpages = {10},
keywords = {nonparametric statistical tests, benford's law, data streams, time series data, anomaly detection},
location = {Halifax, NS, Canada},
series = {KDD '17}
}

@inproceedings{10.1145/3388440.3414907,
author = {Nowling, Ronald J. and Beal, Christopher R. and Emrich, Scott and Behura, Susanta K. and Halfon, Marc S. and Duman-Scheel, Molly},
title = {PeakMatcher: Matching Peaks Across Genome Assemblies},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3414907},
doi = {10.1145/3388440.3414907},
abstract = {When reference genome assemblies are updated, the peaks from DNA enrichment assays such as ChIP-Seq and FAIRE-Seq need to be called again using the new genome assembly. PeakMatcher is an open-source package that aids in validation by matching peaks across two genome assemblies using the alignment of reads or within the same genome. PeakMatcher calculates recall and precision while also outputting lists of peak-to-peak matches.PeakMatcher uses read alignments to match peaks across genome assemblies. PeakMatcher finds all read aligned to one genome that overlap with a given list of peaks. PeakMatcher uses the read names to locate where those reads are aligned against a second genome. Lastly, all peaks called against the second genome that overlap with the aligned reads are found and output. PeakMatcher groups uses the peak-read-peak relationships to discover 1-to-1, 1-to-many, and many-to-many relationships. Overlap queries are performed with interval trees for maximum efficiency.We evaluated PeakMatcher on two data sets. The first data set was FAIRE-Seq (Formaldehyde-Assisted Isolation of Regulatory Elements Sequencing) of DNA isolated embyros of the mosquito Aedes aegypti [2, 4]. We implemented a peak calling pipeline and validated it on the older (highly fragmented) AaegL3 assembly [5]. PeakMatcher matched 92.9% (precision) of the 121,594 previously-called peaks from [2, 4] with 89.4% (recall) of the 124,959 peaks called with our new pipeline. Next, we applied the peak-calling pipeline to call FAIRE peaks using the newer, chromosome-complete AaegL5 assembly [3]. PeakMatcher found matches for 14 of the 16 experimentally-validated AaegL3 FAIRE peaks from [2, 4]. We validated the matches by comparing nearby genes across the genomes. Nearby genes were consistent for 11 of the 14 peaks; inconsistencies for at least two of the remaining peaks were clearly attributable to differences in assemblies. When applied to all of the peaks, Peak-Matcher matched 78.8% (precision) of the 124,959 AaegL3 peaks with 76.7% (recall) of the 128,307 AaegL5 peaks.The second data set was STARR-Seq (Self-Transcribing Active Regulatory Region Sequencing) of Drosophila melanogaster DNA in S2 culture cells [1]. We called STARR peaks against two versions (dm3 and r5.53) of the D. melanogaster genome [6]. PeakMatcher matched 77.4% (precision) of the 4,195 dm3 peaks with 94.8% (recall) of the 3,114 r5.53 peaks.PeakMatcher and associated documentation are available on GitHub (https://github.com/rnowling/peak-matcher) under the open-source Apache Software License v2. PeakMatcher was written in Python 3 using the intervaltree library.},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {70},
numpages = {1},
keywords = {genome assembly, peak calling, DNA enrichment assays},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1145/3238147.3240482,
author = {Hariri, Farah and Shi, August},
title = {SRCIROR: A Toolset for Mutation Testing of C Source Code and LLVM Intermediate Representation},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240482},
doi = {10.1145/3238147.3240482},
abstract = {We present SRCIROR (pronounced “sorcerer“), a toolset for performing mutation testing at the levels of C/C++ source code (SRC) and the LLVM compiler intermediate representation (IR). At the SRC level, SRCIROR identifies program constructs for mutation by pattern-matching on the Clang AST. At the IR level, SRCIROR directly mutates the LLVM IR instructions through LLVM passes. Our implementation enables SRCIROR to (1) handle any program that Clang can handle, extending to large programs with a minimal overhead, and (2) have a small percentage of invalid mutants that do not compile. SRCIROR enables performing mutation testing using the same classes of mutation operators at both the SRC and IR levels, and it is easily extensible to support more operators. In addition, SRCIROR can collect coverage to generate mutants only for covered code elements. Our tool is publicly available on GitHub (https://github.com/TestingResearchIllinois/srciror). We evaluate SRCIROR on Coreutils subjects. Our evaluation shows interesting differences between SRC and IR, demonstrating the value of SRCIROR in enabling mutation testing research across different levels of code representation.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {860–863},
numpages = {4},
keywords = {mutation testing, Software testing},
location = {Montpellier, France},
series = {ASE 2018}
}

@inproceedings{10.1145/3412569.3412575,
author = {Verma, Amit Arjun and Iyengar, S. R.S. and Setia, Simran and Dubey, Neeru},
title = {KDAP: An Open Source Toolkit to Accelerate Knowledge Building Research},
year = {2020},
isbn = {9781450387798},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412569.3412575},
doi = {10.1145/3412569.3412575},
abstract = {With the success of crowdsourced portals, such as Wikipedia, Stack Overflow, Quora, and GitHub, a class of researchers is driven towards understanding the dynamics of knowledge building on these portals. Even though collaborative knowledge building portals are known to be better than expert-driven knowledge repositories, limited research has been performed to understand the knowledge building dynamics in the former. This is mainly due to two reasons; first, unavailability of the standard data representation format, second, lack of proper tools and libraries to analyze the knowledge building dynamics.We describe Knowledge Data Analysis and Processing Platform (KDAP), a programming toolkit that is easy to use and provides high-level operations for analysis of knowledge data. We propose Knowledge Markup Language (Knol-ML), a standard representation format for the data of collaborative knowledge building portals. KDAP can process the massive data of crowdsourced portals like Wikipedia and Stack Overflow efficiently. As a part of this toolkit, a data-dump of various collaborative knowledge building portals is published in Knol-ML format. The combination of Knol-ML and the proposed open-source library will help the knowledge building community to perform benchmark analysis.URL:https://github.com/descentis/kdapSupplementary Material: https://bit.ly/2Z3tZK5},
booktitle = {Proceedings of the 16th International Symposium on Open Collaboration},
articleno = {1},
numpages = {11},
keywords = {datasets, open-source library, Knowledge Building, Wikipedia, Q&amp;A},
location = {Virtual conference, Spain},
series = {OpenSym 2020}
}

@inproceedings{10.1109/ICCPS.2018.00021,
author = {Schmittle, Matt and Lukina, Anna and Vacek, Lukas and Das, Jnaneshwar and Buskirk, Christopher P. and Rees, Stephen and Sztipanovits, Janos and Grosu, Radu and Kumar, Vijay},
title = {OpenUAV: A UAV Testbed for the CPS and Robotics Community},
year = {2018},
isbn = {9781538653012},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICCPS.2018.00021},
doi = {10.1109/ICCPS.2018.00021},
abstract = {Multirotor Unmanned Aerial Vehicles (UAV) have grown in popularity for research and education, overcoming challenges associated with fixed wing and ground robots. Unfortunately, extensive physical testing can be expensive and time consuming because of short flight times due to battery constraints and safety precautions. Simulation tools offer a low barrier to entry and enable testing and validation before field trials. However, most of the well-known simulators today have a high barrier to entry due to the need for powerful computers and the time required for initial set up. In this paper, we present OpenUAV, an open source test bed for UAV education and research that overcomes these barriers. We leverage the Containers as a Service (CaaS) technology to enable students and researchers carry out simulations on the cloud. We have based our framework on open-source tools including ROS, Gazebo, Docker, PX4, and Ansible, we designed the simulation framework so that it has no special hardware requirements. Two use-cases are presented. First, we show how a UAV can navigate around obstacles, and second, we test a multi-UAV swarm formation algorithm. To our knowledge, this is the first open-source, cloud-enabled testbed for UAVs. The code is available on GitHub: https://github.com/Open-UAV.},
booktitle = {Proceedings of the 9th ACM/IEEE International Conference on Cyber-Physical Systems},
pages = {130–139},
numpages = {10},
location = {Porto, Portugal},
series = {ICCPS '18}
}

@inproceedings{10.1109/AIM43001.2020.9159011,
author = {Wan, Fang and Wang, Haokun and Liu, Xiaobo and Yang, Linhan and Song, Chaoyang},
title = {DeepClaw: A Robotic Hardware Benchmarking Platform for Learning Object Manipulation<sup>*</sup>},
year = {2020},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/AIM43001.2020.9159011},
doi = {10.1109/AIM43001.2020.9159011},
abstract = {We present DeepClaw as a reconfigurable benchmark of robotic hardware and task hierarchy for robot learning. The DeepClaw benchmark aims at a mechatronics perspective of the robot learning problem, which features a minimum design of robot cell that can be easily reconfigured to host robot hardware from various vendors, including manipulators, grippers, cameras, desks, and objects, aiming at a streamlined collection of physical manipulation data and evaluation of the learned skills for hardware benchmarking. We provide a detailed design of the robot cell with readily available parts to build the experiment environment that can host a wide range of robotic hardware commonly adopted for robot learning. We propose a hierarchical pipeline of software integration, including localization, recognition, grasp planning, and motion planning, to streamline learning-based robot control, data collection, and experiment validation towards shareability and reproducibility. We present benchmarking results of the DeepClaw system for a baseline Tic-Tac-Toe task, a bin-clearing task, and a jigsaw puzzle task using three sets of standard robotic hardware. Our results show that tasks defined in DeepClaw can be easily reproduced on three robot cells. Under the same task setup, the differences in robotic hardware used will present a non-negligible impact on the performance metrics of robot learning. All design layouts and codes are hosted on Github for open access (https://github.com/bionicdl-sustech/DeepClaw).},
booktitle = {2020 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)},
pages = {2011–2018},
numpages = {8},
location = {Boston, MA, USA}
}

@inproceedings{10.1145/3144763.3144768,
author = {Amreen, Sadika and Mockus, Audris},
title = {Experiences on Clustering High-Dimensional Data Using PbdR},
year = {2017},
isbn = {9781450351355},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3144763.3144768},
doi = {10.1145/3144763.3144768},
abstract = {Motivation: Software engineering for High Performace Computing (HPC) environments in general [1] and for big data in particular [5] faces a set of unique challenges including high complexity of middleware and of computing environments. Tools that make it easier for scientists to utilize HPC are, therefore, of paramount importance. We provide an experience report of using one of such highly effective middleware pbdR [9] that allow the scientist to use R programming language without, at least nominally, having to master many layers of HPC infrastructure, such as OpenMPI [4] and ScalaPACK [2]. Objective: to evaluate the extent to which middleware helps improve scientist productivity, we use pbdR to solve a real problem that we, as scientists, are investigating. Our big data comes from the commits on GitHub and other project hosting sites and we are trying to cluster developers based on the text of these commit messages. Context: We need to be able to identify developer for every commit and to identify commits for a single developer. Developer identifiers in the commits, such as login, email, and name are often spelled in multiple ways since that information may come from different version control systems (Git, Mercurial, SVN, ...) and may depend on which computer is used (what is specified in .git/config of the home folder). Method: We train Doc2Vec [7] model where existing credentials are used as a document identifier and then use the resulting 200-dimensional vectors for the 2.3M identifiers to cluster these identifiers so that each cluster represents a specific individual. The distance matrix occupies 32TB and, therefore, is a good target for HPC in general and pbdR in particular. pbdR allows data to be distributed over computing nodes and even has implemented K-means and mixture-model clustering techniques in the package pmclust. Results: We used strategic prototyping [3] to evaluate the capabilities of pbdR and discovered that a) the use of middleware required extensive understanding of its inner workings thus negating many of the expected benefits; b) the implemented algorithms were not suitable for the particular combination of n, p, and k (sample size, data dimension, and the number of clusters); c) the development environment based on batch jobs increases development time substantially. Conclusions: In addition to finding from Basili et al., we find that the quality of the implementation of HPC infrastructure and its development environment has a tremendous effect on development productivity.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for High Performance Computing in Computational and Data-Enabled Science &amp; Engineering},
pages = {9–12},
numpages = {4},
keywords = {software engineering, developer productivity, prototyping},
location = {Denver, CO, USA},
series = {SE-CoDeSE'17}
}

@inproceedings{10.1145/3301551.3301579,
author = {Sandanayake, T. C. and Limesha, G. A. I. and Madhumali, T. S. S. and Mihirani, W. P. I. and Peiris, M. S. A.},
title = {Automated CV Analyzing and Ranking Tool to Select Candidates for Job Positions},
year = {2018},
isbn = {9781450366298},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301551.3301579},
doi = {10.1145/3301551.3301579},
abstract = {Processing of CVs to find a suitable candidate is a challenging task for many organisations. Most of the time identifying potential candidates for a job post is a time consuming and costly task for HR Divisions. Different CV s consists of information in a many formats. This research study extracts the information from the CV and ranks the CVs according to a given criteria to screen them out from the vast number of received applications in an organization. This research solution also recommends the most appropriate job category for an applicant according to his/ her CV information. At the same time the study creates candidate profiles using data from external professional web sources like Stack overflow, GitHub and Blogs. This research has basically designed for the domain of Information Technology related job postings such as Software Engineering, Quality Assurance etc. This system can rank the CVs according to various aspects presented in the CV, thus saving an enormous amount of time and effort that is required for manual scanning by the recruiters.},
booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
pages = {13–18},
numpages = {6},
keywords = {natural language processing, information extraction, machine learning, Curriculum vitae, job recommendation},
location = {Hong Kong, Hong Kong},
series = {ICIT 2018}
}

@inproceedings{10.1145/3377644.3377667,
author = {Duong-Trung, Nghia and Son, Ha Xuan and Le, Hai Trieu and Phan, Tan Tai},
title = {Smart Care: Integrating Blockchain Technology into the Design of Patient-Centered Healthcare Systems},
year = {2020},
isbn = {9781450377447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377644.3377667},
doi = {10.1145/3377644.3377667},
abstract = {Cross-institutional sharing of medical data is essential to provide e.ective collaborative treatment and clinical decisions for patients. Medical data privacy involves ensuring only authorized parties may access the health records under the awareness and approval of patients in any circumstances. This is crucial to any healthcare system because the protection of patients' clinical data is not only an ethical responsibility but also a legal mandate. Despite the importance of medical data sharing, today's healthcare systems have not provided enough protection of patients' sensitive information to be utilized deliberately or unintentionally. Hence, there is an urgent demand for a clinical transaction mechanism that allows patients to access, trace and control their health records. In this paper, the authors focus on several limitations in the literature and propose appropriate improvement in healthcare systems by (i) addressing information security and privacy, (ii) solving the lack of trust between providers, and (iii) encouraging scalability of healthcare interoperability. Building upon these key insights, we introduce several components of a patient-centered healthcare system using smart contracts via blockchain technology. A complete code solution is publicized on the authors' GitHub repository to engage further reproducibility and improvement.},
booktitle = {Proceedings of the 2020 4th International Conference on Cryptography, Security and Privacy},
pages = {105–109},
numpages = {5},
keywords = {Smart Contract, Healthcare System, Blockchain},
location = {Nanjing, China},
series = {ICCSP 2020}
}

@inproceedings{10.1145/2901739.2903496,
author = {Barnett, Jacob G. and Gathuru, Charles K. and Soldano, Luke S. and McIntosh, Shane},
title = {The Relationship between Commit Message Detail and Defect Proneness in Java Projects on GitHub},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2903496},
doi = {10.1145/2901739.2903496},
abstract = {Just-In-Time (JIT) defect prediction models aim to predict the commits that will introduce defects in the future. Traditionally, JIT defect prediction models are trained using metrics that are primarily derived from aspects of the code change itself (e.g., the size of the change, the author's prior experience). In addition to the code that is submitted during a commit, authors write commit messages, which describe the commit for archival purposes. It is our position that the level of detail in these commit messages can provide additional explanatory power to JIT defect prediction models. Hence, in this paper, we analyze the relationship between the defect proneness of commits and commit message volume (i.e., the length of the commit message) and commit message content (approximated using spam filtering technology). Through analysis of JIT models that were trained using 342 GitHub repositories, we find that our JIT models outperform random guessing models, achieving AUC and Brier scores that range between 0.63-0.96 and 0.01-0.21, respectively. Furthermore, our metrics that are derived from commit message detail provide a statistically significant boost to the explanatory power to the JIT models in 43%-80% of the studied systems, accounting for up to 72% of the explanatory power. Future JIT studies should consider adding commit message detail metrics.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {496–499},
numpages = {4},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.1145/3239235.3267432,
author = {Nepomuceno, Vilmar and Soares, Sergio},
title = {Maintaining Systematic Literature Reviews: Benefits and Drawbacks},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3267432},
doi = {10.1145/3239235.3267432},
abstract = {Background: Maintenance and traceability (versioning) are constant concerns in Software Engineering (SE), however, few works related to these topics in Systematic Literature Reviews (SLR) were found. Goal: The goal of this research is to elucidate how SLRs can be maintained and what are the benefits and drawbacks in this process. Method: This work presents a survey where experienced researchers that conducted SLRs between 2011 and 2015 answered questions about maintenance and traceability and, using software maintenance concepts, it addresses the SLRs maintenance process. From the 79 e-mails sent we reach 28 answers. Results: 19 of surveyed researchers have shown interest in keeping their SLRs up-to-date, but they have expressed concerns about the effort to be made to accomplish it. It was also observed that 20 participants would be willing to share their SLRs in common repositories, such as GitHub. Conclusions: There is a need to perform maintenance on SLRs. Thus, we are proposing a SLR maintenance process, taking into account some benefits and drawbacks identified during our study and presented through the paper.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {47},
numpages = {4},
keywords = {systematic literature review, maintenance, traceability, evidence based software engineering},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.5555/3241189.3241259,
author = {Stevens, Marc and Shumow, Daniel},
title = {Speeding up Detection of SHA-1 Collision Attacks Using Unavoidable Attack Conditions},
year = {2017},
isbn = {9781931971409},
publisher = {USENIX Association},
address = {USA},
abstract = {Counter-cryptanalysis, the concept of using cryptanalytic techniques to detect cryptanalytic attacks, was introduced at CRYPTO 2013 [23] with a hash collision detection algorithm. That is, an algorithm that detects whether a given single message is part of a colliding message pair constructed using a cryptanalytic collision attack on MD5 or SHA-1.Unfortunately, the original collision detection algorithm is not a low-cost solution as it costs 15 to 224 times more than a single hash computation. In this paper we present a significant performance improvement for collision detection based on the new concept of unavoidable conditions. Unavoidable conditions are conditions that are necessary for all feasible attacks in a certain attack class. As such they can be used to quickly dismiss particular attack classes that may have been used in the construction of the message. To determine an unavoidable condition one must rule out any feasible variant attack where this condition might not be necessary, otherwise adversaries aware of counter-cryptanalysis could easily bypass this improved collision detection with a carefully chosen variant attack. Based on a conjecture solidly supported by the current state of the art, we show how we can determine such unavoidable conditions for SHA-1.We have implemented the improved SHA-1 collision detection using such unavoidable conditions and which is more than 20 times faster than without our unavoidable condition improvements. We have measured that overall our implemented SHA-1 with collision detection is only a factor 1.60 slower, on average, than SHA-1. With the demonstration of a SHA-1 collision, the algorithm presented here has been deployed by Git, GitHub, Google Drive, Gmail, Microsoft OneDrive and others, showing the effectiveness of this technique.},
booktitle = {Proceedings of the 26th USENIX Conference on Security Symposium},
pages = {881–897},
numpages = {17},
location = {Vancouver, BC, Canada},
series = {SEC'17}
}

@inproceedings{10.1145/3219104.3229261,
author = {Canas, Karen and Ubiera, Brandon and Liu, Xinlian and Liu, Yanling},
title = {Scalable Biomedical Image Synthesis with GAN},
year = {2018},
isbn = {9781450364461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3219104.3229261},
doi = {10.1145/3219104.3229261},
abstract = {Despite the fast-paced progress in imaging techniques made possible by ubiquitous applications of convolutional neural networks, biomedical imaging has yet to benefit from the full potential of deep learning. An unresolved bottleneck is the lack of training set data. Some experimentally obtained data are kept and preserved by individual research groups where they were produced, out of the reach of the public; more often, high cost and rare occurrences simply mean not enough such images have been made. We propose to develop deep learning based workflow to overcome this barrier. Leveraging the largest radiology data (chest X-Ray) recently published by the NIH, we train a generative adversarial network (GAN) and use it to produce photorealistic images that retain pathological quality. We also explore porting our models to a range of supercomputing platforms and systems that we have access to, including XSEDE, NERSC, OLCF, Blue Waters, NIH Biowulf etc., to investigate and compare their performance. In addition to the obvious benefits of biomedical research, our work will help understand how current supercomputing infrastructure embraces machine learning demands. Our code and enhanced data set are available through GitHub/Binder.},
booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
articleno = {95},
numpages = {3},
keywords = {Biomedical Imaging, Image Synthesis, Convolutional Neural Network, Generative Adversarial Network, Deep Learning},
location = {Pittsburgh, PA, USA},
series = {PEARC '18}
}

@inproceedings{10.1145/3394171.3414535,
author = {Zhang, Huaizheng and Li, Yuanming and Huang, Yizheng and Wen, Yonggang and Yin, Jianxiong and Guan, Kyle},
title = {MLModelCI: An Automatic Cloud Platform for Efficient MLaaS},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3414535},
doi = {10.1145/3394171.3414535},
abstract = {MLModelCI provides multimedia researchers and developers with a one-stop platform for efficient machine learning (ML) services. The system leverages DevOps techniques to optimize, test, and manage models. It also containerizes and deploys these optimized and validated models as cloud services (MLaaS). In its essence, MLModelCI serves as a housekeeper to help users publish models. The models are first automatically converted to optimized formats for production purpose and then profiled under different settings (e.g., batch size and hardware). The profiling information can be used as guidelines for balancing the trade-off between performance and cost of MLaaS. Finally, the system dockerizes the models for ease of deployment to cloud environments. A key feature of MLModelCI is the implementation of a controller, which allows elastic evaluation which only utilizes idle workers while maintaining online service quality. Our system bridges the gap between current ML training and serving systems and thus free developers from manual and tedious work often associated with service deployment. We release the platform as an open-source project on GitHub under Apache 2.0 license, with the aim that it will facilitate and streamline more large-scale ML applications and research projects.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {4453–4456},
numpages = {4},
keywords = {profiling, inference serving, model deployment, conversion},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.1145/3203217.3203239,
author = {Eslami, Taban and Saeed, Fahad},
title = {Similarity Based Classification of ADHD Using Singular Value Decomposition},
year = {2018},
isbn = {9781450357616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3203217.3203239},
doi = {10.1145/3203217.3203239},
abstract = {Attention deficit hyperactivity disorder (ADHD) is one of the most common brain disorders among children. This disorder is considered as a big threat for public health and causes attention, focus and organizing difficulties for children and even adults. Since the cause of ADHD is not known yet, data mining algorithms are being used to help discover patterns which discriminate healthy from ADHD subjects. Numerous efforts are underway with the goal of developing classification tools for ADHD diagnosis based on functional and structural magnetic resonance imaging data of the brain. In this paper, we used Eros, which is a technique for computing similarity between two multivariate time series along with k-Nearest-Neighbor classifier, to classify healthy vs ADHD children. We designed a model selection scheme called J-Eros which is able to pick the optimum value of k for k-Nearest-Neighbor from the training data. We applied this technique to the public data provided by ADHD-200 Consortium competition and our results show that J-Eros is capable of discriminating healthy from ADHD children such that we outperformed the best results reported by ADHD-200 competition about 20 percent for two datasets.The implemented code is available as GPL license on GitHub portal of our lab at https://github.com/pcdslab/J-Eros.},
booktitle = {Proceedings of the 15th ACM International Conference on Computing Frontiers},
pages = {19–25},
numpages = {7},
keywords = {cross validation, eros, ADHD disorder, fMRI, multivariate time series},
location = {Ischia, Italy},
series = {CF '18}
}

@inproceedings{10.1145/3368089.3417940,
author = {Xie, Mulong and Feng, Sidong and Xing, Zhenchang and Chen, Jieshan and Chen, Chunyang},
title = {UIED: A Hybrid Tool for GUI Element Detection},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417940},
doi = {10.1145/3368089.3417940},
abstract = {Graphical User Interface (GUI) elements detection is critical for many GUI automation and GUI testing tasks. Acquiring the accurate positions and classes of GUI elements is also the very first step to conduct GUI reverse engineering or perform GUI testing. In this paper, we implement a User Iterface Element Detection (UIED), a toolkit designed to provide user with a simple and easy-to-use platform to achieve accurate GUI element detection. UIED integrates multiple detection methods including old-fashioned computer vision (CV) approaches and deep learning models to handle diverse and complicated GUI images. Besides, it equips with a novel customized GUI element detection methods to produce state-of-the-art detection results. Our tool enables the user to change and edit the detection result in an interactive dashboard. Finally, it exports the detected UI elements in the GUI image to design files that can be further edited in popular UI design tools such as Sketch and Photoshop. UIED is evaluated to be capable of accurate detection and useful for downstream works. Tool URL: <a>http://uied.online</a> Github Link: <a>https://github.com/MulongXie/UIED</a>},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1655–1659},
numpages = {5},
keywords = {Deep Learning, Object Detection, Computer Vision, User Interface},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3359061.3361082,
author = {Estep, Samuel},
title = {Gradual Program Analysis},
year = {2019},
isbn = {9781450369923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3359061.3361082},
doi = {10.1145/3359061.3361082},
abstract = {The designers of static analyses for null safety often try to reduce the number of false positives reported by the analysis through increased engineering effort, user-provided annotations, and/or weaker soundness guarantees. To produce a null-pointer analysis with little engineering effort, reduced false positives, and strong soundness guarantees in a principled way, we adapt the “Abstracting Gradual Typing” framework to the abstract-interpretation based program analysis setting. In particular, a simple static dataflow analysis that relies on user-provided annotations and has nullability lattice N ⊑ ⊤ (where N means “definitely not null” and ⊤ means “possibly null”) is gradualized producing a new lattice N ⊑ ? ⊑ ⊤. Question mark explicitly represents “optimistic uncertainty” in the analysis itself, supporting a formal soundness property and the “gradual guarantees” laid out in the gradual typing literature. We then implement a prototype of our gradual null-pointer analysis as a Facebook Infer checker, and compare it to existing null-pointer analyses via a suite of GitHub repositories used originally by Uber to evaluate their NullAway tool. Our prototype has architecture and output very similar to these existing tools, suggesting the value of applying our approach to more sophisticated program analyses in the future.},
booktitle = {Proceedings Companion of the 2019 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {52–53},
numpages = {2},
keywords = {abstract interpretation, gradual typing, program analysis, null safety},
location = {Athens, Greece},
series = {SPLASH Companion 2019}
}

@inproceedings{10.1109/ICPC.2017.30,
author = {Zampetti, Fiorella and Ponzanelli, Luca and Bavota, Gabriele and Mocci, Andrea and Di Penta, Massimiliano and Lanza, Michele},
title = {How Developers Document Pull Requests with External References},
year = {2017},
isbn = {9781538605356},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2017.30},
doi = {10.1109/ICPC.2017.30},
abstract = {Online resources of formal and informal documentation-such as reference manuals, forum discussions and tutorials-have become an asset to software developers, as they allow them to tackle problems and to learn about new tools, libraries, and technologies. This study investigates to what extent and for which purpose developers refer to external online resources when they contribute changes to a repository by raising a pull request. Our study involved (i) a quantitative analysis of over 150k URLs occurring in pull requests posted in GitHub; (ii) a manual coding of the kinds of software evolution activities performed in commits related to a statistically significant sample of 2,130 pull requests referencing external documentation resources; (iii) a survey with 69 participants, who provided feedback on how they use online resources and how they refer to them when filing a pull request. Results of the study indicate that, on the one hand, developers find external resources useful to learn something new or to solve specific problems, and they perceive useful referring such resources to better document changes. On the other hand, both interviews and repository mining suggest that external resources are still rarely referred in document changes.},
booktitle = {Proceedings of the 25th International Conference on Program Comprehension},
pages = {23–33},
numpages = {11},
keywords = {documenting changes, empirical study, online resources},
location = {Buenos Aires, Argentina},
series = {ICPC '17}
}

@inproceedings{10.1145/2904081.2904085,
author = {Kozma, Viktor and Broman, David},
title = {MORAP: A Modular Robotic Arm Platform for Teaching and Experimenting with Equation-Based Modeling Languages},
year = {2016},
isbn = {9781450342025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2904081.2904085},
doi = {10.1145/2904081.2904085},
abstract = {Equation-based object-oriented (EOO) modeling and simulation techniques have in the last decades gained significant attention both in academia and industry. One of the key properties of EOO languages is modularity, where different components can be developed independently and then connected together to form a complete acausal model. However, extensive modeling without explicit model validation together with a real physical system can result in incorrect assumptions and false conclusions. In particular, in an educational and research setting, it is vital that students experiment both with equation-based models and the real system that is being modeled. In this work-in-progress paper, we present a physical experimental robotic arm platform that is designed for teaching and research. Similar to EOO models, the robotic arm is modular, meaning that its parts can be reconfigured and composed together in various settings, and used for different experiments. The platform is completely open source, where electronic schematics, CAD models for 3D printing, controller software, and component specifications are available on GitHub. The vision is to form a community, where new open source components are continuously added, to enable an open and freely available physical experimental platform for EOO languages.},
booktitle = {Proceedings of the 7th International Workshop on Equation-Based Object-Oriented Modeling Languages and Tools},
pages = {27–30},
numpages = {4},
keywords = {modeling, robotic arm, equations, simulations},
location = {Milano, Italy},
series = {EOOLT '16}
}

@inproceedings{10.1145/3236024.3275525,
author = {Liang, Jie and Jiang, Yu and Chen, Yuanliang and Wang, Mingzhe and Zhou, Chijin and Sun, Jiaguang},
title = {PAFL: Extend Fuzzing Optimizations of Single Mode to Industrial Parallel Mode},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275525},
doi = {10.1145/3236024.3275525},
abstract = {Researchers have proposed many optimizations to improve the efficiency of fuzzing, and most optimized strategies work very well on their targets when running in single mode with instantiating one fuzzer instance. However, in real industrial practice, most fuzzers run in parallel mode with instantiating multiple fuzzer instances, and those optimizations unfortunately fail to maintain the efficiency improvements.  In this paper, we present PAFL, a framework that utilizes efficient guiding information synchronization and task division to extend those existing fuzzing optimizations of single mode to industrial parallel mode. With an additional data structure to store the guiding information, the synchronization ensures the information is shared and updated among different fuzzer instances timely. Then, the task division promotes the diversity of fuzzer instances by splitting the fuzzing task into several sub-tasks based on branch bitmap. We first evaluate PAFL using 12 different real-world programs from Google fuzzer-test-suite. Results show that in parallel mode, two AFL improvers–AFLFast and FairFuzz do not outperform AFL, which is different from the case in single mode. However, when augmented with PAFL, the performance of AFLFast and FairFuzz in parallel mode improves. They cover 8% and 17% more branches, trigger 79% and 52% more unique crashes. For further evaluation on more widely-used software systems from GitHub, optimized fuzzers augmented with PAFL find more real bugs, and 25 of which are security-critical vulnerabilities registered as CVEs in the US National Vulnerability Database.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {809–814},
numpages = {6},
keywords = {Software testing, Parallel, Fuzzing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/2819009.2819091,
author = {Lyons, Kelly and Oh, Christie},
title = {SOA4DM: Applying an SOA Paradigm to Coordination in Humanitarian Disaster Response},
year = {2015},
publisher = {IEEE Press},
abstract = {Despite efforts to achieve a sustainable state of control over the management of global crises, disasters are occurring with greater frequency, intensity, and affecting many more people than ever before while the resources to deal with them do not grow apace. As we enter 2015, with continued concerns that mega-crises may become the new normal, we need to develop novel methods to improve the efficiency and effectiveness of our management of disasters. Software engineering as a discipline has long had an impact on society beyond its role in the development of software systems. In fact, software engineers have been described as the developers of prototypes for future knowledge workers; tools such as Github and Stack Overflow have demonstrated applications beyond the domain of software engineering. In this paper, we take the potential influence of software engineering one-step further and propose using the software service engineering paradigm as a new approach to managing disasters. Specifically, we show how the underlying principles of service-oriented architectures (SOA) can be applied to the coordination of disaster response operations. We describe key challenges in coordinating disaster response and discuss how an SOA approach can address those challenges.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {519–522},
numpages = {4},
keywords = {disaster response, SOA},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3395363.3404367,
author = {Thompson, George and Sullivan, Allison K.},
title = {ProFL: A Fault Localization Framework for Prolog},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404367},
doi = {10.1145/3395363.3404367},
abstract = {Prolog is a declarative, first-order logic that has been used in a variety of domains to implement heavily rules-based systems. However, it is challenging to write a Prolog program correctly. Fortunately, the SWI-Prolog environment supports a unit testing framework, plunit, which enables developers to systematically check for correctness. However, knowing a program is faulty is just the first step. The developer then needs to fix the program which means the developer needs to determine what part of the program is faulty. ProFL is a fault localization tool that adapts imperative-based fault localization techniques to Prolog’s declarative environment. ProFL takes as input a faulty Prolog program and a plunit test suite. Then, ProFL performs fault localization and returns a list of suspicious program clauses to the user. Our toolset encompasses two different techniques: ProFLs, a spectrum-based technique, and ProFLm, a mutation-based technique. This paper describes our Python implementation of ProFL, which is a command-line tool, released as an open-source project on GitHub (https://github.com/geoorge1d127/ProFL). Our experimental results show ProFL is accurate at localizing faults in our benchmark programs.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {561–564},
numpages = {4},
keywords = {Prolog, Fault localization, Declarative programming},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.5555/2820518.2820528,
author = {Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando},
title = {Mining Energy-Aware Commits},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {56–67},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.1145/3275219.3275226,
author = {Yan, Jiafei and Sun, Hailong and Wang, Xu and Liu, Xudong and Song, Xiaotao},
title = {Profiling Developer Expertise across Software Communities with Heterogeneous Information Network Analysis},
year = {2018},
isbn = {9781450365901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275219.3275226},
doi = {10.1145/3275219.3275226},
abstract = {Knowing developer expertise is critical for achieving effective task allocation. However, it is of great challenge to accurately profile the expertise of developers over the Internet as their activities often disperse across different online communities. In this regard, the existing works either merely concern a single community, or simply sum up the expertise in individual communities. The former suffers from low accuracy due to incomplete data, while the latter impractically assumes that developer expertise is completely independent and irrelavant across communities. To overcome those limitations, we propose a new approach to profile developer expertise across software communities through heterogeneous information network (HIN) analysis. A HIN is first built by analyzing the developer activities in various communities, where nodes represent objects like developers and skills, and edges represent the relations among objects. Second, as random walk with restart (RWR) is known for its ability to capture the global structure of the whole network, we adopt RWR over the HIN to estimate the proximity of developer nodes and skill nodes, which essentially reflects developer expertise. Based on the data of 72,645 common users of GitHub and Stack Overflow, we conducted an empirical study and evaluated developer expertise using proposed approach. To evaluate the effect of our approach, we use the obtained expertise to estimate the competency of developers in answering the questions posted in Stack Overflow. The experimental results demonstrate the superiority of our approach over existing methods.},
booktitle = {Proceedings of the Tenth Asia-Pacific Symposium on Internetware},
articleno = {2},
numpages = {9},
keywords = {heterogeneous information network, random walk with restart, Developer expertise},
location = {Beijing, China},
series = {Internetware '18}
}

@inproceedings{10.1145/3106237.3117771,
author = {Zhou, Yaqin and Sharma, Asankhaya},
title = {Automated Identification of Security Issues from Commit Messages and Bug Reports},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3117771},
doi = {10.1145/3106237.3117771},
abstract = { The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {914–919},
numpages = {6},
keywords = {machine learning, commit, vulnerability identification, bug report},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1109/ICPC.2019.00025,
author = {de Almeida Filho, Francisco Gon\c{c}alves and Martins, Ant\^{o}nio Diogo Forte and Vinuto, Tiago da Silva and Monteiro, Jos\'{e} Maria and de Sousa, \'{I}talo Pereira and de Castro Machado, Javam and Rocha, Lincoln Souza},
title = {Prevalence of Bad Smells in PL/SQL Projects},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICPC.2019.00025},
doi = {10.1109/ICPC.2019.00025},
abstract = {Code Smell can be defined as any feature in the source code of a software that may indicate possible problems. In database languages, the term Bad Smell has been used as a generalization of Code Smell, once some features that are not directly related to code also can indicate problems, such as, for instance, the inappropriate type of an index structure or a SQL query written inefficiently. Bearing in mind the recurrence of different Bad Smell, they were catalogued. Along with these catalogs, tools were developed to automatically identify Bad Smell occurrences in a given code. With the help of these tools, it has become possible to perform quick and effective analysis. In this context, this paper proposes an exploratory study about Bad Smell in PL/SQL codes, from free software projects, published on GitHub. We analyzed 20 open-source PL/SQL projects and empirically study the prevalence of bad smells. Our results showed that some smells occur together. Besides, some smells are more frequent than others. Based on this principle, this paper has the potential to aid professionals from the databases area to avoid future problems during the development of a PL/SQL project.},
booktitle = {Proceedings of the 27th International Conference on Program Comprehension},
pages = {116–121},
numpages = {6},
keywords = {prevalence, bad smell, code smell, PL/SQL},
location = {Montreal, Quebec, Canada},
series = {ICPC '19}
}

@inproceedings{10.1145/3239235.3267440,
author = {Sahal, Emre and Tosun, Ayse},
title = {Identifying Bug-Inducing Changes for Code Additions},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3267440},
doi = {10.1145/3239235.3267440},
abstract = {Background. SZZ algorithm has been popularly used to identify bug-inducing changes in version history. It is still limited to link a fixing change to an inducing one, when the fix constitutes of code additions only. Goal. We improve the original SZZ by proposing a way to link the code additions in a fixing change to a list of candidate inducing changes. Method. The improved version, A-SZZ, finds the code block encapsulating the new code added in a fixing change, and traces back to the historical changes of the code block. We mined the GitHub repositories of two projects, Angular.js and Vue, and ran A-SZZ to identify bug-inducing changes of code additions. We evaluated the effectiveness of A-SZZ in terms of inducing and fixing ratios, and time span between the two changes. Results. The approach works well for linking code additions with previous changes, although it still produces many false positives. Conclusions. Nearly a quarter of the files in fixing changes contain code additions only, and hence, new heuristics should be implemented to link those with inducing changes in a more efficient way.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {57},
numpages = {2},
keywords = {SZZ, repository mining, bug inducing changes},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3416506.3423576,
author = {Phan, Hung and Jannesari, Ali},
title = {Statistical Machine Translation Outperforms Neural Machine Translation in Software Engineering: Why and How},
year = {2020},
isbn = {9781450381253},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3416506.3423576},
doi = {10.1145/3416506.3423576},
abstract = {Neural Machine Translation (NMT) is the current trend approach in Natural Language Processing (NLP) to solve the problem of auto- matically inferring the content of target language given the source language. The ability of NMT is to learn deep knowledge inside lan- guages by deep learning approaches. However, prior works show that NMT has its own drawbacks in NLP and in some research problems of Software Engineering (SE). In this work, we provide a hypothesis that SE corpus has inherent characteristics that NMT will confront challenges compared to the state-of-the-art translation engine based on Statistical Machine Translation. We introduce a problem which is significant in SE and has characteristics that challenges the abil- ity of NMT to learn correct sequences, called Prefix Mapping. We implement and optimize the original SMT and NMT to mitigate those challenges. By the evaluation, we show that SMT outperforms NMT for this research problem, which provides potential directions to optimize the current NMT engines for specific classes of parallel corpus. By achieving the accuracy from 65% to 90% for code tokens generation of 1000 Github code corpus, we show the potential of using MT for code completion at token level.},
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program Languages},
pages = {3–12},
numpages = {10},
keywords = {Neural Machine Translation, Statistical Machine Translation},
location = {Virtual, USA},
series = {RL+SE&amp;PL 2020}
}

@inproceedings{10.1145/2950290.2983932,
author = {Gyori, Alex and Lambeth, Ben and Shi, August and Legunsen, Owolabi and Marinov, Darko},
title = {NonDex: A Tool for Detecting and Debugging Wrong Assumptions on Java API Specifications},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983932},
doi = {10.1145/2950290.2983932},
abstract = { We present NonDex, a tool for detecting and debugging wrong assumptions on Java APIs. Some APIs have underdetermined specifications to allow implementations to achieve different goals, e.g., to optimize performance. When clients of such APIs assume stronger-than-specified guarantees, the resulting client code can fail. For example, HashSet’s iteration order is underdetermined, and code assuming some implementation-specific iteration order can fail. NonDex helps to proactively detect and debug such wrong assumptions. NonDex performs detection by randomly exploring different behaviors of underdetermined APIs during test execution. When a test fails during exploration, NonDex searches for the invocation instance of the API that caused the failure. NonDex is open source, well-integrated with Maven, and also runs from the command line. During our experiments with the NonDex Maven plugin, we detected 21 new bugs in eight Java projects from GitHub, and, using the debugging feature of NonDex, we identified the underlying wrong assumptions for these 21 new bugs and 54 previously detected bugs. We opened 13 pull requests; developers already accepted 12, and one project changed the continuous-integration configuration to run NonDex on every push. The demo video is at: https://youtu.be/h3a9ONkC59c },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {993–997},
numpages = {5},
keywords = {NonDex, underdetermined API, flaky tests},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/2635868.2635901,
author = {Allamanis, Miltiadis and Sutton, Charles},
title = {Mining Idioms from Source Code},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635901},
doi = {10.1145/2635868.2635901},
abstract = { We present the first method for automatically mining code idioms from a corpus of previously written, idiomatic software projects. We take the view that a code idiom is a syntactic fragment that recurs across projects and has a single semantic purpose. Idioms may have metavariables, such as the body of a for loop. Modern IDEs commonly provide facilities for manually defining idioms and inserting them on demand, but this does not help programmers to write idiomatic code in languages or using libraries with which they are unfamiliar. We present Haggis, a system for mining code idioms that builds on recent advanced techniques from statistical natural language processing, namely, nonparametric Bayesian probabilistic tree substitution grammars. We apply Haggis to several of the most popular open source projects from GitHub. We present a wide range of evidence that the resulting idioms are semantically meaningful, demonstrating that they do indeed recur across software projects and that they occur more frequently in illustrative code examples collected from a Q&amp;A site. Manual examination of the most common idioms indicate that they describe important program concepts, including object creation, exception handling, and resource management. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {472–483},
numpages = {12},
keywords = {naturalness of source code, code idioms, syntactic code patterns},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/2950290.2950324,
author = {Ahmed, Iftekhar and Gopinath, Rahul and Brindescu, Caius and Groce, Alex and Jensen, Carlos},
title = {Can Testedness Be Effectively Measured?},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950324},
doi = {10.1145/2950290.2950324},
abstract = { Among the major questions that a practicing tester faces are deciding where to focus additional testing effort, and deciding when to stop testing. Test the least-tested code, and stop when all code is well-tested, is a reasonable answer. Many measures of "testedness" have been proposed; unfortunately, we do not know whether these are truly effective. In this paper we propose a novel evaluation of two of the most important and widely-used measures of test suite quality. The first measure is statement coverage, the simplest and best-known code coverage measure. The second measure is mutation score, a supposedly more powerful, though expensive, measure.  We evaluate these measures using the actual criteria of interest: if a program element is (by these measures) well tested at a given point in time, it should require fewer future bug-fixes than a "poorly tested" element. If not, then it seems likely that we are not effectively measuring testedness. Using a large number of open source Java programs from Github and Apache, we show that both statement coverage and mutation score have only a weak negative correlation with bug-fixes. Despite the lack of strong correlation, there are statistically and practically significant differences between program elements for various binary criteria. Program elements (other than classes) covered by any test case see about half as many bug-fixes as those not covered, and a similar line can be drawn for mutation score thresholds. Our results have important implications for both software engineering practice and research evaluation. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {547–558},
numpages = {12},
keywords = {test suite evaluation, mutation testing, coverage criteria, statistical analysis},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1109/RAISE.2019.00010,
author = {Ferenc, Rudolf and Hegedundefineds, P\'{e}ter and Gyimesi, P\'{e}ter and Antal, G\'{a}bor and B\'{a}n, D\'{e}nes and Gyim\'{o}thy, Tibor},
title = {Challenging Machine Learning Algorithms in Predicting Vulnerable JavaScript Functions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/RAISE.2019.00010},
doi = {10.1109/RAISE.2019.00010},
abstract = {The rapid rise of cyber-crime activities and the growing number of devices threatened by them place software security issues in the spotlight. As around 90% of all attacks exploit known types of security issues, finding vulnerable components and applying existing mitigation techniques is a viable practical approach for fighting against cyber-crime. In this paper, we investigate how the state-of-the-art machine learning techniques, including a popular deep learning algorithm, perform in predicting functions with possible security vulnerabilities in JavaScript programs.We applied 8 machine learning algorithms to build prediction models using a new dataset constructed for this research from the vulnerability information in public databases of the Node Security Project and the Snyk platform, and code fixing patches from GitHub. We used static source code metrics as predictors and an extensive grid-search algorithm to find the best performing models. We also examined the effect of various re-sampling strategies to handle the imbalanced nature of the dataset.The best performing algorithm was KNN, which created a model for the prediction of vulnerable functions with an F-measure of 0.76 (0.91 precision and 0.66 recall). Moreover, deep learning, tree and forest based classifiers, and SVM were competitive with F-measures over 0.70. Although the F-measures did not vary significantly with the re-sampling strategies, the distribution of precision and recall did change. No re-sampling seemed to produce models preferring high precision, while resampling strategies balanced the IR measures.},
booktitle = {Proceedings of the 7th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering},
pages = {8–14},
numpages = {7},
keywords = {dataset, machine learning, JavaScript, vulnerability, code metrics, deep learning},
location = {Montreal, Quebec, Canada},
series = {RAISE '19}
}

@inproceedings{10.1145/3377811.3380403,
author = {Tabassum, Sadia and Minku, Leandro L. and Feng, Danyi and Cabral, George G. and Song, Liyan},
title = {An Investigation of Cross-Project Learning in Online Just-in-Time Software Defect Prediction},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380403},
doi = {10.1145/3377811.3380403},
abstract = {Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {554–565},
numpages = {12},
keywords = {cross-project learning, online learning, software defect prediction, class imbalance, verification latency, transfer learning, concept drift},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2460625.2460716,
author = {Kirton, Travis},
title = {C4: Creative Coding for IOS},
year = {2013},
isbn = {9781450318983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2460625.2460716},
doi = {10.1145/2460625.2460716},
abstract = {C4 is a new creative coding framework that focuses on interactivity, visualization and the relationship between various media. Designed for iOS, C4 makes it extremely easy to create apps for iPad, iPhone and iPod devices. Initially developed as a platform for quickly creating interactive artistic works, C4 is developing into a more broad-based language for other areas such as music and data visualization.In this workshop, participants will rapidly prototype interactive animated interfaces on iOS devices. Participants will have the opportunity to learn how to easily create dynamic animations, using all kinds of media including audio, video, shapes, OpenGL objects and more. In addition to this, participants will learn how to easily add the full suite of iOS gestural interaction to their applications and objects. Furthermore, C4 provides easy access to the camera, as well as access to the compass, motion, acceleration, proximity and light sensors. Along the way, participants will be introduced to the larger C4 community through their participation on various social networks, the Stackoverflow community-moderated Q&amp;A forum, and will also be shown how to access and share code on Github.},
booktitle = {Proceedings of the 7th International Conference on Tangible, Embedded and Embodied Interaction},
pages = {411–413},
numpages = {3},
keywords = {media, application programming interface, mobile, creative coding, multitouch, first-class objects},
location = {Barcelona, Spain},
series = {TEI '13}
}

@inproceedings{10.1145/3183713.3196888,
author = {Yan, Cong and He, Yeye},
title = {Synthesizing Type-Detection Logic for Rich Semantic Data Types Using Open-Source Code},
year = {2018},
isbn = {9781450347037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183713.3196888},
doi = {10.1145/3183713.3196888},
abstract = {Given a table of data, existing systems can often detect basic atomic types (e.g., strings vs. numbers) for each column. A new generation of data-analytics and data-preparation systems are starting to automatically recognize rich semantic types such as date-time, email address, etc., for such metadata can bring an array of benefits including better table understanding, improved search relevance, precise data validation, and semantic data transformation. However, existing approaches only detect a limited number of types using regular-expression-like patterns, which are often inaccurate, and cannot handle rich semantic types such as credit card and ISBN numbers that encode semantic validations (e.g., checksum).We developed AUTOTYPE from open-source repositories like GitHub. Users only need to provide a set of positive examples for a target data type and a search keyword, our system will automatically identify relevant code, and synthesize type-detection functions using execution traces. We compiled a benchmark with 112 semantic types, out of which the proposed system can synthesize code to detect 84 such types at a high precision. Applying the synthesized type-detection logic on web table columns have also resulted in a significant increase in data types discovered compared to alternative approaches.},
booktitle = {Proceedings of the 2018 International Conference on Management of Data},
pages = {35–50},
numpages = {16},
keywords = {type detection, open-source code, semantic data types, metadata management, data preparation, code search},
location = {Houston, TX, USA},
series = {SIGMOD '18}
}

@inproceedings{10.1145/3387904.3389288,
author = {Tushev, Miroslav and Mahmoud, Anas},
title = {Linguistic Documentation of Software History},
year = {2020},
isbn = {9781450379588},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387904.3389288},
doi = {10.1145/3387904.3389288},
abstract = {Open Source Software (OSS) projects start with an initial vocabulary, often determined by the first generation of developers. This vocabulary, embedded in code identifier names and internal code comments, goes through multiple rounds of change, influenced by the interrelated patterns of human (e.g., developers joining and departing) and system (e.g., maintenance activities) interactions. Capturing the dynamics of this change is crucial for understanding and synthesizing code changes over time. However, existing code evolution analysis tools, available in modern version control systems such as GitHub and SourceForge, often overlook the linguistic aspects of code evolution. To bridge this gap, in this paper, we propose to study code evolution in OSS projects through the lens of developers' language, also known as code lexicon. Our analysis is conducted using 32 OSS projects sampled from a broad range of application domains. Our results show that different maintenance activities impact code lexicon differently. These insights lay out a preliminary foundation for modeling the linguistic history of OSS projects. In the long run, this foundation will be utilized to provide support for basic program comprehension tasks and help researchers gain new insights into the complex interplay between linguistic change and various system and human aspects of OSS development.},
booktitle = {Proceedings of the 28th International Conference on Program Comprehension},
pages = {386–390},
numpages = {5},
keywords = {Code Lexicon, Open Source Software, Linguistic Change},
location = {Seoul, Republic of Korea},
series = {ICPC '20}
}

@inproceedings{10.1145/3017680.3022383,
author = {Bart, Austin Cory and Kafura, Dennis},
title = {BlockPy Interactive Demo: Dual Text/Block Python Programming Environment for Guided Practice and Data Science (Abstract Only)},
year = {2017},
isbn = {9781450346986},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3017680.3022383},
doi = {10.1145/3017680.3022383},
abstract = {Introductory non-major learners face the challenge of mastering programming fundamentals while remaining sufficiently motivated to engage with the computing discipline. In particular, multi-disciplinary students struggle to find relevance in traditional computing curricula that tend to either emphasize abstract concepts, focus on entertainment (e.g., game and animation design), or rely on decontextualized settings. To address these issues, this demo introduces BlockPy, a web-based environment for Python (https://blockpy.com). The most powerful feature of BlockPy is a dual text/block view that beginners can freely move between, using advanced Mutual Language Translation techniques. The environment contextualizes introductory programming with data science by integrating real-world data including weather reports, classic book statistics, and historical crime data. A fusion of Blockly and Skulpt, the entire interface runs locally with no need for server sandboxing. BlockPy is also a platform for interactive, guided practice problems with automatic feedback that scaffolds learners. This demo will walk through the novel features of BlockPy's environment, including the instructor's perspective of creating new problems and how BlockPy can be embedded in modern LTI-compatible learning management systems. BlockPy is available online for free and is open-sourced on GitHub. This material is based on work supported by the NSF under Grants No. DGE-0822220, DUE-1444094, and DUE-1624320.},
booktitle = {Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education},
pages = {639–640},
numpages = {2},
keywords = {python, blocks, intelligent tutoring, scaffolding, introductory programming, mutual language translation, data science, guided feedback, LTI, web environments},
location = {Seattle, Washington, USA},
series = {SIGCSE '17}
}

@inproceedings{10.1145/3308558.3313729,
author = {Waller, Isaac and Anderson, Ashton},
title = {Generalists and Specialists: Using Community Embeddings to Quantify Activity Diversity in Online Platforms},
year = {2019},
isbn = {9781450366748},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308558.3313729},
doi = {10.1145/3308558.3313729},
abstract = {In many online platforms, people must choose how broadly to allocate their energy. Should one concentrate on a narrow area of focus, and become a specialist, or apply oneself more broadly, and become a generalist? In this work, we propose a principled measure of how generalist or specialist a user is, and study behavior in online platforms through this lens. To do this, we construct highly accurate community embeddings that represent communities in a high-dimensional space. We develop sets of community analogies and use them to optimize our embeddings so that they encode community relationships extremely well. Based on these embeddings, we introduce a natural measure of activity diversity, the GS-score. Applying our embedding-based measure to online platforms, we observe a broad spectrum of user activity styles, from extreme specialists to extreme generalists, in both community membership on Reddit and programming contributions on GitHub. We find that activity diversity is related to many important phenomena of user behavior. For example, specialists are much more likely to stay in communities they contribute to, but generalists are much more likely to remain on platforms as a whole. We also find that generalists engage with significantly more diverse sets of users than specialists do. Furthermore, our methodology leads to a simple algorithm for community recommendation, matching state-of-the-art methods like collaborative filtering. Our methods and results introduce an important new dimension of online user behavior and shed light on many aspects of online platform use.},
booktitle = {The World Wide Web Conference},
pages = {1954–1964},
numpages = {11},
keywords = {generalist and specialists, activity diversity, community embeddings, community recommendation},
location = {San Francisco, CA, USA},
series = {WWW '19}
}

@inproceedings{10.1109/ESEM.2017.42,
author = {Griffith, Isaac and Izurieta, Clemente and Huvaere, Chris},
title = {An Industry Perspective to Comparing the SQALE and Quamoco Software Quality Models},
year = {2017},
isbn = {9781509040391},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ESEM.2017.42},
doi = {10.1109/ESEM.2017.42},
abstract = {Context: We investigate the different perceptions of quality provided by leading operational quality models when used to evaluate software systems from an industry perspective. Goal: To compare and evaluate the quality assessments of two competing quality models and to develop an extensible solution to meet the quality assurance measurement needs of an industry stakeholder -The Construction Engineering Research Laboratory (CERL). Method: In cooperation with our industry partner TechLink, we operationalize the Quamoco quality model and employ a multiple case study design comparing the results of Quamoco and SQALE, two implementations of well known quality models. The study is conducted across current versions of several open source software projects sampled from GitHub and commercial software for sustainment management systems implemented in the C# language from our industry partner. Each project represents a separate embedded unit of study in a given context -open source or commercial. We employ inter-rater agreement and correlation analysis to compare the results of both models, focusing on Maintainability, Reliability, and Security assessments. Results: Our observations suggest that there is a significant disconnect between the assessments of quality under both quality models. Conclusion: In order to support industry adoption, additional work is required to bring competing implementations of quality models into alignment. This exploratory case study helps us shed light into this problem.},
booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {287–296},
numpages = {10},
keywords = {quality assurance, software quality, quality standards},
location = {Markham, Ontario, Canada},
series = {ESEM '17}
}

@inproceedings{10.1145/3183440.3190332,
author = {Beller, Moritz},
title = {Toward an Empirical Theory of Feedback-Driven Development},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190332},
doi = {10.1145/3183440.3190332},
abstract = {Software developers today crave for feedback, be it from their peers or even bots in the form of code review, static analysis tools like their compiler, or the local or remote execution of their tests in the Continuous Integration (CI) environment. With the advent of social coding sites like GitHub and tight integration of CI services like Travis CI, software development practices have fundamentally changed. Despite a highly changed software engineering landscape, however, we still lack a suitable description of an individual's contemporary software development practices, that is how an individual code contribution comes to be. Existing descriptions like the v-model are either too coarse-grained to describe an individual contributor's workflow, or only regard a sub-part of the development process like Test-Driven Development. In addition, most existing models are pre- rather than de-scriptive. By contrast, in our thesis, we perform a series of empirical studies to describe the individual constituents of Feedback-Driven Development (FDD) and then compile the evidence into an initial framework on how modern software development works. Our thesis culminates in the finding that feedback loops are the characterizing criterion of contemporary software development. Our model is flexible enough to accommodate a broad bandwidth of contemporary workflows, despite large variances in how projects use and configure parts of FDD.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {503–505},
numpages = {3},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2908131.2908145,
author = {Chen, Guanliang and Davis, Dan and Lin, Jun and Hauff, Claudia and Houben, Geert-Jan},
title = {Beyond the MOOC Platform: Gaining Insights about Learners from the Social Web},
year = {2016},
isbn = {9781450342087},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908131.2908145},
doi = {10.1145/2908131.2908145},
abstract = {Massive Open Online Courses (MOOCs) have enabled millions of learners across the globe to increase their levels of expertise in a wide variety of subjects. Research efforts surrounding MOOCs are typically focused on improving the learning experience, as the current retention rates (less than 7% of registered learners complete a MOOC) show a large gap between vision and reality in MOOC learning.Current data-driven approaches to MOOC adaptations rely on data traces learners generate within a MOOC platform such as edX or Coursera. As a MOOC typically lasts between five and eight weeks and with many MOOC learners being rather passive consumers of the learning material, this exclusive use of MOOC platform data traces limits the insights that can be gained from them.The Social Web potentially offers a rich source of data to supplement the MOOC platform data traces, as many learners are also likely to be active on one or more Social Web platforms. In this work, we present a first exploratory analysis of the Social Web platforms MOOC learners are active on --- we consider more than 320,000 learners that registered for 18 MOOCs on the edX platform and explore their user profiles and activities on StackExchange, GitHub, Twitter and LinkedIn.},
booktitle = {Proceedings of the 8th ACM Conference on Web Science},
pages = {15–24},
numpages = {10},
location = {Hannover, Germany},
series = {WebSci '16}
}

@inproceedings{10.1145/3184558.3191543,
author = {Zaveri, Amrapali and Serrano, Pedro Hernandez and Desai, Manisha and Dumontier, Michel},
title = {CrowdED: Guideline for Optimal Crowdsourcing Experimental Design},
year = {2018},
isbn = {9781450356404},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3184558.3191543},
doi = {10.1145/3184558.3191543},
abstract = {Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.},
booktitle = {Companion Proceedings of the The Web Conference 2018},
pages = {1109–1116},
numpages = {8},
keywords = {reproducibility, fair, data science, metadata, crowdsourcing, data quality, biomedical},
location = {Lyon, France},
series = {WWW '18}
}

@inproceedings{10.1109/ICGSE.2019.00033,
author = {Kanakis, Georgios and Fischer, Stefan and Khelladi, Djamel Eddine and Egyed, Alexander},
title = {Supporting a Flexible Grouping Mechanism for Collaborating Engineering Teams},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICGSE.2019.00033},
doi = {10.1109/ICGSE.2019.00033},
abstract = {Most engineering tools do not provide much support for collaborating teams and today's engineering knowledge repositories lack flexibility and are limited. Engineering teams have different needs and their team members have different preferences on how and when to collaborate. These needs may depend on the individual work style, the role an engineer has, and the tasks they have to perform within the collaborating group. However, individual collaboration is insufficient and engineers need to collaborate in groups. This work presents a collaboration framework for collaborating groups capable of providing synchronous and asynchronous mode of collaboration. Additionally, our approach enables engineers to mix these collaboration modes to meet the preferences of individual group members. We evaluate the scalability of this framework using four real life large collaboration projects. These projects were found from GitHub and they were under active development by the time of evaluation. We have tested our approach creating groups of different sizes for each project. The results showed that our approach scales to support every case for the groups created. Additionally, we scouted the literature and discovered studies that support the usefulness of different groups with collaboration styles.},
booktitle = {Proceedings of the 14th International Conference on Global Software Engineering},
pages = {119–128},
numpages = {10},
keywords = {collaboration, collaborating groups, change propagation, software engineering},
location = {Montreal, Quebec, Canada},
series = {ICGSE '19}
}

@inproceedings{10.1145/3107411.3108173,
author = {Eslami, Taban and Awan, Muaaz Gul and Saeed, Fahad},
title = {GPU-PCC: A GPU Based Technique to Compute Pairwise Pearson's Correlation Coefficients for Big FMRI Data},
year = {2017},
isbn = {9781450347228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107411.3108173},
doi = {10.1145/3107411.3108173},
abstract = {Functional Magnetic Resonance Imaging (fMRI) is a non-invasive brain imaging technique for studying the brain's functional activities. Pearson's Correlation Coefficient is an important measure for capturing dynamic behaviors and functional connectivity between brain components. One bottleneck in computing Correlation Coefficients is the time it takes to process big fMRI data. In this paper, we propose GPU-PCC, a GPU based algorithm based on vector dot product, which is able to compute pairwise Pearson's Correlation Coefficients while performing computation once for each pair. Our method is able to compute Correlation Coefficients in an ordered fashion without the need to do post-processing reordering of coefficients. We evaluated GPU-PCC using synthetic and real fMRI data and compared it with sequential version of computing Correlation Coefficient on CPU and existing state-of-the-art GPU method. We show that our GPU-PCC runs 94.62x faster as compared to the CPU version and 4.28x faster than the existing GPU based technique on a real fMRI dataset of size 90k voxels. The implemented code is available as GPL license on GitHub portal of our lab at https://github.com/pcdslab/GPU-PCC.},
booktitle = {Proceedings of the 8th ACM International Conference on Bioinformatics, Computational Biology,and Health Informatics},
pages = {723–728},
numpages = {6},
keywords = {GPU, time series, CUDA, FMRI, Pearson's correlation coefficient},
location = {Boston, Massachusetts, USA},
series = {ACM-BCB '17}
}

@inproceedings{10.1145/3388440.3412418,
author = {Li, Yue and Nair, Pratheeksha and Wen, Zhi and Chafi, Imane and Okhmatovskaia, Anya and Powell, Guido and Shen, Yannan and Buckeridge, David},
title = {Global Surveillance of COVID-19 by Mining News Media Using a Multi-Source Dynamic Embedded Topic Model},
year = {2020},
isbn = {9781450379649},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388440.3412418},
doi = {10.1145/3388440.3412418},
abstract = {As the COVID-19 pandemic continues to unfold, understanding the global impact of non-pharmacological interventions (NPI) is important for formulating effective intervention strategies, particularly as many countries prepare for future waves. We used a machine learning approach to distill latent topics related to NPI from large-scale international news media. We hypothesize that these topics are informative about the timing and nature of implemented NPI, dependent on the source of the information (e.g., local news versus official government announcements) and the target countries. Given a set of latent topics associated with NPI (e.g., self-quarantine, social distancing, online education, etc), we assume that countries and media sources have different prior distributions over these topics, which are sampled to generate the news articles. To model the source-specific topic priors, we developed a semi-supervised, multi-source, dynamic, embedded topic model. Our model is able to simultaneously infer latent topics and learn a linear classifier to predict NPI labels using the topic mixtures as input for each news article. To learn these models, we developed an efficient end-to-end amortized variational inference algorithm. We applied our models to news data collected and labelled by the World Health Organization (WHO) and the Global Public Health Intelligence Network (GPHIN). Through comprehensive experiments, we observed superior topic quality and intervention prediction accuracy, compared to the baseline embedded topic models, which ignore information on media source and intervention labels. The inferred latent topics reveal distinct policies and media framing in different countries and media sources, and also characterize reaction to COVID-19 and NPI in a semantically meaningful manner. Our PyTorch code is available on Github (htps://github.com/li-lab-mcgill/covid19_media).},
booktitle = {Proceedings of the 11th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics},
articleno = {34},
numpages = {14},
keywords = {Bayesian inference, Topic models, media news, coronavirus, text mining},
location = {Virtual Event, USA},
series = {BCB '20}
}

@inproceedings{10.1109/MSR.2017.42,
author = {Omran, Fouad Nasser A Al and Treude, Christoph},
title = {Choosing an NLP Library for Analyzing Software Documentation: A Systematic Literature Review and a Series of Experiments},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.42},
doi = {10.1109/MSR.2017.42},
abstract = {To uncover interesting and actionable information from natural language documents authored by software developers, many researchers rely on "out-of-the-box" NLP libraries. However, software artifacts written in natural language are different from other textual documents due to the technical language used. In this paper, we first analyze the state of the art through a systematic literature review in which we find that only a small minority of papers justify their choice of an NLP library. We then report on a series of experiments in which we applied four state-of-the-art NLP libraries to publicly available software artifacts from three different sources. Our results show low agreement between different libraries (only between 60% and 71% of tokens were assigned the same part-of-speech tag by all four libraries) as well as differences in accuracy depending on source: For example, spaCy achieved the best accuracy on Stack Overflow data with nearly 90% of tokens tagged correctly, while it was clearly outperformed by Google's SyntaxNet when parsing GitHub ReadMe files. Our work implies that researchers should make an informed decision about the particular NLP library they choose and that customizations to libraries might be necessary to achieve good results when analyzing software artifacts written in natural language.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {187–197},
numpages = {11},
keywords = {natural language processing, part-of-speech tagging, software documentation, NLP libraries},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1145/2635868.2661678,
author = {Thung, Ferdian and Le, Tien-Duy B. and Kochhar, Pavneet Singh and Lo, David},
title = {BugLocalizer: Integrated Tool Support for Bug Localization},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2661678},
doi = {10.1145/2635868.2661678},
abstract = { To manage bugs that appear in a software, developers often make use of a bug tracking system such as Bugzilla. Users can report bugs that they encounter in such a system. Whenever a user reports a new bug report, developers need to read the summary and description of the bug report and manually locate the buggy files based on this information. This manual process is often time consuming and tedious. Thus, a number of past studies have proposed bug localization techniques to automatically recover potentially buggy files from bug reports. Unfortunately, none of these techniques are integrated to bug tracking systems and thus it hinders their adoption by practitioners. To help disseminate research in bug localization to practitioners, we develop a tool named BugLocalizer, which is implemented as a Bugzilla extension and builds upon a recently proposed bug localization technique. Our tool extracts texts from summary and description fields of a bug report and source code files. It then computes similarities of the bug report with source code files to find the buggy files. Developers can use our tool online from a Bugzilla web interface by providing a link to a git source code repository and specifying the version of the repository to be analyzed. We have released our tool publicly in GitHub, which is available at: https://github.com/smagsmu/buglocalizer. We have also provided a demo video, which can be accessed at: http://youtu.be/iWHaLNCUjBY. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {767–770},
numpages = {4},
keywords = {Bugzilla, git, Bug localization},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.5555/3155562.3155689,
author = {Lin, Jinfeng and Liu, Yalin and Guo, Jin and Cleland-Huang, Jane and Goss, William and Liu, Wenchuang and Lohar, Sugandha and Monaikul, Natawut and Rasin, Alexander},
title = {TiQi: A Natural Language Interface for Querying Software Project Data},
year = {2017},
isbn = {9781538626849},
publisher = {IEEE Press},
abstract = { Abstract—Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo. Keywords-Natural Language Interface, Project Data, Query },
booktitle = {Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
pages = {973–977},
numpages = {5},
keywords = {Project Data, Query, Natural Language Interface},
location = {Urbana-Champaign, IL, USA},
series = {ASE 2017}
}

@inproceedings{10.1145/3332186.3332217,
author = {Tward, Daniel and Kolasny, Anthony and Khan, Fatima and Troncoso, Juan and Miller, Michael},
title = {Expanding the Computational Anatomy Gateway from Clinical Imaging to Basic Neuroscience Research},
year = {2019},
isbn = {9781450372275},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332186.3332217},
doi = {10.1145/3332186.3332217},
abstract = {The Computational Anatomy Gateway, powered largely by the Comet (San Diego Super-computer Center) and Stampede (Texas Advanced Computing Center) clusters through XSEDE, provides software as a service tools for atlas based analysis of human brain magnetic resonance images. This includes deformable registration, automatic labeling of tissue types, and morphometric analysis. Our goal is to extend these services to the broader neuroscience community, accommodating multiple model organisms and imaging modalities, as well as low quality or missing data. We developed a new approach to multimodality registration: by predicting one modality from another, we can replace ad hoc image similarity metrics (such as mutual information or normalized cross correlation) with a log likelihood under a noise model. This statistical approach enables us to account for missing data using the Expectation Maximization algorithm. For portability and scalability we have implemented this algorithm in tensorflow. For accessibility we have compiled and many working examples for multiple model organisms, imaging systems, and missing tissue or image anomaly situations. These examples are made easily usable in the form of Jupyter notebooks, and made publicly available through github. This framework will significantly reduce the barrier to entry for basic neuroscientists, enabling the community to benefit from atlas based computational image analysis techniques.},
booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
articleno = {9},
numpages = {6},
keywords = {neuroimaging, brain mapping, image registration},
location = {Chicago, IL, USA},
series = {PEARC '19}
}

@inproceedings{10.1145/3314221.3314648,
author = {Chibotaru, Victor and Bichsel, Benjamin and Raychev, Veselin and Vechev, Martin},
title = {Scalable Taint Specification Inference with Big Code},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314648},
doi = {10.1145/3314221.3314648},
abstract = {We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library APIs (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on GitHub), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7,000 API roles from over 210,000 candidate APIs with very little supervision (less than 300 annotations) and with high estimated precision (67%). Further, using the learned specifications, our taint analyzer flagged more than 20,000 violations in open source projects, 97% of which were undetectable without the inferred specifications.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {760–774},
numpages = {15},
keywords = {Taint Analysis, Specification Inference, Big Code},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1109/FIE43999.2019.9028657,
author = {Rahman, Md Mahmudur and Paudel, Roshan and Sharker, Monir H},
title = {Effects of Infusing Interactive and Collaborative Learning to Teach an Introductory Programming Course},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/FIE43999.2019.9028657},
doi = {10.1109/FIE43999.2019.9028657},
abstract = {This Innovate Practice Full Paper presents positive effects in teaching an introductory programming course in Python by infusing both interactive and collaborative learning. For a dynamic classroom, we used an interactive computer programming environment, Repl.it, as a top-level shell and created several in-class exercises, assignments, small lab-based projects. In addition, we used an eBook, which offers an animation and software visualization tool where students can step through code line-by-line and a program editing and execution area where students can execute examples, change them, and execute the updated code. We also introduced collaborative learning at the beginning of this introductory programming course in the form of doing team projects submitted at the end of the semester. The students were instructed to commit code to GitHub which ensures that their work will not be lost as well as, provide them basic task management tools to collaborate. The proposed pedagogical approaches were applied in the Fall’2017 semester to teach an introductory CS course in Python. The traditional course instruction that has historically been used in the department are used as the control group. For evaluation and result analysis, thirteen sections of COSC 111 were included in this study over three semesters: Fall 2014, Fall 2016 and Fall 2017. The initial evaluation of summative assessment and analysis of the survey results enable us to conclude that the proposed instructional approach increased student motivation and engagement, facilitated learning, and contributed to the progress of students in this course as well as reduced the failure rates.},
booktitle = {2019 IEEE Frontiers in Education Conference (FIE)},
pages = {1–8},
numpages = {8},
location = {Covington, KY, USA}
}

@inproceedings{10.1145/3180155.3180167,
author = {Gu, Xiaodong and Zhang, Hongyu and Kim, Sunghun},
title = {Deep Code Search},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180167},
doi = {10.1145/3180155.3180167},
abstract = {To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {933–944},
numpages = {12},
keywords = {deep learning, code search, joint embedding},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3366423.3380272,
author = {Maldeniya, Danaja and Budak, Ceren and Robert Jr., Lionel P. and Romero, Daniel M.},
title = {Herding a Deluge of Good Samaritans: How GitHub Projects Respond to Increased Attention},
year = {2020},
isbn = {9781450370233},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366423.3380272},
doi = {10.1145/3366423.3380272},
abstract = {Collaborative crowdsourcing is a well-established model of work, especially in the case of open source software development. The structure and operation of these virtual and loosely-knit teams differ from traditional organizations. As such, little is known about how their behavior may change in response to an increase in external attention. To understand these dynamics, we analyze millions of actions of thousands of contributors in over 1100 open source software projects that topped the GitHub Trending Projects page and thus experienced a large increase in attention, in comparison to a control group of projects identified through propensity score matching. In carrying out our research, we use the lens of organizational change, which considers the challenges teams face during rapid growth and how they adapt their work routines, organizational structure, and management style. We show that trending results in an explosive growth in the effective team size. However, most newcomers make only shallow and transient contributions. In response, the original team transitions towards administrative roles, responding to requests and reviewing work done by newcomers. Projects evolve towards a more distributed coordination model with newcomers becoming more central, albeit in limited ways. Additionally, teams become more modular with subgroups specializing in different aspects of the project. We discuss broader implications for collaborative crowdsourcing teams that face attention shocks. },
booktitle = {Proceedings of The Web Conference 2020},
pages = {2055–2065},
numpages = {11},
keywords = {GitHub, attention shocks, crowdsourcing, PSM, coordination},
location = {Taipei, Taiwan},
series = {WWW '20}
}

@inproceedings{10.1109/ICSE.2019.00058,
author = {Arya, Deeksha and Wang, Wenting and Guo, Jin L. C. and Cheng, Jinghui},
title = {Analysis and Detection of Information Types of Open Source Software Issue Discussions},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00058},
doi = {10.1109/ICSE.2019.00058},
abstract = {Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {454–464},
numpages = {11},
keywords = {collaborative software engineering, issue tracking system, issue discussion analysis},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3364641.3364648,
author = {Oliveira, Johnatan and Viggiato, Markos and Figueiredo, Eduardo},
title = {How Well Do You Know This Library? Mining Experts from Source Code Analysis},
year = {2019},
isbn = {9781450372824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3364641.3364648},
doi = {10.1145/3364641.3364648},
abstract = {Third-party libraries have been widely adopted in modern software projects due to several benefits, such as code reuse and software quality. Software development is increasingly complex and requires specialists with knowledge in several technologies, such as the nowadays libraries. Such complexity turns it extremely challenging to deliver quality software given the time pressure. For this purpose, it is necessary to identify and hire qualified developers, to obtain a good team, both in open source and proprietary systems. For these reasons, enterprise and open source projects try to build teams composed of highly skilled developers in specific libraries. Developers with expertise in specific libraries may reduce the time spent on software development tasks and improve the quality of the final product. However, their identification may not be trivial. In this paper, we first argue that source code activities can be used to identify library experts. We then evaluate a mining-based strategy to identify library experts. To achieve our goal, we selected the 9 most popular Java libraries and identified the top-10 experts in each library by analyzing commits in 16,703 Java projects on GitHub. We validated the results by applying a survey with 137 library expert candidates and observed, on average, 88% of precision for the applied strategy.},
booktitle = {Proceedings of the XVIII Brazilian Symposium on Software Quality},
pages = {49–58},
numpages = {10},
keywords = {Mining Software Repositories, Expert Identification, Software Skills, Library Experts},
location = {Fortaleza, Brazil},
series = {SBQS'19}
}

@inproceedings{10.1145/2656434.2656440,
author = {Kim, William and Chung, Sam and Endicott-Popovsky, Barbara},
title = {Software Architecture Model Driven Reverse Engineering Approach to Open Source Software Development},
year = {2014},
isbn = {9781450327114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656434.2656440},
doi = {10.1145/2656434.2656440},
abstract = {Popular Open Source Software (OSS) development platforms like GitHub, Google Code, and Bitbucket take advantage of some best practices of traditional software development like version control and issue tracking. Current major open source software environments, including IDE tools and online code repositories, do not provide support for visual architecture modeling. Research has shown that visual modeling of complex software projects has benefits throughout the software lifecycle. Then why is it that software architecture modeling is so conspicuously missing from popular online open source code repositories? How can including visual documentation improve the overall quality of open source software projects? Our goal is to answer both of these questions and bridge the gap between traditional software engineering best practices and open source development by applying a software architecture documentation methodology using Unified Modeling Language, called 5W1H Re-Doc, on a real open source project for managing identity and access, MITREid Connect. We analyze the effect of a model-driven software engineering approach on collaboration of open source contributors, quality of specification conformance, and state-of-the-art of architecture modeling. Our informal experiment revealed that in some cases, having the visual documentation can significantly increase comprehension of an online OSS project over having only the textual information that currently exists for that project.},
booktitle = {Proceedings of the 3rd Annual Conference on Research in Information Technology},
pages = {9–14},
numpages = {6},
keywords = {open source software development, software architecture documentation, model-driven software engineering},
location = {Atlanta, Georgia, USA},
series = {RIIT '14}
}

@inproceedings{10.1145/3368089.3409764,
author = {Mahajan, Sonal and Abolhassani, Negarsadat and Prasad, Mukul R.},
title = {Recommending Stack Overflow Posts for Fixing Runtime Exceptions Using Failure Scenario Matching},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409764},
doi = {10.1145/3368089.3409764},
abstract = {Using online Q&amp;A forums, such as Stack Overflow (SO), for guidance to resolve program bugs, among other development issues, is commonplace in modern software development practice. Runtime exceptions (RE) is one such important class of bugs that is actively discussed on SO. In this work we present a technique and prototype tool called MAESTRO that can automatically recommend an SO post that is most relevant to a given Java RE in a developer's code. MAESTRO compares the exception-generating program scenario in the developer's code with that discussed in an SO post and returns the post with the closest match. To extract and compare the exception scenario effectively, MAESTRO first uses the answer code snippets in a post to implicate a subset of lines in the post's question code snippet as responsible for the exception and then compares these lines with the developer's code in terms of their respective Abstract Program Graph (APG) representations. The APG is a simplified and abstracted derivative of an abstract syntax tree, proposed in this work, that allows an effective comparison of the functionality embodied in the high-level program structure, while discarding many of the low-level syntactic or semantic differences. We evaluate MAESTRO on a benchmark of 78 instances of Java REs extracted from the top 500 Java projects on GitHub and show that MAESTRO can return either a highly relevant or somewhat relevant SO post corresponding to the exception instance in 71% of the cases, compared to relevant posts returned in only 8% - 44% instances, by four competitor tools based on state-of-the-art techniques. We also conduct a user experience study of MAESTRO with 10 Java developers, where the participants judge MAESTRO reporting a highly relevant or somewhat relevant post in 80% of the instances. In some cases the post is judged to be even better than the one manually found by the participant.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1052–1064},
numpages = {13},
keywords = {code search, crowd intelligence, runtime exceptions, static analysis},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3243734.3243738,
author = {Abuhamad, Mohammed and AbuHmed, Tamer and Mohaisen, Aziz and Nyang, DaeHun},
title = {Large-Scale and Language-Oblivious Code Authorship Identification},
year = {2018},
isbn = {9781450356930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243734.3243738},
doi = {10.1145/3243734.3243738},
abstract = {Efficient extraction of code authorship attributes is key for successful identification. However, the extraction of such attributes is very challenging, due to various programming language specifics, the limited number of available code samples per author, and the average code lines per file, among others. To this end, this work proposes a Deep Learning-based Code Authorship Identification System (DL-CAIS) for code authorship attribution that facilitates large-scale, language-oblivious, and obfuscation-resilient code authorship identification. The deep learning architecture adopted in this work includes TF-IDF-based deep representation using multiple Recurrent Neural Network (RNN) layers and fully-connected layers dedicated to authorship attribution learning. The deep representation then feeds into a random forest classifier for scalability to de-anonymize the author. Comprehensive experiments are conducted to evaluate DL-CAIS over the entire Google Code Jam (GCJ) dataset across all years (from 2008 to 2016) and over real-world code samples from 1987 public repositories on GitHub. The results of our work show the high accuracy despite requiring a smaller number of files per author. Namely, we achieve an accuracy of 96% when experimenting with 1,600 authors for GCJ, and 94.38% for the real-world dataset for 745 C programmers. Our system also allows us to identify 8,903 authors, the largest-scale dataset used by far, with an accuracy of 92.3%. Moreover, our technique is resilient to language-specifics, and thus it can identify authors of four programming languages (e.g. C, C++, Java, and Python), and authors writing in mixed languages (e.g. Java/C++, Python/C++). Finally, our system is resistant to sophisticated obfuscation (e.g. using C Tigress) with an accuracy of 93.42% for a set of 120 authors.},
booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
pages = {101–114},
numpages = {14},
keywords = {software forensics, program features, code authorship identification, deep learning identification},
location = {Toronto, Canada},
series = {CCS '18}
}

@inproceedings{10.1145/3328778.3366905,
author = {Barowy, Daniel W. and Jannen, William K.},
title = {Infrastructor: Flexible, No-Infrastructure Tools for Scaling CS},
year = {2020},
isbn = {9781450367936},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3328778.3366905},
doi = {10.1145/3328778.3366905},
abstract = {Demand for computer science education has skyrocketed in the last decade. Although challenging everywhere, scaling up CS course capacities is especially painful at small, liberal arts colleges (SLACs). SLACs tend to have few instructors, few large-capacity classrooms, and little or no dedicated IT support staff. As CS enrollment growth continues to outpace the ability to hire instructional staff, maintaining the quality of the close, nurturing learning environment that SLACs advertise-and students expect-is a major challenge.We present Infrastructor, a workflow and collection of course scaling tools that address the needs of resource-strapped CS departments. Infrastructor removes unnecessary administrative burdens so that instructors can focus on teaching and mentoring students. Unlike a traditional learning management system (LMS), which is complex, monolithic, and usually administered by a campus-wide IT staff, instructors deploy Infrastructor themselves and can trivially tailor the software to suit their own needs. Notably, Infrastructor does not require local hardware resources or platform-specific tools. Instead, Infrastructor is built on top of version control systems. This design choice lets instructors host courses on commodity, cloud-based repositories like GitHub. Since developing Infrastructor two years ago, we have successfully deployed it in ten sections of CS courses (323 students), and over the next year, we plan to more than double its use in our CS program.},
booktitle = {Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
pages = {1005–1011},
numpages = {7},
keywords = {capacity scaling, github, enrollment crunch, course workflow},
location = {Portland, OR, USA},
series = {SIGCSE '20}
}

@inproceedings{10.1145/3127005.3127014,
author = {Thompson, Christopher and Wagner, David},
title = {A Large-Scale Study of Modern Code Review and Security in Open Source Projects},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127014},
doi = {10.1145/3127005.3127014},
abstract = {Background: Evidence for the relationship between code review process and software security (and software quality) has the potential to help improve code review automation and tools, as well as provide a better understanding of the economics for improving software security and quality. Prior work in this area has primarily been limited to case studies of a small handful of software projects. Aims: We investigate the effect of modern code review on software security. We extend and generalize prior work that has looked at code review and software quality. Method: We gather a very large dataset from GitHub (3,126 projects in 143 languages, with 489,038 issues and 382,771 pull requests), and use a combination of quantification techniques and multiple regression modeling to study the relationship between code review coverage and participation and software quality and security. Results: We find that code review coverage has a significant effect on software security. We confirm prior results that found a relationship between code review coverage and software defects. Most notably, we find evidence of a negative relationship between code review of pull requests and the number of security bugs reported in a project. Conclusions: Our results suggest that implementing code review policies within the pull request model of development may have a positive effect on the quality and security of software.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {83–92},
numpages = {10},
keywords = {software security, multiple regression models, code review, quantification models, mining software repositories, software quality},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1109/MSR.2019.00039,
author = {Campos, Uriel and Smethurst, Guilherme and Moraes, Jo\~{a}o Pedro and Bonif\'{a}cio, Rodrigo and Pinto, Gustavo},
title = {Mining Rule Violations in JavaScript Code Snippets},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2019.00039},
doi = {10.1109/MSR.2019.00039},
abstract = {Programming code snippets readily available on platforms such as StackOverflow are undoubtedly useful for software engineers. Unfortunately, these code snippets might contain issues such as deprecated, misused, or even buggy code. These issues could pass unattended, if developers do not have adequate knowledge, time, or tool support to catch them. In this work we expand the understanding of such issues (or the so called "violations") hidden in code snippets written in JavaScript, the programming language with the highest number of questions on StackOverflow. To characterize the violations, we extracted 336k code snippets from answers to JavaScript questions on StackOverflow and statically analyzed them using ESLinter, a JavaScript linter. We discovered that there is no single JavaScript code snippet without a rule violation. On average, our studied code snippets have 11 violations, but we found instances of more than 200 violations. In particular, rules related to stylistic issues are by far the most violated ones (82.9% of the violations pertain to this category). Possible errors, which developers might be more interested in, represent only 0.1% of the violations. Finally, we found a small fraction of code snippets flagged with possible errors being reused on actual GitHub software projects. Indeed, one single code snippet with possible errors was reused 1,261 times.},
booktitle = {Proceedings of the 16th International Conference on Mining Software Repositories},
pages = {195–199},
numpages = {5},
keywords = {rule violations, JavaScript code snippets, ES-linter},
location = {Montreal, Quebec, Canada},
series = {MSR '19}
}

@inproceedings{10.1145/3196398.3196428,
author = {Cassee, Nathan and Pinto, Gustavo and Castor, Fernando and Serebrenik, Alexander},
title = {How Swift Developers Handle Errors},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196428},
doi = {10.1145/3196398.3196428},
abstract = {Swift is a new programming language developed by Apple as a replacement to Objective-C. It features a sophisticated error handling (EH) mechanism that provides the kind of separation of concerns afforded by exception handling mechanisms in other languages, while also including constructs to improve safety and maintainability. However, Swift also inherits a software development culture stemming from Objective-C being the de-facto standard programming language for Apple platforms for the last 15 years. It is, therefore, a priori unclear whether Swift developers embrace the novel EH mechanisms of the programming language or still rely on the old EH culture of Objective-C even working in Swift.In this paper, we study to what extent developers adhere to good practices exemplified by EH guidelines and tutorials, and what are the common bad EH practices particularly relevant to Swift code. Furthermore, we investigate whether perception of these practices differs between novices and experienced Swift developers.To answer these questions we employ a mixed-methods approach and combine 10 semi-structured interviews with Swift developers and quantitative analysis of 78,760 Swift 4 files extracted from 2,733 open-source GitHub repositories. Our findings indicate that there is ample opportunity to improve the way Swift developers use error handling mechanisms. For instance, some recommendations derived in this work are not well spread in the corpus of studied Swift projects. For example, generic catch handlers are common in Swift (even though it is not uncommon for them to share space with their counterparts: non empty catch handlers), custom, developerdefined error types are rare, and developers are mostly reactive when it comes to error handling, using Swift's constructs mostly to handle errors thrown by libraries, instead of throwing and handling application-specific errors.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {292–302},
numpages = {11},
keywords = {error handling, language feature usage, swift},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/2896825.2896833,
author = {Wilder, Nathan and Smith, Jared M. and Mockus, Audris},
title = {Exploring a Framework for Identity and Attribute Linking across Heterogeneous Data Systems},
year = {2016},
isbn = {9781450341523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896825.2896833},
doi = {10.1145/2896825.2896833},
abstract = {Online-activity-generated digital traces provide opportunities for novel services and unique insights as demonstrated in, for example, research on mining software repositories. The inability to link these traces within and among systems, such as Twitter, GitHub, or Reddit, inhibit the advances in this area. Furthermore, no single approach to integrate data from these disparate sources is likely to work. We aim to design Foreseer, an extensible framework, to design and evaluate identity matching techniques for public, large, and low-accuracy operational data. Foreseer consists of three functionally independent components designed to address the issues of discovery and preparation, storage and representation, and analysis and linking of traces from disparate online sources. The framework includes a domain specific language for manipulating traces, generating insights, and building novel services. We have applied it in a pilot study of roughly 10TB of data from Twitter, Reddit, and StackExchange including roughly 6M distinct entities and, using basic matching techniques, found roughly 83,000 matches among these sources. We plan to add additional entity extraction and identification algorithms, data from other sources, and design tools for facilitating dynamic ingestion and tagging of incoming data on a more robust infrastructure using Apache Spark or another distributed processing framework. We will then evaluate the utility and effectiveness of the framework in applications ranging from identifying malicious contributors in software repositories to the evaluation of the utility of privacy preservation schemes.},
booktitle = {Proceedings of the 2nd International Workshop on BIG Data Software Engineering},
pages = {19–25},
numpages = {7},
keywords = {big data architecture, entity extraction, domain specific language, entity identification, identity linking},
location = {Austin, Texas},
series = {BIGDSE '16}
}

@inproceedings{10.1109/ASE.2019.00081,
author = {Alizadeh, Vahid and Ouali, Mohamed Amine and Kessentini, Marouane and Chater, Meriem},
title = {RefBot: Intelligent Software Refactoring Bot},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00081},
doi = {10.1109/ASE.2019.00081},
abstract = {The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost.In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any "open" or "merge" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {823–834},
numpages = {12},
keywords = {refactoring, quality, Software bot},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3368089.3409699,
author = {Pan, Linjie and Cui, Baoquan and Liu, Hao and Yan, Jiwei and Wang, Siqi and Yan, Jun and Zhang, Jian},
title = {Static Asynchronous Component Misuse Detection for Android Applications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409699},
doi = {10.1145/3368089.3409699},
abstract = {Facing the limited resource of smartphones, asynchronous programming significantly improves the performance of Android applications. Android provides several packaged components to ease the development of asynchronous programming. Among them, the AsyncTask component is widely used by developers since it is easy to implement. However, the abuse of AsyncTask component can decrease responsiveness and even lead to crashes. By investigating the Android Developer Documentation and technical forums, we summarize five misuse patterns about AsyncTask. To detect them, we propose a flow, context, object and field-sensitive inter-procedural static analysis approach. Specifically, the static analysis includes typestate analysis, reference analysis and loop analysis. Based on the AsyncTask-related information obtained during static analysis, we check the misuse according to predefined detection rules. The proposed approach is implemented into a tool called AsyncChecker. We evaluate AsyncChecker on a self-designed benchmark suite called AsyncBench and 1,759 real-world apps. AsyncChecker finds 17,946 misused AsyncTask instances in 1,417 real-world apps (80.6%). The precision, recall and F-measure of AsyncChecker on real-world applications are 97.2%, 89.8% and 0.93, respectively. Compared with existing tools, AsyncChecker can detect more asynchronous problems. We report the misuse problems to developers via GitHub. Several developers have confirmed and fixed the problems found by AsyncChecker. The result implies that our approach is effective and developers do take the misuse of AsyncTask as a serious problem.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {952–963},
numpages = {12},
keywords = {AsyncTask, Misuse Detection, Android, Asynchronous Programming, Static Analysis},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3379597.3387477,
author = {Sousa, Leonardo and Cedrim, Diego and Garcia, Alessandro and Oizumi, Willian and Bibiano, Ana C. and Oliveira, Daniel and Kim, Miryung and Oliveira, Anderson},
title = {Characterizing and Identifying Composite Refactorings: Concepts, Heuristics and Patterns},
year = {2020},
isbn = {9781450375177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379597.3387477},
doi = {10.1145/3379597.3387477},
abstract = {Refactoring consists of a transformation applied to improve the program internal structure, for instance, by contributing to remove code smells. Developers often apply multiple interrelated refactorings called composite refactoring. Even though composite refactoring is a common practice, an investigation from different points of view on how composite refactoring manifests in practice is missing. Previous empirical studies also neglect how different kinds of composite refactorings affect the removal, prevalence or introduction of smells. To address these matters, we provide a conceptual framework and two heuristics to respectively characterize and identify composite refactorings within and across commits. Then, we mined the commit history of 48 GitHub software projects. We identified and analyzed 24,911 composite refactorings involving 104,505 single refactorings. Amongst several findings, we observed that most composite refactorings occur in the same commit and have the same refactoring type. We found that several refactorings are semantically related to each other, which occur in different parts of the system but are still related to the same task. Our study is the first to reveal that many smells are introduced in a program due to "incomplete" composite refactorings. Our study is also the first to reveal 111 patterns of composite refactorings that frequently introduce or remove certain smell types. These patterns can be used as guidelines for developers to improve their refactoring practices as well as for designers of recommender systems.},
booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
pages = {186–197},
numpages = {12},
location = {Seoul, Republic of Korea},
series = {MSR '20}
}

@inproceedings{10.1109/ICSM.2015.7332449,
author = {Vendome, Christopher and Linares-Vasquez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel M. and Poshyvanyk, Denys},
title = {When and Why Developers Adopt and Change Software Licenses},
year = {2015},
isbn = {9781467375320},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSM.2015.7332449},
doi = {10.1109/ICSM.2015.7332449},
abstract = {Software licenses legally govern the way in which developers can use, modify, and redistribute a particular system. While previous studies either investigated licensing through mining software repositories or studied licensing through FOSS reuse, we aim at understanding the rationale behind developers' decisions for choosing or changing software licensing by surveying open source developers. In this paper, we analyze when developers consider licensing, the reasons why developers pick a license for their project, and the factors that influence licensing changes. Additionally, we explore the licensing-related problems that developers experienced and expectations they have for licensing support from forges (e.g., GitHub). Our investigation involves, on one hand, the analysis of the commit history of 16,221 Java open source projects to identify the commits where licenses were added or changed. On the other hand, it consisted of a survey—in which 138 developers informed their involvement in licensing-related decisions and 52 provided deeper insights about the rationale behind the actions that they had undertaken. The results indicate that developers adopt licenses early in the project's development and change licensing after some period of development (if at all). We also found that developers have inherent biases with respect to software licensing. Additionally, reuse—whether by a non-contributor or for commercial purposes—is a dominant reason why developers change licenses of their systems. Finally, we discuss potential areas of research that could ameliorate the difficulties that software developers are facing with regard to licensing issues of their software systems.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
pages = {31–40},
numpages = {10},
series = {ICSME '15}
}

