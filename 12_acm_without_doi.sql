--
-- PostgreSQL database dump
--

-- Dumped from database version 13.1 (Debian 13.1-1.pgdg100+1)
-- Dumped by pg_dump version 13.1 (Debian 13.1-1.pgdg100+1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: acm_manual; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.acm_manual (
    id integer NOT NULL,
    author text,
    title text,
    doi text,
    year text,
    abstract text,
    url text
);


ALTER TABLE public.acm_manual OWNER TO postgres;

--
-- Name: acm_manual_id_seq; Type: SEQUENCE; Schema: public; Owner: postgres
--

CREATE SEQUENCE public.acm_manual_id_seq
    AS integer
    START WITH 1
    INCREMENT BY 1
    NO MINVALUE
    NO MAXVALUE
    CACHE 1;


ALTER TABLE public.acm_manual_id_seq OWNER TO postgres;

--
-- Name: acm_manual_id_seq; Type: SEQUENCE OWNED BY; Schema: public; Owner: postgres
--

ALTER SEQUENCE public.acm_manual_id_seq OWNED BY public.acm_manual.id;


--
-- Name: acm_manual id; Type: DEFAULT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.acm_manual ALTER COLUMN id SET DEFAULT nextval('public.acm_manual_id_seq'::regclass);


--
-- Data for Name: acm_manual; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.acm_manual (id, author, title, doi, year, abstract, url) FROM stdin;
5	Acar, Yasemin and Stransky, Christian and Wermke, Dominik and Mazurek, Michelle L. and Fahl, Sascha	Security Developer Studies with Github Users: Exploring a Convenience Sample		2017	The usable security community is increasingly considering how to improve security decision-making not only for end users, but also for information technology professionals, including system administrators and software developers. Recruiting these professionals for user studies can prove challenging, as, relative to end users more generally, they are limited in numbers, geographically concentrated, and accustomed to higher compensation. One potential approach is to recruit active GitHub users, who are (in some ways) conveniently available for online studies. However, it is not well understood how GitHub users perform when working on security-related tasks. As a first step in addressing this question, we conducted an experiment in which we recruited 307 active GitHub users to each complete the same security-relevant programming tasks. We compared the results in terms of functional correctness as well as security, finding differences in performance for both security and functionality related to the participant's self-reported years of experience, but no statistically significant differences related to the participant's self-reported status as a student, status as a professional developer, or security background. These results provide initial evidence for how to think about validity when recruiting convenience samples as substitutes for professional developers in security developer studies.	
9	Badashian, Ali Sajedi and Esteki, Afsaneh and Gholipour, Ameneh and Hindle, Abram and Stroulia, Eleni	Involvement, Contribution and Influence in GitHub and Stack Overflow		2014	Software developers are increasingly adopting social-media platforms to contribute to software development, learn and develop a reputation for themselves. GitHub supports version-controlled code sharing and social-networking functionalities and Stack Overflow is a social forum for question answering on programming topics. Motivated by the features' overlap of the two networks, we set out to mine and analyze and correlate the members' core contributions, editorial activities and influence in the two networks. We aim to better understand the similarities and differences of the members' contributions in the two platforms and their evolution over time. In this context, while studying the activities of different user groups, we conducted a three-step investigation of GitHub activity, Stack Overflow activity and inter-network activity over a five-year period. We report our findings on interesting membership and activity patterns within each platform and some relations between the two.	
12	Gousios, Georgios and Spinellis, Diomidis	GHTorrent: GitHub's Data from a Firehose		2012	A common requirement of many empirical software engineering studies is the acquisition and curation of data from software repositories. During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive rest api, which enables researchers to retrieve both the commits to the projects' repositories and events generated through user actions on project resources. GHTorrent aims to create a scalable off line mirror of GitHub's event streams and persistent data, and offer it to the research community as a service. In this paper, we present the project's design and initial implementation and demonstrate how the provided datasets can be queried and processed.	
17	Coelho, Roberta and Almeida, Lucas and Gousios, Georgios and van Deursen, Arie	Unveiling Exception Handling Bug Hazards in Android Based on GitHub and Google Code Issues		2015	This paper reports on a study mining the exception stack traces included in 159,048 issues reported on Android projects hosted in GitHub (482 projects) and Google Code (157 projects). The goal of this study is to investigate whether stack trace information can reveal bug hazards related to exception handling code that may lead to a decrease in application robustness. Overall 6,005 exception stack traces were extracted, and subjected to source code and bytecode analysis. The outcomes of this study include the identification of the following bug hazards: (i) unexpected cross-type exception wrappings (for instance, trying to handle an instance of OutOfMemoryError "hidden" in a checked exception) which can make the exception-related code more complex and negatively impact the application robustness; (ii) undocumented runtime exceptions thrown by both the Android platform and third party libraries; and (iii) undocumented checked exceptions thrown by the Android Platform. Such undocumented exceptions make it difficult, and most of the times infeasible for the client code to protect against "unforeseen" situations that may happen while calling third-party code. This study provides further insights on such bug hazards and the robustness threats they impose to Android apps as well as to other systems based on the Java exception model.	
28	Liu, Gary and Siu, Joran and Dawson, Michael and Ho, Ivy and Yan, Yunliang	Introduction to Debugging and Monitoring Node.Js		2015	Node.js is a server-side JavaScript platform that has seen explosive growth in recent years. In its 6 years of existence, Node.js has leaped to being the third most-starred repository on GitHub, and is now one of the most popular frameworks for developing cloud, mobile and Internet-of-Things applications.	
29	Low, Andrew and Siu, Joran and Ho, Ivy and Liu, Gary	Introduction to Node.Js		2014	Node.js is a server-side JavaScript platform that has seen explosive growth in recent years. In its 5 years of existence, Node.js has leaped to being the third most-starred repository on GitHub, and is now one of the most popular frameworks for developing cloud, mobile and Internet-of-Things applications.	
22	Gousios, Georgios	The GHTorent Dataset and Tool Suite		2013	During the last few years, GitHub has emerged as a popular project hosting, mirroring and collaboration platform. GitHub provides an extensive REST API, which enables researchers to retrieve high-quality, interconnected data. The GHTorent project has been collecting data for all public projects available on Github for more than a year. In this paper, we present the dataset details and construction process and outline the challenges and research opportunities emerging from it. 	
23	Melo, Amanda Meincke	Acessibilidade e Inclus\\~ao Digital		2014	A promo\\cc\\~ao da acessibilidade est\\'a diretamente relacionada ao exerc\\'\\icio da cidadania. Efetiv\\'a-la no desenvolvimento de sistemas computacionais interativos para uso humano -- massivamente presentes em nosso dia a dia -- envolve ter um claro entendimento de seu significado nos dias de hoje, que deve estar alinhado \\`a defini\\cc\\~ao contempor\\^anea para defici\\^encia apresentada na Conven\\cc\\~ao Internacional sobre os Direitos das Pessoas com Defici\\^encia [2]:[...] a defici\\^encia resulta da intera\\cc\\~ao entre pessoas com defici\\^encia e as barreiras devidas \\`as atitudes e ao ambiente que impedem a plena e efetiva participa\\cc\\~ao dessas pessoas na sociedade em igual de oportunidades com as demais pessoas [...]\\'E no contexto do Desafio 4 da Sociedade Brasileira de Computa\\cc\\~ao (SBC) para o dec\\^enio 2006-2016 [1] -- "Acesso Participativo e Universal do Cidad\\~ao Brasileiro ao Conhecimento" -- e do desafio "Acessibilidade e Inclus\\~ao Digital", enunciado entre os Grandes Desafios de Pesquisa em Intera\\cc\\~ao Humano-Computador do Brasil [5], que este minicurso \\'e proposto.	
24	Zhao, Yangyang and Serebrenik, Alexander and Zhou, Yuming and Filkov, Vladimir and Vasilescu, Bogdan	The Impact of Continuous Integration on Other Software Development Practices: A Large-Scale Empirical Study		2017	Continuous Integration (CI) has become a disruptive innovation in software development: with proper tool support and adoption, positive effects have been demonstrated for pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential, and "best practices" to that end have been proposed. Here we study the adaptation and evolution of code writing and submission, issue and pull request closing, and testing practices as Travis CI is adopted by hundreds of established projects on GitHub. To help essentialize the quantitative results, we also survey a sample of GitHub developers about their experiences with adopting Travis CI. Our findings suggest a more nuanced picture of how GitHub teams are adapting to, and benefiting from, continuous integration technology than suggested by prior work. 	
25	Packer, Heather S. and Chapman, Adriane and Carr, Leslie	GitHub2PROV: Provenance for Supporting Software Project Management		2019	Software project management is a complex task that requires accurate information and experience to inform the decision-making process. In the real world software project managers rarely have access to perfect information. In order to support them, we propose leveraging information from Version Control Systems and their repositories to support decision-making. In this paper, we propose a PROV model GitHub2PROV, which extends Git2PROV with details about GitHub commits and issues from GitHub repositories. We discuss how this model supports project management decisions in agile development, specifically in terms of Control Schedule Reviews and workload.	
32	Barboza, John and Mallick, Muntasir and Siu, Joran and Bajwa, Jaideep and Dawson, Michael	Hands-on: Microservices on NodeJS		2016	Node.js is a server-side JavaScript platform that has seen explosive growth in recent years. In its 5 years of existence, Node.js has leaped to being the third most-starred repository on GitHub, and is now one of the most popular frameworks for developing cloud, mobile and Internet-of-Things applications.	
34	Singh, Dhirendra and Padgham, Lin and Logan, Brian	Integrating BDI Agents with Agent-Based Simulation Platforms: (JAAMAS Extended Abstract)		2017	This paper describes an integration framework that allows development of simulations where the cognitive reasoning and decision making is programmed and executed within an existing BDI (Belief, Desire, Intention) system, and the simulation is played out in an existing ABM (Agent Based Modelling) system. The framework has a generic layer which manages communication and synchronisation, a system layer which integrates specific BDI and ABM systems, and the application layer which contains the program code for a particular application. The code is available on GitHub: https://github.com/agentsoz/bdi-abm-integration	
37	van der Veen, Erik and Gousios, Georgios and Zaidman, Andy	Automatically Prioritizing Pull Requests		2015	In previous work, we observed that in the pull-based development model integrators face challenges with regard to prioritizing work in the face of multiple concurrent pull requests. We present the design and initial implementation of a prototype pull request prioritisation tool called prioritizer. prioritizer works like a priority inbox for pull requests, recommending the top pull requests the project owner should focus on. A preliminary user study showed that prioritizer provides functionality that GitHub is currently lacking, even though users need more insight into how the priority ranking is established to make prioritizer really useful.	
39	Watanabe, Keisuke and Ubayashi, Naoyasu and Fukamachi, Takuya and Nakamura, Shunya and Muraoka, Hokuto and Kamei, Yasutaka	IArch-U: Interface-Centric Integrated Uncertainty-Aware Development Environment		2017	Uncertainty can appear in all aspects of software development: uncertainty in requirements analysis, design decisions, implementation and testing. If uncertainty can be dealt with modularly, we can add or delete uncertain concerns to/from models, code and tests whenever these concerns arise or are fixed to certain concerns. To deal with this problem, we developed iArch-U, an IDE (Integrated Development Environment) for managing uncertainty modularly in all phases in software development. In this paper, we introduce an overview of iArch-U. The iArch-U IDE is open source software and can be downloaded from GitHub.	
40	Faria, Daniel and Pesquita, Catia and Santos, Emanuel and Cruz, Isabel F. and Couto, Francisco M.	AgreementMakerLight 2.0: Towards Efficient Large-Scale Ontology Matching		2014	Ontology matching is a critical task to realize the Semantic Web vision, by enabling interoperability between ontologies. However, handling large ontologies efficiently is a challenge, given that ontology matching is a problem of quadratic complexity.AgreementMakerLight (AML) is a scalable automated ontology matching system developed to tackle large ontology matching problems, particularly for the life sciences domain. Its new 2.0 release includes several novel features, including an innovative algorithm for automatic selection of background knowledge sources, and an updated repair algorithm that is both more complete and more efficient.AML is an open source system, and is available through GitHub both for developers (as an Eclipse project) and end-users (as a runnable Jar with a graphical user interface).	
46	Hata, Hideaki and Todo, Taiki and Onoue, Saya and Matsumoto, Kenichi	Characteristics of Sustainable OSS Projects: A Theoretical and Empirical Study		2015	How can we attract developers? What can we do to incentivize developers to write code? We started the study by introducing the population pyramid visualization to software development communities, called software population pyramids, and found a typical pattern in shapes. This pattern comes from the differences in attracting coding contributors and discussion contributors. To understand the causes of the differences, we then build game-theoretical models of the contribution situation. Based on these results, we again analyzed the projects empirically to support the outcome of the models, and found empirical evidence. The answers to the initial questions are clear. To incentivize developers to code, the projects should prepare documents, or the projects or third parties should hire developers, and these are what sustainable projects in GitHub did in reality. In addition, making innovations to reduce the writing costs can also have an impact in attracting coding contributors.	
49	Oakes, Edward and Yang, Leon and Zhou, Dennis and Houck, Kevin and Harter, Tyler and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.	SOCK: Rapid Task Provisioning with Serverless-Optimized Containers		2018	Serverless computing promises to provide applications with cost savings and extreme elasticity. Unfortunately, slow application and container initialization can hurt common-case latency on serverless platforms. In this work, we analyze Linux container primitives, identifying scalability bottlenecks related to storage and network isolation. We also analyze Python applications from GitHub and show that importing many popular libraries adds about 100 ms to startup. Based on these findings, we implement SOCK, a container system optimized for serverless workloads. Careful avoidance of kernel scalability bottlenecks gives SOCK an 18\\texttimes speedup over Docker. A generalized-Zygote provisioning strategy yields an additional 3\\texttimes speedup. A more sophisticated three-tier caching strategy based on Zygotes provides a 45\\texttimes speedup over SOCK without Zygotes. Relative to AWS Lambda and OpenWhisk, OpenLambda with SOCK reduces platform overheads by 2.8\\texttimes and 5.3\\texttimes respectively in an image processing case study.	
51	Hashimoto, Tatsunori B. and Guu, Kelvin and Oren, Yonatan and Liang, Percy	A Retrieve-and-Edit Framework for Predicting Structured Outputs		2018	For the task of generating complex outputs such as source code, editing existing outputs can be easier than generating complex outputs from scratch. With this motivation, we propose an approach that first retrieves a training example based on the input (e.g., natural language description) and then edits it to the desired output (e.g., code). Our contribution is a computationally efficient method for learning a retrieval model that embeds the input in a task-dependent way without relying on a hand-crafted metric or incurring the expense of jointly training the retriever with the editor. Our retrieve-and-edit framework can be applied on top of any base model. We show that on a new autocomplete task for GitHub Python code and the Hearthstone cards benchmark, retrieve-and-edit significantly boosts the performance of a vanilla sequence-to-sequence model on both tasks.	
52	Zheng, Renjie and Chen, Junkun and Qiu, Xipeng	Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks		2018	Distributed representation plays an important role in deep learning based natural language processing. However, the representation of a sentence often varies in different tasks, which is usually learned from scratch and suffers from the limited amounts of training data. In this paper, we claim that a good sentence representation should be invariant and can benefit the various subsequent tasks. To achieve this purpose, we propose a new scheme of information sharing for multi-task learning. More specifically, all tasks share the same sentence representation and each task can select the task-specific information from the shared sentence representation with attention mechanisms. The query vector of each task's attention could be either static parameters or generated dynamically. We conduct extensive experiments on 16 different text classification tasks, which demonstrate the benefits of our architecture. Source codes of this paper are available on Github.	
54	Bijlani, Ashish and Ramachandran, Umakishore	Extension Framework for File Systems in User Space		2019	User file systems offer numerous advantages over their in-kernel implementations, such as ease of development and better system reliability. However, they incur heavy performance penalty. We observe that existing user file system frameworks are highly general; they consist of a minimal interposition layer in the kernel that simply forwards all low-level requests to user space. While this design offers flexibility, it also severely degrades performance due to frequent kernel-user context switching.This work introduces EXTFUSE, a framework for developing extensible user file systems that also allows applications to register "thin" specialized request handlers in the kernel to meet their specific operative needs, while retaining the complex functionality in user space. Our evaluation with two FUSE file systems shows that EXTFUSE can improve the performance of user file systems with less than a few hundred lines on average. EXTFUSE is available on GitHub.	
55	Mehdi, Nabeel and Starly, Binil	A Simulator Testbed for MT-Connect Based Machines in a Scalable and Federated Multi-Enterprise Environment		2019	The emergence and steady adoption of machine communication protocols like the MTConnect are steering the manufacturing sector towards greater machine interoperability, higher operational productivity, substantial cost savings with advanced decision-making capabilities at the shop-floor level. MTConnect GitHub repository and NIST Smart Manufacturing Systems (SMS) Test Bed are two major resources for collecting data from CNC machines. However, these tools would be insufficient and protractive in Modeling &amp; Simulation (M&amp;S) scenarios where spawning hundreds of MTConnect agents and thousands of adapters with real-time virtual machining is necessary for advancing research in the digital supply chain. This paper introduces a flexible simulator testbed of multiple MTConnect agents and adapters for simulating Levels 0 &amp; 1 of the ISA-95 framework and help support R&amp;D activities in complex multi-enterprise supply chain scenarios. To the best knowledge of the authors, there is no publicly accessible multi-enterprise MTConnect testbed yet.	
56	Bass, Len and Holz, Ralph and Rimba, Paul and Tran, An Binh and Zhu, Liming	Securing a Deployment Pipeline		2015	At the RELENG 2014 Q&amp;A, the question was asked, "What is your greatest concern?" and the response was "someone subverting our deployment pipeline". That is the motivation for this paper. We explore what it means to subvert a pipeline and provide several different scenarios of subversion. We then focus on the issue of securing a pipeline. As a result, we provide an engineering process that is based on having trusted components mediate access to sensitive portions of the pipeline from other components, which can remain untrusted. Applying our process to a pipeline we constructed involving Chef, Jenkins, Docker, Github, and AWS, we find that some aspects of our process result in easy to make changes to the pipeline, whereas others are more difficult. Consequently, we have developed a design that hardens the pipeline, although it does not yet completely secure it.	
57	Xiao, Jun and Ye, Hao and He, Xiangnan and Zhang, Hanwang and Wu, Fei and Chua, Tat-Seng	Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks		2017	Factorization Machines  (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named  Attentional Factorization Machine  (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a 8.6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&amp;Deep [Cheng  et al. , 2016] and Deep-Cross [Shan  et al. , 2016] with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: https://github. com/hexiangnan/attentional_factorization_machine	
68	He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi	Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks		2018	This paper proposed a Soft Filter Pruning (SFP) method to accelerate the inference procedure of deep Convolutional Neural Networks (CNNs). Specifically, the proposed SFP enables the pruned filters to be updated when training the model after pruning. SFP has two advantages over previous works: (1) Larger model capacity. Updating previously pruned filters provides our approach with larger optimization space than fixing the filters to zero. Therefore, the network trained by our method has a larger model capacity to learn from the training data. (2) Less dependence on the pretrained model. Large capacity enables SFP to train from scratch and prune the model simultaneously. In contrast, previous filter pruning methods should be conducted on the basis of the pre-trained model to guarantee their performance. Empirically, SFP from scratch outperforms the previous filter pruning methods. Moreover, our approach has been demonstrated effective for many advanced CNN architectures. Notably, on ILSCRC-2012, SFP reduces more than 42% FLOPs on ResNet-101 with even 0.2% top-5 accuracy improvement, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/soft-filter-pruning	
70	Kazemi, Seyed Mehran and Poole, David	SimplE Embedding for Link Prediction in Knowledge Graphs		2018	Knowledge graphs contain knowledge about the world and provide a structured representation of this knowledge. Current knowledge graphs contain only a small subset of what is true in the world. Link prediction approaches aim at predicting new links for a knowledge graph given the existing links among the entities. Tensor factorization approaches have proved promising for such link prediction problems. Proposed in 1927, Canonical Polyadic (CP) decomposition is among the first tensor factorization approaches. CP generally performs poorly for link prediction as it learns two independent embedding vectors for each entity, whereas they are really tied. We present a simple enhancement of CP (which we call SimplE) to allow the two embeddings of each entity to be learned dependently. The complexity of SimplE grows linearly with the size of embeddings. The embeddings learned through SimplE are interpretable, and certain types of background knowledge can be incorporated into these embeddings through weight tying. We prove SimplE is fully expressive and derive a bound on the size of its embeddings for full expressivity. We show empirically that, despite its simplicity, SimplE outperforms several state-of-the-art tensor factorization techniques. SimplE's code is available on GitHub at https://github.com/Mehran-k/SimplE.	
77	Fursin, Grigori and Lokhmotov, Anton and Plowman, Ed	Collective Knowledge: Towards R&amp;D Sustainability		2016	Research funding bodies strongly encourage research projects to disseminate discovered knowledge and transfer developed technology to industry. Unfortunately, capturing, sharing, reproducing and building upon experimental results has become close to impossible in computer systems' R&amp;D. The main challenges include the ever changing hardware and software technologies, lack of standard experimental methodology and lack of robust knowledge exchange mechanisms apart from publications where reproducibility is still rarely considered.Supported by the EU FP7 TETRACOM Coordination Action, we have developed Collective Knowledge (CK), an open-source framework and methodology that involves the R&amp;D community to solve the above problems collaboratively. CK helps researchers gradually convert their code and data into reusable components and share them via repositories such as GitHub, design and evolve over time experimental scenarios, replay experiments under the same or similar conditions, apply state-of-the-art statistical techniques, crowdsource experiments across different platforms, and enable interactive publications. Importantly, CK encourages the continuity and sustainability of R&amp;D efforts: researchers and engineers can build upon the work of others and make their own work available for others to build upon. We believe that R&amp;D sustainability will lead to better research and faster commercialization, thus increasing return-on-investment.	
78	Bernardo, Jo\\~ao Helis and da Costa, Daniel Alencar and Kulesza, Uir\\'a	Studying the Impact of Adopting Continuous Integration on the Delivery Time of Pull Requests		2018	Continuous Integration (CI) is a software development practice that leads developers to integrate their work more frequently. Software projects have broadly adopted CI to ship new releases more frequently and to improve code integration. The adoption of CI is motivated by the allure of delivering new functionalities more quickly. However, there is little empirical evidence to support such a claim. Through the analysis of 162,653 pull requests (PRs) of 87 GitHub projects that are implemented in 5 different programming languages, we empirically investigate the impact of adopting CI on the time to deliver merged PRs. Surprisingly, only 51.3% of the projects deliver merged PRs more quickly after adopting CI. We also observe that the large increase of PR submissions after CI is a key reason as to why projects deliver PRs more slowly after adopting CI. To investigate the factors that are related to the time-to-delivery of merged PRs, we train regression models that obtain sound median R-squares of 0.64-0.67. Finally, a deeper analysis of our models indicates that, before the adoption of CI, the integration-load of the development team, i.e., the number of submitted PRs competing for being merged, is the most impactful metric on the time to deliver merged PRs before CI. Our models also reveal that PRs that are merged more recently in a release cycle experience a slower delivery time.	https://doi.org/10.1145/3196398.3196421
92	Blincoe, Kelly and Harrison, Francis and Damian, Daniela	Ecosystems in GitHub and a Method for Ecosystem Identification Using Reference Coupling		2015	Software projects are not developed in isolation. Recent research has shifted to studying software ecosystems, communities of projects that depend on each other and are developed together. However, identifying technical dependencies at the ecosystem level can be challenging. In this paper, we propose a new method, known as reference coupling, for detecting technical dependencies between projects. The method establishes dependencies through user-specified cross-references between projects. We use our method to identify ecosystems in GitHub-hosted projects, and we identify several characteristics of the identified ecosystems. We find that most ecosystems are centered around one project and are interconnected with other ecosystems. The predominant type of ecosystems are those that develop tools to support software development. We also found that the project owners' social behaviour aligns well with the technical dependencies within the ecosystem, but project contributors' social behaviour does not align with these dependencies. We conclude with a discussion on future research that is enabled by our reference coupling method.	
80	Zhu, Jieming and He, Pinjia and Fu, Qiang and Zhang, Hongyu and Lyu, Michael R. and Zhang, Dongmei	Learning to Log: Helping Developers Make Informed Logging Decisions		2015	Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a "learning to log" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, LogAdvisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate LogAdvisor on two industrial software systems from Microsoft and two open-source software systems from GitHub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of "learning to log".	
84		SIGCSE '14: Proceedings of the 45th ACM Technical Symposium on Computer Science Education		2014	Welcome to the 45th ACM Technical Symposium on Computer Science Education (i.e., SIGCSE 2014). We invite you to explore this collection of papers, posters, workshops and other materials that capture the latest ideas, tools and pedagogy in computer science education. We also encourage you to connect with the people that generate, document and "implement" this body of knowledge, take this experience with you and share.The theme is "Leveraging Computing to Change Education," and features a number of talks and sessions focused on how the ideas and applications of computing can benefit not only those learning computer science, but learning overall. SIGCSE 2014 is co-located with a new ACM conference on Learning at Scale; it will be interesting to see what ideas and connections emerge between the two conferences, and we urge you to consider attending both.We are happy to announce the winners of the two annual SIGCSE Awards. Robert M. Panoff, Founder and Executive Director of the Shodor Education Foundation, Inc., will receive the SIGCSE Award for Outstanding Contribution to Computer Science Education, and will provide Thursday morning's keynote address. Andrea W. Lawrence of Spelman College will accept the SIGCSE Award for Lifetime Service to the Computer Science Education Community and speak at our First Timer's Lunch on Thursday. (SIGCSE First Timers will receive lunch for free; more seasoned SIGCSE attendees can purchase a ticket and (a) enjoy a delicious meal and (b) mentor a First Timer).We are also quite excited about the remaining keynote presentations, which include Hadi Partovi, Founder and CEO of Code.org, and A.J. Brush of Microsoft Research, who will each present on Friday morning and Saturday lunch, respectively.Symposium statistics are presented in the following table. We thank the authors, reviewers, and Program Committee members involved in the production of this program. The schedule this year has the usual varied selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition on Thursday afternoon. There are also some atypical offerings, such as Experience-IT, and of course the co-located Learning at Scale conference. The latest in hardware, software tools, textbooks, educational programs and research are found in the exhibit hall.SIGCSE 2014 Pre-symposium Events will occur on Wednesday and cover the following topics: Process Oriented Guided Inquiry Learning (POGIL)Integrating Professional/Ethical Issues into the CS CurriculumNew Educators WorkshopSIGCAS Nowadays: What is it? What should it be?Git &amp; GitHub Foundations for Educators	
87		PPoPP '17: Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming		2017	It is our great pleasure to welcome you to PPoPP 2017, the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming held in Austin, Texas during February 4-8, 2017, and co-located with the CGO 2017 and HPCA 2017 conferences. This year's symposium continues and reinforces the PPoPP tradition of publishing leading work on all aspects of parallel programming, including foundational and theoretical aspects, techniques, languages, compilers, runtime systems, tools, and practical experiences. Given the pervasiveness of parallel architectures in the general consumer market, PPoPP, with its interest in new parallel workloads, techniques and productivity tools for parallel programming, is becoming more relevant than ever to the computer science community.PPoPP 2017 received 132 submissions from countries all over the world. The submitted papers underwent a rigorous two-phase review process. To maintain fairness and uniform standards, the review process was double-blind, throughout. Almost all of the 132 submissions were reviewed in the first phase by four members of the combined PC and ERC. (A very small fraction received three reviews in the first phase.) All papers were assigned a discussion lead from the PC. After the first rebuttal phase and ongoing online discussions, reviewers reached a consensus to relegate half of the submissions. Their authors were subsequently notified and given the choice of withdrawing their papers. The submissions for which there was no clear consensus or that had only three reviews (very few) were retained for the second evaluation stage. In the second phase, the remaining papers received at least two additional reviews exclusively from PC members and some external specialists. After a second rebuttal period, PC and ERC members continued their online discussions and grouped papers in top, bottom and discuss categories. Finally, 35 PC members met in person over two half days from noon, November 5th through the afternoon of November 6th at the Department of Computer Science in Rice University, Houston, TX, and concluded the meeting by accepting (or conditionally accepting) a total of 29 papers. The 14 conditionally accepted papers were shepherded by volunteer PC members, and the final version was made available to all original reviewers for their approval. All in all, this process resulted in a manageable average load of 12 papers for PC members, 6 papers for ERC members, offered all authors the possibility to respond to all reviews, and helped ensure that reviewing efforts were focused on where they were needed the most.Because many quality papers could not be accommodated as regular contributions, all papers retained for the second review phase were invited to be presented as posters at the conference. As a result, 17 posters were included in the proceedings as 2 page abstracts. The posters were presented during a special two-hour late afternoon session. All authors of accepted papers were given the option of participating in a joint CGO-PPoPP Artifact Evaluation (AE) process. The AE process is intended to encourage researchers to conduct experiments in a reproducible way, to package experimental work-flows and all related materials for broad availability, and ultimately, to enable fair comparison of published techniques. This year saw a considerable increase in the amount of submitted artifacts: 27 versus 18 two years ago, almost equally split between CGO and PPoPP. The Artifact Evaluation Committee of 41 researchers and engineers spent two weeks validating and evaluating the artifacts. Each submission received at least three reviews and only eight of them fell below acceptance criteria. To help educate authors about result reproducibility, these papers were shepherded during a week's time by the AE Committee. Concurrently, the AE committee successfully tried an "open reviewing model", i.e., asked the community to publicly evaluate several artifacts already available at Github, Gila and other project hosting services. This enabled us to find additional external reviewers with access to HPC servers or proprietary benchmarks and tools. At the end of this process all submissions qualified to receive the AE seal and their authors were encouraged to submit a two page Artifact Appendix to document the process.The success of a major conference like PPoPP very much depends on the hard work of all members of the organizing committee who volunteer their time in this service. We thank all PC and ERC members for their thoughtful reviews and extensive online discussions, with special thanks to the PC members who came from all over the world to the meeting in Houston and deliberated for two half days and read additional papers overnight to produce the PPoPP 2017 program. Several of the PC members volunteered to shepherd papers, and deserve special thanks for their efforts. The AE process was lengthy and demanding, and the AE Chairs, Wonsun Ahn (for PPoPP) and Joe Devietti (for CGO) and their team did an amazing job on that Herculean task.	
88	Vendome, Christopher	A Large Scale Study of License Usage on GitHub		2015	The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from GitHub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.	
89	Casalnuovo, Casey and Devanbu, Prem and Oliveira, Abilio and Filkov, Vladimir and Ray, Baishakhi	Assert Use in GitHub Projects		2015	Asserts have long been a strongly recommended (if non-functional) adjunct to programs. They certainly don't add any user-evident feature value; and it can take quite some skill and effort to devise and add useful asserts. However, they are believed to add considerable value to the developer. Certainly, they can help with automated verification; but even in the absence of that, claimed advantages include improved understandability, maintainability, easier fault localization and diagnosis, all eventually leading to better software quality. We focus on this latter claim, and use a large dataset of asserts in C and C++ programs to explore the connection between asserts and defect occurrence. Our data suggests a connection: functions with asserts do have significantly fewer defects. This indicates that asserts do play an important role in software quality; we therefore explored further the factors that play a role in assertion placement: specifically, process factors (such as developer experience and ownership) and product factors, particularly interprocedural factors, exploring how the placement of assertions in functions are influenced by local and global network properties of the callgraph. Finally, we also conduct a differential analysis of assertion use across different application domains.	
90	Vendome, Christopher and Linares-V\\'asquez, Mario and Bavota, Gabriele and Di Penta, Massimiliano and German, Daniel and Poshyvanyk, Denys	License Usage and Changes: A Large-Scale Study of Java Projects on GitHub		2015	Software licenses determine, from a legal point of view, under which conditions software can be integrated, used, and above all, redistributed. Licenses evolve over time to meet the needs of development communities and to cope with emerging legal issues and new development paradigms. Such evolution of licenses is likely to be accompanied by changes in the way how software uses such licenses, resulting in some licenses being adopted while others are abandoned. This paper reports a large empirical study aimed at quantitatively and qualitatively investigating when and why developer change software licenses. Specifically, we first identify licenses' changes in 1,731,828 commits, representing the entire history of 16,221 Java projects hosted on GitHub. Then, to understand the rationale of license changes, we perform a qualitative analysis---following a grounded theory approach---of commit notes and issue tracker discussions concerning licensing topics and, whenever possible, try to build traceability links between discussions and changes. Our results point out a lack of traceability of when and why licensing changes are made. This can be a major concern, because a change in the license of a system can negatively impact those that reuse it.	
91	Kalliamvakou, Eirini and Damian, Daniela and Blincoe, Kelly and Singer, Leif and German, Daniel M.	Open Source-Style Collaborative Development Practices in Commercial Projects Using GitHub		2015	Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mechanism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (oss) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of oss projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.	
93	Wang, Weiliang and Poo-Caama\\~no, Germ\\'an and Wilde, Evan and German, Daniel M.	What is the Gist? Understanding the Use of Public Gists on GitHub		2015	GitHub is a popular source code hosting site which serves as a collaborative coding platform. The many features of GitHub have greatly facilitated developers' collaboration, communication, and coordination. Gists are one feature of GitHub, which defines them as "a simple way to share snippets and pastes with others." This three-part study explores how users are using Gists. The first part is a quantitative analysis of Gist metadata and contents. The second part investigates the information contained in a Gist: We sampled 750k users and their Gists (totalling 762k Gists), then manually categorized the contents of 398. The third part of the study investigates what users are saying Gists are for by reading the contents of web pages and twitter feeds. The results indicate that Gists are used by a small portion of GitHub users, and those that use them typically only have a few. We found that Gists are usually small and composed of a single file. However, Gists serve a wide variety of uses, from saving snippets of code, to creating reusable components for web pages.	
94	Sawant, Anand Ashok and Bacchelli, Alberto	A Dataset for API Usage		2015	An Application Programming Interface (API) provides a specific set of functionalities to a developer. The main aim of an API is to encourage the reuse of already existing functionality. There has been some work done into API popularity trends, API evolution and API usage. For all the aforementioned research avenues there has been a need to mine the usage of an API in order to perform any kind of analysis. Each one of the approaches that has been employed in the past involved a certain degree of inaccuracy as there was no type check that takes place. We introduce an approach that takes type information into account while mining API method invocations and annotation usages. This approach accurately makes a connection between a method invocation and the class of the API to which the method belongs to. We try collecting as many usages of an API as possible, this is achieved by targeting projects hosted on GitHub. Additionally, we look at the history of every project to collect the usage of an API from earliest version onwards. By making such a large and rich dataset public, we hope to stimulate some more research in the field of APIs with the aid of accurate API usage samples.	
95	Grichi, Manel and Abidi, Mouna and Gu\\'eh\\'eneuc, Yann-Ga\\"el and Khomh, Foutse	State of Practices of Java Native Interface		2019	The use of the Java Native Interface (JNI) allows taking advantage of the existing libraries written in different programming languages for code reuse, performance, and security. Despite the importance of JNI in development, practices on its usages are not well studied yet. In this paper, we investigated the usage of JNI in 100 open source systems collected from OpenHub and Github, around 8k of source code files combined between Java and C/C++, including the Java class libraries part of the JDK v9. We identified the state of the practice in JNI systems by semi-automatically and manually analyzing the source code.Our qualitative analysis shows eleven JNI practices where they are mainly related to loading libraries, implementing native methods, exception management, return types, and local/global references management. Basing on our findings, we provided some suggestions and recommendations to developers to facilitate the debugging tasks of JNI in multi-language systems, which can also help them to deal with the Java and C memory.	
96	Weicheng, Yang and Beijun, Shen and Ben, Xu	Mining GitHub: Why Commit Stops -- Exploring the Relationship between Developer's Commit Pattern and File Version Evolution		2013	Using the freeware in GitHub, we are often confused by a phenomenon: the new version of GitHub freeware usually was released in an indefinite frequency, and developers often committed nothing for a long time. This evolution phenomenon interferes with our own development plan and architecture design. Why do these updates happen at that time? Can we predict GitHub software version evolution by developers' activities? This paper aims to explore the developer commit patterns in GitHub, and try to mine the relationship between these patterns (if exists) and code evolution. First, we define four metrics to measure commit activity and code evolution: the changes in each commit, the time between two commits, the author of each changes, and the source code dependency. Then, we adopt visualization techniques to explore developers' commit activity and code evolution. Visual techniques are used to describe the progress of the given project and the authors' contributions. To analyze the commit logs in GitHub software repository automatically, Commits Analysis Tool (CAT) is designed and implemented. Finally, eight open source projects in GitHub are analyzed using CAT, and we find that: 1) the file changes in the previous versions may affect the file depend on it in the next version, 2) the average days around "huge commit" is 3 times of that around normal commit. Using these two patterns and developer's commit model, we can predict when his next commit comes and which file may be changed in that commit. Such information is valuable for project planning of both GitHub projects and other projects which use GitHub freeware to develop software.	
97	Badashian, Ali Sajedi and Shah, Vraj and Stroulia, Eleni	GitHub's Big Data Adaptor: An Eclipse Plugin		2015	The data of GitHub, the most popular code-sharing platform, fits the characteristics of "big data" (Volume, Variety and Velocity). To facilitate studies on this huge GitHub data volume, the GHTorrent web-site publishes a MYSQL dump of (some) GitHub data quarterly. Unfortunately, developers using these published data dumps face challenges with respect to the time required to parse and ingest the data, the space required to store it, and the latency of their queries. To help address these challenges, we developed a data adaptor as an Eclipse plugin, which efficiently handles this dump. The plugin offers an interactive interface through which users can explore and select any field in any table. After extracting the data selected by the user, the parser exports it in easy-to-use spreadsheets. We hope that using this plugin will facilitate further studies on the GitHub data as a whole.	
98	Hauff, Claudia and Gousios, Georgios	Matching GitHub Developer Profiles to Job Advertisements		2015	GitHub is a social coding platform that enables developers to efficiently work on projects, connect with other developers, collaborate and generally "be seen" by the community. This visibility also extends to prospective employers and HR personnel who may use GitHub to learn more about a developer's skills and interests. We propose a pipeline that automatizes this process and automatically suggests matching job advertisements to developers, based on signals extracting from their activities on GitHub.	
99	Onoue, Saya and Hata, Hideaki and Matsumoto, Ken-ichi	A Study of the Characteristics of Developers' Activities in GitHub		2013	What types of developers do active software projects have? This paper presents a study of the characteristics of developers' activities in open source software development. GitHub, a widely-used hosting service for software development projects, provides APIs for collecting various kinds of GitHub data. To clarify the characteristics of developers' activities, we used these APIs to investigate GitHub events generated by each developer. Using this information, we categorized developers based on measures such as whether they prefer communication by coding or comments, or whether they are specialists or generalists. Our study indicates that active software projects have various kinds of developers characterized by different types of development activities.	
100	Yu, Yue and Wang, Huaimin and Filkov, Vladimir and Devanbu, Premkumar and Vasilescu, Bogdan	Wait for It: Determinants of Pull Request Evaluation Latency on GitHub		2015	The pull-based development model, enabled by git and popularised by collaborative coding platforms like BitBucket, Gitorius, and GitHub, is widely used in distributed software teams. While this model lowers the barrier to entry for potential contributors (since anyone can submit pull requests to any repository), it also increases the burden on integrators (i.e., members of a project's core team, responsible for evaluating the proposed changes and integrating them into the main development line), who struggle to keep up with the volume of incoming pull requests. In this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in GitHub. Using regression modeling on data extracted from a sample of GitHub projects using the Travis-CI continuous integration service, we find that latency is a complex issue, requiring many independent variables to explain adequately.	
101	Pham, Raphael and Singer, Leif and Liskin, Olga and Figueira Filho, Fernando and Schneider, Kurt	Creating a Shared Understanding of Testing Culture on a Social Coding Site		2013	Many software development projects struggle with creating and communicating a testing culture that is appropriate for the project's needs. This may degrade software quality by leaving defects undiscovered. Previous research suggests that social coding sites such as GitHub provide a collaborative environment with a high degree of social transparency. This makes developers' actions and interactions more visible and traceable. We conducted interviews with 33 active users of GitHub to investigate how the increased transparency found on GitHub influences developers' testing behaviors. Subsequently, we validated our findings with an online questionnaire that was answered by 569 members of GitHub. We found several strategies that software developers and managers can use to positively influence the testing behavior in their projects. However, project owners on GitHub may not be aware of them. We report on the challenges and risks caused by this and suggest guidelines for promoting a sustainable testing culture in software development projects. 	
102	Perrie, Jessica and Xie, Jing and Nayebi, Maleknaz and Fokaefs, Marios and Lyons, Kelly and Stroulia, Eleni	City on the River: Visualizing Temporal Collaboration		2019	Collaboration is an important component of most work activities. We are interested in understanding how configurations of people come together to create outputs over time. We propose an interactive visualization tool (City on the River) for visualizing collaborations over time. The City on the River (CotR) visualization shows the contributions and artifacts ("products") of a team on a timeline and the individuals on the team who contributed to each product. CotR enables interactive analyses of each of these components for answering questions such as, which people work together on the most products, which products involve the most people, what kinds of products were produced when and by whom, etc. CotR can be used for analyzing diverse domains such as research collaborations, conference participation, email conversations, and software development. In this paper, we present the results of an experiment to assess CotR for analyzing collaboration and outcomes in GitHub projects. We compared the quality of answers, time to answer, and approaches taken to analyze the project collaborations by two groups of people: one group used the GitHub data displayed in a spreadsheet; the other group used the GitHub data displayed using CotR.	
103	Vasilescu, Bogdan and Serebrenik, Alexander and Filkov, Vladimir	A Data Set for Social Diversity Studies of GitHub Teams		2015	Like any other team oriented activity, the software development process is effected by social diversity in the programmer teams. The effect of team diversity can be significant, but also complex, especially in decentralized teams. Discerning the precise contribution of diversity on teams' effectiveness requires quantitative studies of large data sets.Here we present for the first time a large data set of social diversity attributes of programmers in GitHub teams. Using alias resolution, location data, and gender inference techniques, we collected a team social diversity data set of 23,493 GitHub projects. We illustrate how the data set can be used in practice with a series of case studies, and we hope its availability will foster more interest in studying diversity issues in software teams.	
104	Vasilescu, Bogdan and Filkov, Vladimir and Serebrenik, Alexander	Perceptions of Diversity on GitHub: A User Survey		2015	Understanding one's work environment is important for one's success, especially when working in teams. In virtual collaborative environments this amounts to being aware of the technical and social attributes of one's team members. Focusing on Open Source Software teams, naturally very diverse both socially and technically, we report the results of a user survey that tries to resolve how teamwork and individual attributes are perceived by developers collaborating on GitHub, and how those perceptions influence their work. Our findings can be used as complementary data to quantitative studies of developers' behavior on GitHub.	
105	Chapman, Carl and Wang, Peipei and Stolee, Kathryn T.	Exploring Regular Expression Comprehension		2017	The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 of pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluated the understandability of various regex language features. We further analyzed regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics. 	
106	Kavaler, David and Sirovica, Sasha and Hellendoorn, Vincent and Aranovich, Raul and Filkov, Vladimir	Perceived Language Complexity in GitHub Issue Discussions and Their Effect on Issue Resolution		2017	Modern software development is increasingly collaborative. Open Source Software (OSS) are the bellwether; they support dynamic teams, with tools for code sharing, communication, and issue tracking. The success of an OSS project is reliant on team communication. E.g., in issue discussions, individuals rely on rhetoric to argue their position, but also maintain technical relevancy. Rhetoric and technical language are on opposite ends of a language complexity spectrum: the former is stylistically natural; the latter is terse and concise. Issue discussions embody this duality, as developers use rhetoric to describe technical issues. The style mix in any discussion can define group culture and affect performance, e.g., issue resolution times may be longer if discussion is imprecise.  Using GitHub, we studied issue discussions to understand whether project-specific language differences exist, and to what extent users conform to a language norm. We built project-specific and overall GitHub language models to study the effect of perceived language complexity on multiple responses. We find that experienced users conform to project-specific language norms, popular individuals use overall GitHub language rather than project-specific language, and conformance to project-specific language norms reduces issue resolution times. We also provide a tool to calculate project-specific perceived language complexity. 	
107	Podolskiy, Vladimir and Patrou, Maria and Patros, Panos and Gerndt, Michael and Kent, Kenneth B.	The Weakest Link: Revealing and Modeling the Architectural Patterns of Microservice Applications		2020	Cloud microservice applications comprise interconnected services packed into containers. Such applications generate complex communication patterns among their microservices. Studying such patterns can support assuring various quality attributes, such as autoscaling for satisfying performance, availability and scalability, or targeted penetration testing for satisfying security and correctness. We study the structure of containerized microservice applications via providing the methodology and the results of a structural graph-based analysis of 103 Docker Compose deployment files from open-sourced Github repositories. Our findings indicate the dominance of a power-law distribution of microservice interconnections. Further analysis highlights the suitability of the Barab\\'asi-Albert model for generating large random graphs that model the architecture of real microservice applications. The exhibited structures and their usage for engineering microservice applications are discussed.	
108	Pham, Raphael and Singer, Leif and Schneider, Kurt	Building Test Suites in Social Coding Sites by Leveraging Drive-by Commits		2013	GitHub projects attract contributions from a community of users with varying coding and quality assurance skills. Developers on GitHub feel a need for automated tests and rely on test suites for regression testing and continuous integration. However, project owners report to often struggle with implementing an exhaustive test suite. Convincing contributors to provide automated test cases remains a challenge. The absence of an adequate test suite or using tests of low quality can degrade the quality of the software product. We present an approach for reducing the effort required by project owners for extending their test suites. We aim to utilize the phenomenon of drive-by commits: capable users quickly and easily solve problems in others' projects---even though they are not particularly involved in that project---and move on. By analyzing and directing the drive-by commit phenomenon, we hope to use crowdsourcing to improve projects' quality assurance efforts. Valuable test cases and maintenance tasks would be completed by capable users, giving core developers more resources to work on the more complicated issues. 	
109	Matteis, Luca and Verborgh, Ruben	Hosting Queryable and Highly Available Linked Data for Free		2014	SPARQL endpoints suffer from low availability, and require to buy and configure complex servers to host them. With the advent of Linked Data Fragments, and more specifically Triple Pattern Fragments (TPFs), we can now perform complex queries on low-cost servers. Online file repositories and cloud hosting services, such as GitHub, Google Code, Google App Engine or Dropbox can be exploited to host this type of linked data for free. For this purpose we have developed two different proof-of-concept tools that can be used to publish TPFs on GitHub and Google App Engine. A generic TPF client can then be used to perform SPARQL queries on the freely hosted TPF servers.	
110	Sinha, Vibha Singhal and Saha, Diptikalyan and Dhoolia, Pankaj and Padhye, Rohan and Mani, Senthil	Detecting and Mitigating Secret-Key Leaks in Source Code Repositories		2015	Several news articles in the past year highlighted incidents in which malicious users stole API keys embedded in files hosted on public source code repositories such as GitHub and BitBucket in order to drive their own work-loads for free. While some service providers such as Amazon have started taking steps to actively discover such developer carelessness by scouting public repositories and suspending leaked API keys, there is little support for tackling the problem from the code sharing platforms themselves.In this paper, we discuss practical solutions to detecting, preventing and fixing API key leaks. We first outline a handful of methods for detecting API keys embedded within source code, and evaluate their effectiveness using a sample set of projects from GitHub. Second, we enumerate the mechanisms which could be used by developers to prevent or fix key leaks in code repositories manually. Finally, we outline a possible solution that combines these techniques to provide tool support for protecting against key leaks in version control systems.	
111	Eckman, David J. and Henderson, Shane G. and Pasupathy, Raghu	Redesigning a Testbed of Simulation-Optimization Problems and Solvers for Experimental Comparisons		2019	We describe major improvements to the testing capabilities of SimOpt, a library of simulation-optimization problems and solvers. Foremost among these improvements is a transition to GitHub that makes SimOpt easier to use and maintain. We also design two new wrapper functions that facilitate empirical comparisons of solvers. The wrapper functions make extensive use of common random numbers (CRN) both within and across solvers for various purposes; e.g., identifying random initial solutions and running simulation replications. We examine some of the intricacies of using CRN to compare simulation-optimization solvers.	
112	Amir, Ofra and Grosz, Barbara J. and Gajos, Krzysztof Z.	MIP-Nets: Enabling Information Sharing in Loosely-Coupled Teamwork		2016	People collaborate in carrying out such complex activities as treating patients, co-authoring documents and developing software. While technologies such as Dropbox and Github enable groups to work in a distributed manner, coordinating team members' individual activities poses significant challenges. In this paper, we formalize the problem of "information sharing in loosely-coupled extended-duration teamwork". We develop a new representation, Mutual Influence Potential Networks (MIP-Nets), to model collaboration patterns and dependencies among activities, and an algorithm, MIP-DOI, that uses this representation to reason about information sharing.	
113	Bhattacharya, Parantapa and Ekanayake, Saliya and Kuhlman, Chris J. and Lebiere, Christian and Morrison, Don and Swarup, Samarth and Wilson, Mandy L. and Orr, Mark G.	The Matrix: An Agent-Based Modeling Framework for Data Intensive Simulations		2019	Human decision-making is influenced by social, psychological, neurological, emotional, normative, and learning factors, as well as individual traits like age and education level. Social/cognitive computational models that incorporate these factors are increasingly used to study how humans make decisions. A result is that agent models, within agent-based modeling (ABM), are becoming more heavyweight, i.e., are more computationally demanding, making scalability and at-scale simulations all the more difficult to achieve. To address these challenges, we have developed an ABM simulation framework that addresses data-intensive simulation at-scale. We describe system requirements and design, and demonstrate at-scale simulation by modeling 3 million users (each as an individual agent), 13 million repositories, and 239 million user-repository interactions on GitHub. Simulations predict user interactions with GitHub repositories, which, to our knowledge, are the first simulations of this kind. Our simulations demonstrate a three-order of magnitude increase in the number of cognitive agents simultaneously interacting.	
114	Vendome, Christopher and Rao, Dhananjai M. and Giabbanelli, Philippe J.	How Do Modelers Code Artificial Societies? Investigating Practices and Quality of Netlogo Codes from Large Repositories		2020	Many guidelines have been developed for simulations in general or for agent-based models which support artificial societies. When applying such guidelines to examine existing practices, assessment studies are limited by the artifacts released by modelers. Although code is the final product defining an artificial society, 90% of the code produced is not released hence previous assessments necessarily focused on higher-level items such as conceptual design or validation. We address this gap by collecting 338 artificial societies from two hosting platforms, CoMSES/OpenABM and GitHub. An innovation of our approach is the use of software engineering techniques to automatically examine the models with respect to items such as commenting the code, using libraries, or dividing code into functions. We found that developers of artificial societies code the decision-making of their agents from scratch in every model, despite the existence of several libraries that could be used as building blocks.	
115	Blythe, James and Ferrara, Emilio and Huang, Di and Lerman, Kristina and Muric, Goran and Sapienza, Anna and Tregubov, Alexey and Pacheco, Diogo and Bollenbacher, John and Flammini, Alessandro and Hui, Pik-Mai and Menczer, Filippo	The DARPA SocialSim Challenge: Massive Multi-Agent Simulations of the Github Ecosystem		2019	We model the evolution of GitHub, a large collaborative software-development ecosystem, using massive multi-agent simulations as a part of DARPA's SocialSim program. Our best performing models and our agent-based simulation framework are described here. Six different agent models were tested based on a variety of machine learning and statistical methods. The most successful models are based on sampling from a stationary probability distribution of actions and repositories for each agent.	
116	Leibzon, William	Social Network of Software Development at GitHub		2016	This paper looks at organization of software development teams and project communities at GitHub. Using social network analysis several open-source projects are analyzed and social networks of users with ties to a project are shown to have some scale-free properties. We further show how to find core development group and a network metric is introduced to measure collaboration among core members, corresponding to if a project is healthy and more likely to be successful.	
117	Yao, Shuochao and Hao, Yifan and Liu, Dongxin and Liu, Shengzhong and Shao, Huajie and Wu, Jiahao and Bamba, Mouna and Abdelzaher, Tarek and Flamino, James and Szymanski, Boleslaw	A Predictive Self-Configuring Simulator for Online Media		2018	This paper describes the design, implementation, and early experiences with a novel agent-based simulator of online media streams, developed under DARPA's SocialSim Program to extract and predict trends in information dissemination on online media. A hallmark of the simulator is its self-configuring property. Instead of requiring initial set-up, the input to the simulator constitutes data traces collected from the medium to be simulated. The simulator automatically learns from the data such elements as the number of agents involved, the number of objects involved, and the rate of introduction of new agents and objects. It also develops behavior models of simulated agents and objects, and their dependencies. These models are then used to run simulations allowing for future extrapolations and "what if" analyses. Results are presented on using this system to simulate GitHub transactions. They show good performance in terms of both simulation accuracy and overhead.	
118	Nakazawa, Shun and Tanaka, Tetsuo	Prototype of Kanban Tool and Preliminary Evaluation of Visualizing Method for Task Assignment		2015	Kanban is a method used in agile software development. It is a most important tool as it acts as a central communication hub among the members of an agile development team. In this research, the authors develop a prototype of a Kanban tool. The tool displays each developer's tasks across multiple horizontal rows. Therefore, users can assess the task assignment and workloads of team members in one glance. The board also links up with GitHub and has a feature of real time synchronization among clients for distributed development. An experiment showed that the proposed approach was effective.	
119	Loyola, Pablo and Ko, In-Young	Biological Mutualistic Models Applied to Study Open Source Software Development		2012	The evolution of the Web has allowed the generation of several platforms for collaborative work. One of the main contributors to these advances is the Open Source initiative, in which projects are boosted to a new level of interaction and cooperation that improves their software quality and reliability. In order to understand how the group of contributors interacts with the software under development, we propose a novel methodology that adapts Lotka-Volterra-based biological models used for host-parasite interaction. In that sense, we used the concept mutualism from social parasites. Preliminary results based on experiments on the Github collaborative platform showed that Open Source phenomena can be modeled as a mutualistic system, in terms of the evolution of the population of developers and repositories.	
120	Dantas, Carlos E. C. and de A. Maia, Marcelo	On the Actual Use of Inheritance and Interface in Java Projects: Evolution and Implications		2017	Background: Inheritance is one of the main features in the object-oriented paradigm (OOP). Nonetheless, previous work recommend carefully using it, suggesting alternatives such as the adoption of composition with implementation of interfaces. Despite of being a well-studied theme, there is still little knowledge if such recommendations have been widely adopted by developers in general. Aims: This work aims at evaluating how the inheritance and composition with interfaces have been used in Java, comparing new projects with older ones (transversal), and also the different releases of the same projects (longitudinal). Method: A total of 1, 656 open-source projects built between 1997 and 2013, hosted in the repositories GitHub and SourceForge, were analyzed. The likelihood of more recent projects using inheritance and interfaces differently from older ones was analyzed considering indicators, such as, the prevalence of corrective changes, instanceof operations, and code smells. Regression analysis, chi-squared test of proportions and descriptive statistics were used to analyze the data. In addition, a thematic analysis based method was used to verify how often and why inheritance and interface are added or removed from classes. Results: We observed that developers still use inheritance primarily for code reuse, motivated by the need to avoid duplicity of source code. In newer projects, classes in inheritance had fewer corrective changes and subclasses had fewer use of the instance of operator. However, as they evolve, classes in inheritance tend to become complex as changes occur. Classes implementing interfaces have shown little relation to the interfaces, and there is indication that interfaces are still underutilized. Conclusion: These results show there is still some lack of knowledge about the use of recommended object-oriented practices, suggesting the need of training developers on how to design better classes.	
121	Vasilescu, Bogdan and Serebrenik, Alexander and Mens, Tom	A Historical Dataset of Software Engineering Conferences		2013	The Mining Software Repositories community typically focuses on data from software configuration management tools, mailing lists, and bug tracking repositories to uncover interesting and actionable information about the evolution of software systems. However, the techniques employed and the challenges faced when mining are not restricted to these types of repositories. In this paper, we present an atypical dataset of software engineering conferences, containing historical data about the accepted papers and the composition of programme committees for eleven well-established conferences. The dataset (published on Github at https://github.com/tue-mdse/conferenceMetrics) can be used, e.g., by conference steering committees or programme committee chairs to assess their selection process and compare against other conferences in the field, or by prospective authors to decide in which conferences to publish. 	
122	Spinellis, Diomidis	A Repository with 44 Years of Unix Evolution		2015	The evolution of the Unix operating system is made available as a version-control repository, covering the period from its inception in 1972 as a five thousand line kernel, to 2015 as a widely-used 26 million line system. The repository contains 659 thousand commits and 2306 merges. The repository employs the commonly used Git system for its storage, and is hosted on the popular GitHub archive. It has been created by synthesizing with custom software 24 snapshots of systems developed at Bell Labs, Berkeley University, and the 386BSD team, two legacy repositories, and the modern repository of the open source FreeBSD system. In total, 850 individual contributors are identified, the early ones through primary research. The data set can be used for empirical research in software engineering, information systems, and software archaeology.	
123	Vassiliadis, Vangelis and Wielemaker, Jan and Mungall, Chris	Processing OWL2 Ontologies Using Thea: An Application of Logic Programming		2009	Traditional object-oriented programming languages can be difficult to use when working with ontologies, leading to the creation of domain-specific languages designed specifically for ontology processing. Prolog, with its logic-based, declarative semantics offers many advantages as a host programming language for querying and processing OWL2 ontologies. The SWI-Prolog semweb library provides some support for OWL but until now there has been a lack of any library providing direct and comprehensive support for OWL2.We have developed Thea, a library based directly on the OWL2 functional-style syntax, allowing storage and manipulation of axioms as a Prolog database. Thea can translate ontologies to Description Logic programs but the emphasis is on using Prolog as an application programming and processing language rather than a reasoning engine. Thea offers the ability to seamless connect to the java OWL API and OWLLink servers. Thea also includes support for SWRL.In this paper we provide examples of using Thea for processing ontologies, and compare the results to alternative methods. Thea is available from GitHub: http://github.com/vangelisv/thea.	
124	Menashe, Jacob and Stone, Peter	Escape Room: A Configurable Testbed for Hierarchical Reinforcement Learning		2019	Recent successes in Reinforcement Learning have encouraged a fast growing network of RL researchers and a number of breakthroughs in RL research. As the RL community and body of work grows, so does the need for widely applicable benchmarks that can fairly and effectively evaluate a variety of RL algorithms. In this paper we present the Escape Room Domain (ERD), a new flexible, scalable, and fully implemented testing domain for Hierarchical RL that bridges the "moderate complexity" gap left behind by existing alternatives. ERD is open-source and freely available through GitHub, and conforms to widely-used public testing interfaces for simple integration and testing with a variety of public RL agent implementations.	
125	Hosek, Petr and Cadar, Cristian	Safe Software Updates via Multi-Version Execution		2013	Software systems are constantly evolving, with new versions and patches being released on a continuous basis. Unfortunately, software updates present a high risk, with many releases introducing new bugs and security vulnerabilities.  We tackle this problem using a simple but effective multi-version based approach. Whenever a new update becomes available, instead of upgrading the software to the new version, we run the new version in parallel with the old one; by carefully coordinating their executions and selecting the behaviour of the more reliable version when they diverge, we create a more secure and dependable multi-version application.  We implemented this technique in Mx, a system targeting Linux applications running on multi-core processors, and show that it can be applied successfully to several real applications such as Coreutils, a set of user-level UNIX applications; Lighttpd, a popular web server used by several high-traffic websites such as Wikipedia and YouTube; and Redis, an advanced key-value data structure server used by many well-known services such as GitHub and Flickr. 	
126	Grigoriou, Marios-Stavros and Kontogiannis, Kostas and Giammaria, Alberto and Brealey, Chris	Report on Evaluation Experiments Using Different Machine Learning Techniques for Defect Prediction		2020	With the emergence of AI, it is of no surprise that the application of Machine Learning techniques has attracted the attention of numerous software maintenance groups around the world. For defect proneness classification in particular, the use of Machine Learning classifiers has been touted as a promising approach. As a consequence, a large volume of research works has been published in the related research literature, utilizing either proprietary data sets or the PROMISE data repository which, for the purposes of this study, focuses only on the use of source code metrics as defect prediction training features. It has been argued though by several researchers, that process metrics may provide a better option as training features than source code metrics. For this paper, we have conducted a detailed extraction of GitHub process metrics from 148 open source systems, and we report on the findings of experiments conducted by using different Machine Learning classification algorithms for defect proneness classification. The main purpose of the paper is not to propose yet another Machine Learning technique for defect proneness classification, but to present to the community a very large data set using process metrics as opposed to source code metrics, and draw some initial interesting conclusions from this statistically significant data set.	
127	Li, Yi	Managing Software Evolution through Semantic History Slicing		2017	Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history instances obtained from real-world open source software projects on GitHub. 	
128	Mirhosseini, Samim and Parnin, Chris	Can Automated Pull Requests Encourage Software Developers to Upgrade Out-of-Date Dependencies?		2017	Developers neglect to update legacy software dependencies, resulting in buggy and insecure software. One explanation for this neglect is the difficulty of constantly checking for the availability of new software updates, verifying their safety, and addressing any migration efforts needed when upgrading a dependency. Emerging tools attempt to address this problem by introducing automated pull requests and project badges to inform the developer of stale dependencies. To understand whether these tools actually help developers, we analyzed 7,470 GitHub projects that used these notification mechanisms to identify any change in upgrade behavior. Our results find that, on average, projects that use pull request notifications upgraded 1.6x as often as projects that did not use any tools. Badge notifications were slightly less effective: users upgraded 1.4x more frequently. Unfortunately, although pull request notifications are useful, developers are often overwhelmed by notifications: only a third of pull requests were actually merged. Through a survey, 62 developers indicated that their most significant concerns are breaking changes, understanding the implications of changes, and migration effort. The implications of our work suggests ways in which notifications can be improved to better align with developers' expectations and the need for new mechanisms to reduce notification fatigue and improve confidence in automated pull requests. 	
129	Wagstrom, Patrick and Jergensen, Corey and Sarma, Anita	A Network of Rails: A Graph Dataset of Ruby on Rails and Associated Projects		2013	Software projects, whether open source, proprietary, or a combination thereof, rarely exist in isolation. Rather, most projects build on a network of people and ideas from dozens, hundreds, or even thousands of other projects. Using the GitHub APIs it is possible to extract these relationships for millions of users and projects. In this paper we present a dataset of a large network of open source projects centered around Ruby on Rails. This dataset provides insight into the relationships between Ruby on Rails and an ecosystem involving 1116 projects. To facilitate understanding of this data in the context of relationships between projects, users, and their activities, it is provided as a graph database suitable for assessing network properties of the community and individuals within those communities and can be found at https://github.com/pridkett/gitminer-data-rails. 	
130	Nobel, Parth	Auto_diff: An Automatic Differentiation Package for Python		2020	We present auto_diff, a package that performs automatic differentiation of numerical Python code. auto_diff overrides Python's NumPy package's functions, augmenting them with seamless automatic differentiation capabilities. Notably, auto_diff is non-intrusive, i.e., the code to be differentiated does not require auto_diff-specific alterations. We illustrate auto_diff on electronic devices, a circuit simulation, and a mechanical system simulation. In our evaluations so far, we found that running simulations with auto_diff takes less than 4 times as long as simulations with hand-written differentiation code. We believe that auto_diff, which was written after attempts to use existing automatic differentiation packages on our applications ran into difficulties, caters to an important need within the numerical Python community. We have attempted to write this paper in a tutorial style to make it accessible to those without prior background in automatic differentiation techniques and packages. We have released auto_diff as open source on GitHub.	
131	Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Bacchelli, Alberto	Will They like This? Evaluating Code Contributions with Language Models		2015	Popular open-source software projects receive and review contributions from a diverse array of developers, many of whom have little to no prior involvement with the project. A recent survey reported that reviewers consider conformance to the project's code style to be one of the top priorities when evaluating code contributions on Github. We propose to quantitatively evaluate the existence and effects of this phenomenon. To this aim we use language models, which were shown to accurately capture stylistic aspects of code. We find that rejected changesets do contain code significantly less similar to the project than accepted ones; furthermore, the less similar changesets are more likely to be subject to thorough review. Armed with these results we further investigate whether new contributors learn to conform to the project style and find that experience is positively correlated with conformance to the project's code style.	
132	Dyer, Robert and Nguyen, Hoan Anh and Rajan, Hridesh and Nguyen, Tien N.	Boa: A Language and Infrastructure for Analyzing Ultra-Large-Scale Software Repositories		2013	In today's software-centric world, ultra-large-scale software repositories, e.g. SourceForge (350,000+ projects), GitHub (250,000+ projects), and Google Code (250,000+ projects) are the new library of Alexandria. They contain an enormous corpus of software and information about software. Scientists and engineers alike are interested in analyzing this wealth of information both for curiosity as well as for testing important hypotheses. However, systematic extraction of relevant data from these repositories and analysis of such data for testing hypotheses is hard, and best left for mining software repository (MSR) experts! The goal of Boa, a domain-specific language and infrastructure described here, is to ease testing MSR-related hypotheses. We have implemented Boa and provide a web-based interface to Boa's infrastructure. Our evaluation demonstrates that Boa substantially reduces programming efforts, thus lowering the barrier to entry. We also see drastic improvements in scalability. Last but not least, reproducing an experiment conducted using Boa is just a matter of re-running small Boa programs provided by previous researchers. 	
133	Oliveira, Johnatan and Fernandes, Eduardo and Souza, Mauricio and Figueiredo, Eduardo	A Method Based on Naming Similarity to Identify Reuse Opportunities		2016	Software reuse is a development strategy in which existing software components, called reusable assets, are used in the development of new software systems. There are many advantages of reuse in software development, such as minimization of development efforts and improvement of software quality. New methods for reusable asset extraction are essential to achieve these advantages. Extraction methods may be used in different contexts including software product lines derivation. However, few methods have been proposed in literature for reusable asset extraction and recommendation of these reuse opportunities. In this paper, we propose a method for extraction of reuse opportunities based on naming similarity of two types of object-oriented entities: classes and methods. Our method, called JReuse, computes a similarity function to identify similarly named classes and methods from a set of software systems from a domain. These classes and methods compose a repository with reuse opportunities. We also present a prototype tool to support the extraction by applying our method. We evaluate the method with 38 e-commerce information systems mined from GitHub. As a result, we observe that our method is able to identify classes and methods that are relevant in the e-commerce domain.	
134	Unruh, Tommi and Shastry, Bhargava and Skoruppa, Malte and Maggi, Federico and Rieck, Konrad and Seifert, Jean-Pierre and Yamaguchi, Fabian	Leveraging Flawed Tutorials for Seeding Large-Scaleweb Vulnerability Discovery		2017	The Web is replete with tutorial-style content on how to accomplish programming tasks. Unfortunately, even topranked tutorials suffer from severe security vulnerabilities, such as cross-site scripting (XSS), and SQL injection (SQLi). Assuming that these tutorials influence real-world software development, we hypothesize that code snippets from popular tutorials can be used to bootstrap vulnerability discovery at scale. To validate our hypothesis, we propose a semi-automated approach to find recurring vulnerabilities starting from a handful of top-ranked tutorials that contain vulnerable code snippets. We evaluate our approach by performing an analysis of tens of thousands of open-source web applications to check if vulnerabilities originating in the selected tutorials recur. Our analysis framework has been running on a standard PC, analyzed 64,415 PHP codebases hosted on GitHub thus far, and found a total of 117 vulnerabilities that have a strong syntactic similarity to vulnerable code snippets present in popular tutorials. In addition to shedding light on the anecdotal belief that programmers reuse web tutorial code in an ad hoc manner, our study finds disconcerting evidence of insufficiently reviewed tutorials compromising the security of open-source projects. Moreover, our findings testify to the feasibility of large-scale vulnerability discovery using poorly written tutorials as a starting point.	
135	Jiang, Siyuan and Armaly, Ameer and McMillan, Collin	Automatically Generating Commit Messages from Diffs Using Neural Machine Translation		2017	Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically ``translate'' diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead. 	
136	Rigby, Peter C. and Barr, Earl T. and Bird, Christian and Devanbu, Prem and German, Daniel M.	What Effect Does Distributed Version Control Have on OSS Project Organization?		2013	Many Open Source Software (OSS) projects are moving form Centralized Version Control (CVC) to Distributed Version Control (DVC). The effect of this shift on project organization and developer collaboration is not well understood. In this paper, we use a theoretical argument to evaluate the appropriateness of using DVC in the context of two very common organization forms in OSS: a dictatorship and a peer group. We find that DVC facilitates large hierarchical communities as well as smaller groups of developers, while CVC allows for consensus-building by a peer group. We also find that the flexibility of DVC systems allows for diverse styles of developer collaboration. With CVC, changes flow up and down (and publicly) via a central repository. In contrast, DVC facilitates collaboration in which work output can flow sideways (and privately) between collaborators, with no repository being inherently more important or central. These sideways flows are a relatively new concept. Developers on the Linux project, who tend to be experienced DVC users, cluster around "sandboxes:" repositories where developers can work together on a particular topic, isolating their changes from other developers. In this work, we focus on two large, mature OSS projects to illustrate these findings. However, we suggest that social media sites like GitHub may engender other original styles of collaboration that deserve further study.	
137	Burlet, Gregory and Hindle, Abram	An Empirical Study of End-User Programmers in the Computer Music Community		2015	Computer musicians are a community of end-user programmers who often use visual programming languages such as Max/MSP or Pure Data to realize their musical compositions. This research study conducts a multifaceted analysis of the software development practices of computer musicians when programming in these visual music-oriented languages. A statistical analysis of project metadata harvested from software repositories hosted on GitHub reveals that in comparison to the general population of software developers, computer musicians' repositories have less commits, less frequent commits, more commits on weekends, yet similar numbers of bug reports and similar numbers of contributing authors. Analysis of source code in these repositories reveals that the vast majority of code can be reconstructed from duplicate fragments. Finally, these results are corroborated by a survey of computer musicians and interviews with individuals in this end-user community. Based on this analysis and feedback from computer musicians we find that there are many avenues where software engineering can be applied to help aid this community of end-user programmers.	
138	Stevens, Marc and Shumow, Daniel	Speeding up Detection of SHA-1 Collision Attacks Using Unavoidable Attack Conditions		2017	Counter-cryptanalysis, the concept of using cryptanalytic techniques to detect cryptanalytic attacks, was introduced at CRYPTO 2013 [23] with a hash collision detection algorithm. That is, an algorithm that detects whether a given single message is part of a colliding message pair constructed using a cryptanalytic collision attack on MD5 or SHA-1.Unfortunately, the original collision detection algorithm is not a low-cost solution as it costs 15 to 224 times more than a single hash computation. In this paper we present a significant performance improvement for collision detection based on the new concept of unavoidable conditions. Unavoidable conditions are conditions that are necessary for all feasible attacks in a certain attack class. As such they can be used to quickly dismiss particular attack classes that may have been used in the construction of the message. To determine an unavoidable condition one must rule out any feasible variant attack where this condition might not be necessary, otherwise adversaries aware of counter-cryptanalysis could easily bypass this improved collision detection with a carefully chosen variant attack. Based on a conjecture solidly supported by the current state of the art, we show how we can determine such unavoidable conditions for SHA-1.We have implemented the improved SHA-1 collision detection using such unavoidable conditions and which is more than 20 times faster than without our unavoidable condition improvements. We have measured that overall our implemented SHA-1 with collision detection is only a factor 1.60 slower, on average, than SHA-1. With the demonstration of a SHA-1 collision, the algorithm presented here has been deployed by Git, GitHub, Google Drive, Gmail, Microsoft OneDrive and others, showing the effectiveness of this technique.	
139	Lyons, Kelly and Oh, Christie	SOA4DM: Applying an SOA Paradigm to Coordination in Humanitarian Disaster Response		2015	Despite efforts to achieve a sustainable state of control over the management of global crises, disasters are occurring with greater frequency, intensity, and affecting many more people than ever before while the resources to deal with them do not grow apace. As we enter 2015, with continued concerns that mega-crises may become the new normal, we need to develop novel methods to improve the efficiency and effectiveness of our management of disasters. Software engineering as a discipline has long had an impact on society beyond its role in the development of software systems. In fact, software engineers have been described as the developers of prototypes for future knowledge workers; tools such as Github and Stack Overflow have demonstrated applications beyond the domain of software engineering. In this paper, we take the potential influence of software engineering one-step further and propose using the software service engineering paradigm as a new approach to managing disasters. Specifically, we show how the underlying principles of service-oriented architectures (SOA) can be applied to the coordination of disaster response operations. We describe key challenges in coordinating disaster response and discuss how an SOA approach can address those challenges.	
140	Moura, Irineu and Pinto, Gustavo and Ebert, Felipe and Castor, Fernando	Mining Energy-Aware Commits		2015	Over the last years, energy consumption has become a first-class citizen in software development practice. While energy-efficient solutions on lower-level layers of the software stack are well-established, there is convincing evidence that even better results can be achieved by encouraging practitioners to participate in the process. For instance, previous work has shown that using a newer version of a concurrent data structure can yield a 2.19x energy savings when compared to the old associative implementation [75]. Nonetheless, little is known about how much software engineers are employing energy-efficient solutions in their applications and what solutions they employ for improving energy-efficiency. In this paper we present a qualitative study of "energy-aware commits". Using Github as our primary data source, we perform a thorough analysis on an initial sample of 2,189 commits and carefully curate a set of 371 energy-aware commits spread over 317 real-world non-trivial applications. Our study reveals that software developers heavily rely on low-level energy management approaches, such as frequency scaling and multiple levels of idleness. Also, our findings suggest that ill-chosen energy saving techniques can impact the correctness of an application. Yet, we found what we call "energy-aware interfaces", which are means for clients (e.g., developers or end-users) to save energy in their applications just by using a function, abstracting away the low-level implementation details.	
141	Lin, Jinfeng and Liu, Yalin and Guo, Jin and Cleland-Huang, Jane and Goss, William and Liu, Wenchuang and Lohar, Sugandha and Monaikul, Natawut and Rasin, Alexander	TiQi: A Natural Language Interface for Querying Software Project Data		2017	AbstractSoftware projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo. Keywords-Natural Language Interface, Project Data, Query 	
142	Muri\\'c, Goran and Tregubov, Alexey and Blythe, Jim and Abeliuk, Andr\\'es and Choudhary, Divya and Lerman, Kristina and Ferrara, Emilio	Massive Cross-Platform Simulations of Online Social Networks		2020	As part of the DARPA SocialSim challenge, we address the problem of predicting behavioral phenomena including information spread involving hundreds of thousands of users across three major linked social networks: Twitter, Reddit and GitHub. Our approach develops a framework for data-driven agent simulation that begins with a discrete-event simulation of the environment populated with generic, flexible agents, then optimizes the decision model of the agents by combining a number of machine learning classification problems. The ML problems predict when an agent will take a certain action in its world and are designed to combine aspects of the agents, gathered from historical data, with dynamic aspects of the environment including the resources, such as tweets, that agents interact with at a given point in time. In this way, each of the agents makes individualized decisions based on their environment, neighbors and history during the simulation, although global simulation data is used to learn accurate generalizations. This approach showed the best performance of all participants in the DARPA challenge across a broad range of metrics. We describe the performance of models both with and without machine learning on measures of cross-platform information spread defined both at the level of the whole population and at the community level. The best-performing model overall combines learned agent behaviors with explicit modeling of bursts in global activity. Because of the general nature of our approach, it is applicable to a range of prediction problems that require modeling individualized, situational agent behavior from trace data that combines many agents.	
143		SIGCSE '13: Proceeding of the 44th ACM Technical Symposium on Computer Science Education		2013	Welcome to the proceedings of the 44th ACM Technical Symposium on Computer Science Education, or SIGCSE 2013, where you will find over one hundred papers as well as multiple other session formats that document the latest in computer science education: research, tool building, teaching, curriculum and philosophy. The theme of this year's symposium is "The Changing Face of Computing", and features a number of talks and sessions focused on how changes in computing technology and changes in student demographics requires a change in the way computing is taught.SIGCSE 2013's opening keynote session on Thursday will be different from anything seen at SIGCSE previously. It consists of "flash talks" (where several "all stars" in computing will be given five minutes to share 20 slides, each of which automatically advance) that answer the question: "What can WE do to change the face of computing?" Jane Margolis of UCLA will provide SIGCE 2013's closing keynote session on Saturday, where she'll examine how underrepresentation in computing relates to a larger educational crisis and issues we face as world citizens. In addition, during a special keynote session on Friday, Stanford's Provost (John Etchemendy) will discuss whether massively open online courses will change our universities as we know them or be a "flash in the pan".We are pleased to announce the winners of the two annual SIGCSE awards. Professor Michael K\\"olling of University of Kent will receive the SIGCSE award for Outstanding Contribution to Computer Science Education, and will provide Friday's keynote address. Henry Walker of Grinnell College will accept the lunch for free; SIGCSE Old Timers can purchase a ticket and (a) enjoy a delicious meal, (b) mentor a First Timer, and (c) listen to Professor Walker's talk.)Symposium statistics are presented in the following table. We thank the authors, reviewers, and Program Committee members whose enormous and vital service generated this program. This year's program includes the usual wide selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition, as well as some unusual offerings, such as the Codebreaker dramadocumentary on Alan Turing's remarkable and tragic life story, a puzzle extravaganza with a raffle for those who complete it, and a CSTA K- 13 Computing Teachers Workshop. Our exhibit hall features a number of exhibitors showcasing the latest in hardware, software tools, textbooks and educational programs and research. We also continue to offer accessibility at SIGCSE 2013 for the deaf and hard of hearing.We are excited about the variety of pre-symposium events that will exist at SIGCSE 2013. As of the press deadline for this overview, meetings on the following topics will occur on Wednesday: Managing the Academic Career for Women, Open Source Software, Computational Thinking Through Computing and Music, CSAB Computing Accreditation Workshop, Git &amp; GitHub Foundations for Educators, SIGCAS Share and Learn, Using the GENI Networking Testbed, Exploring the Next Generation of Scratch, and Integrating Computing Ethics into the Curriculum.	
144		SIGCSE '15: Proceedings of the 46th ACM Technical Symposium on Computer Science Education		2015	Welcome to SIGCSE 2015, the 46th ACM Technical Symposium on Computer Science Education. This year's symposium presents the latest advances in computer science education in a variety of formats: papers, posters, panels, special sessions, workshops, birds of a feather meetings, and, new this year, demonstrations and lightning talks. This year's symposium theme is "Keep Connected. Keep Committed. Keep Computing." We are asking our attendees to consider how they can use the conference to keep connected to each other and the field, to keep committed to the cause of computing education, and most fundamentally, to keep computing and to demonstrate to all our students how exciting this field truly is.We are delighted to have Jessica Hodgins of Carnegie Mellon University and Disney Research giving SIGCSE 2015's opening plenary address on Thursday. Her work at Carnegie Mellon and Disney Research will demonstrate what new and exciting things will keep us computing in the coming years. Keith Hampton of Rutgers University is speaking at our Saturday luncheon. His work in social media demonstrates how keeping us connected is an important part of building strong relationships. Finally, it is our pleasure to announce the recipients of the two annual SIGCSE awards, the recipients of which demonstrate keeping committed to the field of computing education and to the SIGCSE organization and community. Frank Young of Rose-Hulman Institute of Technology will receive the SIGCSE Award for Lifetime Service to the Computer Science Education Community, and will speak at our First Timer's Lunch on Thursday. (SIGCSE First Timers will receive their lunch for free. SIGCSE Old Timers are encouraged to purchase a ticket, join us for lunch, meet some First Timers, and recognize Frank's contributions.) Mark Allen Weiss of Florida International University is the recipient of the SIGCSE Award for Outstanding Contributions to Computer Science Education. Mark will give the plenary address on Friday.As noted above, we are very excited to be introducing two new tracks for SIGCSE 2015. A Demos track will be presented in the exhibit hall during breaks, and a session dedicated to Lightning Talks will take place on Friday afternoon at 3:45pm.Symposium statistics are presented in the following table. This year's program includes the usual wide selection of events, including the Evening Reception on Thursday and the ACM SIGCSE Student Research Competition, as well as another puzzle challenge. Our exhibit hall features a number of exhibitors showcasing the latest in hardware, software tools, textbooks and educational programs and research.Proposal type Paper - Accepted - 105 Received - 289 Acceptance Rate - 36%Panel - Accepted - 10 Received - 18 Acceptance Rate - 56%Special Session - Accepted - 13 Received - 25 Acceptance Rate - 52%Workshop - Accepted - 30 Received - 71 Acceptance Rate - 42%Poster - Accepted - 51 Received - 117 Acceptance Rate - 44%Birds of a Feather - Accepted - 38 Received - 55 Acceptance Rate - 69%Demos - Accepted - 10 Received - 32 Acceptance Rate - 31%Lightning Talks - Accepted - 11 Received - 26 Acceptance Rate - 42%We encourage you to participate in our SIGCSE 2015 Pre-symposium Events. As of the publication deadline for this overview, meetings on the following topics will occur on Wednesday: ACM SIGCAS Symposium on Computing for the Social Good: Educational Practices, CSTeachingTips.org: Tip-A-Thon, GENI in your Classroom, Git and GitHub: Foundations for Educators, LittleFe Build-Out, Managing the Academic Career for Women Faculty in Undergraduate Computing Programs, SIGCSE 2015 Department Chairs Roundtable, and Teaching to Diversity in Computer Science.	
\.


--
-- Name: acm_manual_id_seq; Type: SEQUENCE SET; Schema: public; Owner: postgres
--

SELECT pg_catalog.setval('public.acm_manual_id_seq', 144, true);


--
-- Name: acm_manual acm_manual_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.acm_manual
    ADD CONSTRAINT acm_manual_pkey PRIMARY KEY (id);


--
-- PostgreSQL database dump complete
--

